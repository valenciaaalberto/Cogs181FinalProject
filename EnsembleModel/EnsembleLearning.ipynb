{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0954e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0180c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d2c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534e4dd",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da1270f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineConvolutionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineConvolutionModel,self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 100)             \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d7f13",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c64648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvutionModelAddedLayersBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvutionModelAddedLayersBatchNorm, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_seven = nn.Conv2d(in_channels=1024,out_channels=2048,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm3 = nn.BatchNorm2d(3)\n",
    "        self.batchNorm512 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, 256)\n",
    "        self.fc6 = nn.Linear(256, 100)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.batchNorm512(x)\n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_seven(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da92f8",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ba97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4\n",
    "class ConvolutionalEnsemble(nn.Module):\n",
    "    def __init__(self, modelA, modelB, nb_classes=100):\n",
    "        super(ConvolutionalEnsemble, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        # Remove last linear layer\n",
    "        self.modelA.fc = nn.Identity()\n",
    "        self.modelB.fc = nn.Identity()\n",
    "        \n",
    "        # Create new classifier\n",
    "#         self.classifier = nn.Linear(100 + 100, nb_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.modelA(x.clone())  \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        x2 = self.modelB(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "#         x = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "#         x = self.classifier(F.sigmoid(x))\n",
    "#         x = (x1 + x2) / 2\n",
    "        x = torch.max(x1,x2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511633e2",
   "metadata": {},
   "source": [
    "## Define Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb6d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cb0ff",
   "metadata": {},
   "source": [
    "## Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb085fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelA = BaselineConvolutionModel()\n",
    "modelA.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(modelA.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a40c72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.608\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.603\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.586\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.532\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.495\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.462\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.441\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.402\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.372\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.328\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.301\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.254\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.204\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.176\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.134\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.083\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 4.070\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 4.046\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 4.038\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 4.024\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.960\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.941\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.930\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.889\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.860\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.856\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.843\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.718\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.719\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.684\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.639\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.681\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.596\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.602\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.544\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.551\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.502\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.428\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.409\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.350\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.300\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.331\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.277\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.254\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.175\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.188\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.136\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.117\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.061\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.060\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.005\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.961\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.947\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 2.978\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 2.844\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.836\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 2.801\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 2.845\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.846\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.807\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.764\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.700\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.745\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.612\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.650\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.590\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.566\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.573\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.585\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.545\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.592\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.524\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.495\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.446\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.480\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.378\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.341\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.406\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.340\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.346\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.330\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.326\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.320\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.331\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.284\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.330\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.295\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.154\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.124\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.144\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.157\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.141\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.153\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.204\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.129\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.105\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.153\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.192\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.144\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 1.952\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 1.976\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 1.993\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 1.992\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 1.969\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 1.936\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 2.004\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 1.977\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 1.990\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 1.914\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 1.982\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 2.006\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 1.748\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 1.790\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 1.783\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 1.801\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 1.792\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 1.795\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 1.818\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 1.824\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 1.827\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 1.850\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 1.841\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 1.862\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 1.523\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 1.623\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 1.645\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 1.637\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 1.674\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 1.661\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 1.673\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 1.701\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 1.690\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 1.718\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 1.659\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 1.690\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 1.406\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 1.458\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 1.493\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 1.465\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 1.452\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 1.501\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 1.546\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 1.524\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 1.562\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 1.553\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 1.566\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 1.584\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 1.220\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 1.299\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 1.309\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 1.331\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 1.381\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 1.379\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 1.358\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 1.376\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 1.435\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 1.366\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 1.376\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 1.468\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 1.072\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 1.111\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 1.192\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 1.196\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 1.229\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 1.235\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 1.257\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 1.240\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 1.298\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 1.300\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 1.303\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 1.309\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 0.928\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 1.009\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 1.007\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 1.056\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 1.069\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 1.084\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 1.118\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 1.101\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 1.177\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 1.164\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 1.144\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 1.203\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 0.805\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 0.830\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 0.923\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 0.928\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 0.933\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 0.959\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 1.020\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 0.949\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 1.039\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 1.039\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 1.013\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 1.043\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 0.679\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 0.727\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 0.770\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 0.779\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 0.836\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 0.890\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 0.851\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 0.898\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 0.899\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 0.886\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 0.968\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 0.936\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 0.603\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 0.623\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 0.646\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 0.711\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 0.718\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 0.778\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 0.809\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 0.739\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 0.830\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 0.802\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 0.775\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 0.845\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 0.499\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 0.536\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 0.579\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 0.606\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 0.662\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 0.670\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 0.666\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 0.648\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 0.719\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 0.700\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 0.764\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 0.776\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 0.420\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 0.428\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 0.514\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 0.505\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 0.568\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 0.574\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 0.611\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 0.626\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 0.640\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 0.649\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 0.625\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 0.645\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 0.441\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 0.382\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 0.443\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 0.480\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 0.534\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 0.531\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 0.537\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 0.567\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 0.516\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 0.581\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 0.588\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 0.606\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 0.354\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 0.393\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 0.379\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 0.418\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 0.456\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 0.495\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 0.473\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 0.495\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 0.519\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 0.480\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 0.567\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 0.521\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 0.313\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 0.349\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 0.332\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 0.385\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 0.426\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 0.466\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 0.417\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 0.482\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 0.449\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 0.466\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 0.487\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 0.516\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 0.264\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 0.285\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 0.328\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 0.335\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 0.362\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 0.410\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 0.391\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 0.373\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 0.392\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 0.431\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 0.452\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 0.434\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 0.272\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 0.271\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 0.304\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 0.275\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 0.293\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 0.324\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 0.350\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 0.373\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 0.356\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 0.377\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 0.400\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 0.397\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 0.215\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 0.249\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 0.256\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 0.294\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 0.332\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 0.284\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 0.296\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 0.302\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 0.326\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 0.341\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 0.328\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 0.378\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 0.237\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 0.233\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 0.239\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 0.299\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 0.300\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 0.301\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 0.347\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 0.341\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 0.387\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 0.328\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 0.354\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 0.336\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 0.223\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 0.194\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 0.246\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 0.243\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 0.264\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 0.248\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 0.311\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 0.287\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 0.271\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 0.324\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 0.311\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 0.307\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 0.210\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 0.184\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 0.188\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 0.185\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 0.267\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 0.235\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 0.259\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 0.290\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 0.232\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 0.291\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 0.282\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 0.299\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 0.189\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 0.162\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 0.186\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 0.236\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 0.224\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 0.230\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 0.258\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 0.256\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 0.267\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 0.260\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 0.246\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 0.289\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 0.174\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 0.187\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 0.204\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 0.229\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 0.234\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 0.268\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 0.239\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 0.315\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 0.281\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 0.270\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 0.249\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 0.250\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 0.175\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 0.193\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 0.198\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 0.199\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 0.209\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 0.185\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 0.254\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 0.248\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 0.242\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 0.225\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 0.253\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 0.238\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 0.150\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 0.182\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 0.213\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 0.184\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 0.222\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 0.194\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 0.200\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 0.183\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 0.207\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 0.266\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 0.223\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 0.226\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 0.142\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 0.150\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 0.175\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 0.169\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 0.188\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 0.193\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 0.179\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 0.228\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 0.205\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 0.205\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 0.234\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 0.225\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 0.172\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 0.180\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 0.163\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 0.169\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 0.163\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 0.214\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 0.219\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 0.212\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 0.209\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 0.227\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 0.251\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 0.208\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 0.168\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 0.153\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 0.203\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 0.213\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 0.183\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 0.261\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 0.175\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 0.197\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 0.211\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 0.199\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 0.205\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 0.222\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 0.158\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 0.125\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 0.156\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 0.133\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 0.150\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 0.188\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 0.160\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 0.185\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 0.187\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 0.176\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 0.213\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 0.185\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 0.161\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 0.140\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 0.152\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 0.151\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 0.168\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 0.159\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 0.145\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 0.191\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 0.153\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 0.201\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 0.209\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 0.202\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 0.127\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 0.116\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 0.137\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 0.143\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 0.146\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 0.146\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 0.201\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 0.168\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 0.182\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 0.192\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 0.191\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 0.192\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 0.132\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 0.131\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 0.148\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 0.189\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 0.137\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 0.155\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 0.152\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 0.178\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 0.168\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 0.162\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 0.193\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 0.209\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 0.114\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 0.129\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 0.124\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 0.134\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 0.149\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 0.187\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 0.152\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 0.175\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 0.200\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 0.180\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 0.226\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 0.167\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 0.122\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 0.138\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 0.134\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 0.155\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 0.134\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 0.143\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 0.123\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 0.122\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 0.168\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 0.160\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 0.183\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 0.162\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 0.122\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 0.087\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 0.088\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 0.113\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 0.146\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 0.169\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 0.119\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 0.131\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 0.179\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 0.143\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 0.135\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 0.187\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 0.088\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 0.085\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 0.143\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 0.117\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 0.114\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 0.140\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 0.153\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 0.117\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 0.150\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 0.161\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 0.187\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 0.137\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 0.126\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 0.081\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 0.089\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 0.098\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 0.161\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 0.135\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 0.127\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 0.176\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 0.149\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 0.152\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 0.150\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 0.194\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 0.110\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 0.112\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 0.124\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 0.147\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 0.125\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 0.142\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 0.144\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 0.155\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 0.135\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 0.132\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 0.156\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 0.140\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 0.097\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 0.090\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 0.120\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 0.104\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 0.113\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 0.132\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 0.122\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 0.120\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 0.122\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 0.106\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 0.109\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 0.128\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 0.079\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 0.089\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 0.103\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 0.138\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 0.157\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 0.104\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 0.127\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 0.121\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 0.128\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 0.121\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 0.153\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 0.157\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 0.098\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 0.083\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 0.111\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 0.145\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 0.116\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 0.111\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 0.108\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 0.133\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 0.151\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 0.144\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 0.179\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 0.139\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 0.087\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 0.108\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 0.093\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 0.101\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 0.136\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 0.136\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 0.095\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 0.117\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 0.112\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 0.144\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 0.161\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 0.150\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 0.093\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 0.107\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 0.099\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 0.099\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 0.111\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 0.108\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 0.110\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 0.125\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 0.122\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 0.122\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 0.115\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 0.120\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 0.114\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 0.107\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 0.114\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 0.083\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 0.110\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 0.120\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 0.143\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 0.127\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 0.145\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 0.100\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 0.121\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 0.139\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 0.087\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 0.101\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 0.088\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 0.092\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 0.094\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 0.098\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 0.109\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 0.111\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 0.088\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 0.113\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 0.098\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 0.097\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 0.094\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 0.071\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 0.096\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 0.101\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 0.084\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 0.092\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 0.110\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 0.105\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 0.104\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 0.132\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 0.101\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 0.101\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 0.096\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 0.092\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 0.090\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 0.078\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 0.092\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 0.118\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 0.129\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 0.112\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 0.104\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 0.093\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 0.131\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 0.157\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 0.077\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 0.099\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 0.100\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 0.100\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 0.087\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 0.099\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 0.138\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 0.094\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 0.120\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 0.109\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 0.119\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 0.132\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 0.104\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 0.083\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 0.082\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 0.082\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 0.121\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 0.091\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 0.127\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 0.110\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 0.108\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 0.123\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 0.092\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 0.100\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 0.093\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 0.067\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 0.062\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 0.081\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 0.077\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 0.112\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 0.095\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 0.069\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 0.098\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 0.090\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 0.096\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 0.102\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 0.086\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 0.065\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 0.101\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 0.094\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 0.086\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 0.115\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 0.104\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 0.096\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 0.106\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 0.143\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 0.121\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 0.119\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 0.090\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 0.094\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 0.080\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 0.096\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 0.104\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 0.098\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 0.119\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 0.099\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 0.142\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 0.096\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 0.101\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 0.116\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 0.083\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 0.102\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 0.099\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 0.101\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 0.110\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 0.075\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 0.069\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 0.105\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 0.107\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 0.120\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 0.133\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 0.090\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 0.090\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 0.104\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 0.084\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 0.074\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 0.083\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 0.092\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 0.115\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 0.116\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 0.114\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 0.080\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 0.100\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 0.104\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 0.060\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 0.055\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 0.064\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 0.049\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 0.082\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 0.045\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 0.087\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 0.073\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 0.093\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 0.113\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 0.109\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 0.108\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 0.074\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 0.059\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 0.082\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 0.085\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 0.101\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 0.125\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 0.100\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 0.102\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 0.117\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 0.087\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 0.109\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 0.081\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 0.100\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 0.088\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 0.113\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 0.080\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 0.085\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 0.105\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 0.107\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 0.091\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 0.078\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 0.102\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 0.074\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 0.125\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 0.063\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 0.077\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 0.111\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 0.088\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 0.094\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 0.065\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 0.090\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 0.097\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 0.080\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 0.122\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 0.079\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 0.099\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 0.073\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 0.075\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 0.069\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 0.102\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 0.109\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 0.130\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 0.115\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 0.091\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 0.108\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 0.100\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 0.094\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 0.113\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 0.035\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 0.060\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 0.071\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 0.112\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 0.127\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 0.089\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 0.108\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 0.090\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 0.078\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 0.093\n",
      "[epoch: 68, i: 10999] avg mini-batch loss: 0.111\n",
      "[epoch: 68, i: 11999] avg mini-batch loss: 0.125\n",
      "[epoch: 69, i:   999] avg mini-batch loss: 0.060\n",
      "[epoch: 69, i:  1999] avg mini-batch loss: 0.061\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 0.080\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 0.068\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 0.089\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 0.073\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 0.057\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 0.098\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 0.104\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 0.073\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 0.085\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 0.084\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 0.084\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 0.079\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 0.082\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 0.109\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 0.084\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 0.087\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 0.067\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 0.105\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 0.106\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 0.117\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 0.080\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 0.121\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 0.082\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 0.074\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 0.089\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 0.063\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 0.054\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 0.071\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 0.063\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 0.094\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 0.089\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 0.082\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 0.108\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 0.084\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 0.070\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 0.073\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 0.063\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 0.071\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 0.101\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 0.091\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 0.091\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 0.064\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 0.105\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 0.085\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 0.115\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 0.119\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 0.063\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 0.053\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 0.084\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 0.072\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 0.077\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 0.063\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 0.079\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 0.093\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 0.080\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 0.078\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 0.096\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 0.092\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 0.033\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 0.072\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 0.059\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 0.069\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 0.065\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 0.055\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 0.068\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 0.054\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 0.082\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 0.109\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 0.098\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 0.118\n",
      "[epoch: 75, i:   999] avg mini-batch loss: 0.044\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: 0.056\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: 0.057\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: 0.073\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: 0.077\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: 0.075\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: 0.098\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: 0.099\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: 0.082\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: 0.090\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: 0.097\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: 0.110\n",
      "[epoch: 76, i:   999] avg mini-batch loss: 0.068\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: 0.056\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: 0.064\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: 0.100\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: 0.088\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: 0.079\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: 0.092\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: 0.065\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: 0.090\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: 0.069\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: 0.113\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: 0.114\n",
      "[epoch: 77, i:   999] avg mini-batch loss: 0.067\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: 0.043\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: 0.066\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: 0.057\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: 0.069\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: 0.058\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: 0.081\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: 0.090\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: 0.102\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: 0.072\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: 0.101\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: 0.102\n",
      "[epoch: 78, i:   999] avg mini-batch loss: 0.056\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: 0.051\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: 0.048\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: 0.045\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: 0.059\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: 0.084\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: 0.075\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: 0.089\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: 0.112\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: 0.107\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: 0.074\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: 0.099\n",
      "[epoch: 79, i:   999] avg mini-batch loss: 0.052\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: 0.051\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: 0.071\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: 0.095\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: 0.072\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: 0.078\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: 0.082\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: 0.101\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: 0.077\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: 0.094\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: 0.092\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: 0.080\n",
      "[epoch: 80, i:   999] avg mini-batch loss: 0.062\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: 0.074\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: 0.069\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: 0.103\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: 0.100\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: 0.087\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: 0.065\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: 0.075\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: 0.112\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: 0.077\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: 0.080\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: 0.067\n",
      "[epoch: 81, i:   999] avg mini-batch loss: 0.064\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: 0.074\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: 0.063\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: 0.060\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: 0.059\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: 0.060\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: 0.068\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: 0.121\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: 0.067\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: 0.069\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: 0.065\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: 0.084\n",
      "[epoch: 82, i:   999] avg mini-batch loss: 0.059\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: 0.056\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: 0.072\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: 0.085\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: 0.067\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: 0.092\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: 0.087\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: 0.086\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: 0.088\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: 0.078\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: 0.069\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: 0.090\n",
      "[epoch: 83, i:   999] avg mini-batch loss: 0.065\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: 0.066\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: 0.078\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: 0.068\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: 0.056\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: 0.072\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: 0.056\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: 0.071\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: 0.101\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: 0.117\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: 0.090\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: 0.083\n",
      "[epoch: 84, i:   999] avg mini-batch loss: 0.049\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: 0.063\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: 0.059\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: 0.062\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: 0.062\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: 0.036\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: 0.080\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: 0.091\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: 0.087\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: 0.073\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: 0.085\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: 0.087\n",
      "[epoch: 85, i:   999] avg mini-batch loss: 0.039\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: 0.068\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: 0.066\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: 0.038\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: 0.048\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: 0.075\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: 0.089\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: 0.087\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: 0.091\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: 0.092\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: 0.077\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: 0.069\n",
      "[epoch: 86, i:   999] avg mini-batch loss: 0.045\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: 0.049\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: 0.058\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: 0.087\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: 0.078\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: 0.062\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: 0.093\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: 0.097\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: 0.036\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: 0.069\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: 0.062\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: 0.067\n",
      "[epoch: 87, i:   999] avg mini-batch loss: 0.046\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: 0.078\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: 0.053\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: 0.064\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: 0.062\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: 0.087\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: 0.072\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: 0.063\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: 0.062\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: 0.078\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: 0.072\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: 0.087\n",
      "[epoch: 88, i:   999] avg mini-batch loss: 0.097\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: 0.110\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: 0.083\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: 0.050\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: 0.056\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: 0.086\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: 0.077\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: 0.080\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: 0.076\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: 0.096\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: 0.081\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: 0.071\n",
      "[epoch: 89, i:   999] avg mini-batch loss: 0.052\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: 0.080\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: 0.078\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: 0.093\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: 0.099\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: 0.110\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: 0.100\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: 0.063\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: 0.084\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: 0.084\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: 0.103\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: 0.130\n",
      "[epoch: 90, i:   999] avg mini-batch loss: 0.059\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: 0.063\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: 0.063\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: 0.039\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: 0.075\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: 0.062\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: 0.052\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: 0.062\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: 0.078\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: 0.084\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: 0.070\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: 0.101\n",
      "[epoch: 91, i:   999] avg mini-batch loss: 0.089\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: 0.092\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: 0.051\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: 0.070\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: 0.047\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: 0.047\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: 0.077\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: 0.067\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: 0.084\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: 0.088\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: 0.081\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: 0.077\n",
      "[epoch: 92, i:   999] avg mini-batch loss: 0.071\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: 0.054\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: 0.061\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: 0.075\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: 0.063\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: 0.040\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: 0.086\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: 0.073\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: 0.079\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: 0.073\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: 0.107\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: 0.082\n",
      "[epoch: 93, i:   999] avg mini-batch loss: 0.057\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: 0.041\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: 0.083\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: 0.087\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: 0.090\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: 0.092\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: 0.055\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: 0.081\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: 0.069\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: 0.081\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: 0.075\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: 0.072\n",
      "[epoch: 94, i:   999] avg mini-batch loss: 0.075\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: 0.069\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: 0.046\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: 0.063\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: 0.066\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: 0.112\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: 0.062\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: 0.093\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: 0.056\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: 0.065\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: 0.058\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: 0.084\n",
      "[epoch: 95, i:   999] avg mini-batch loss: 0.045\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: 0.065\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: 0.058\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: 0.069\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: 0.070\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: 0.091\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: 0.058\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: 0.070\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: 0.035\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: 0.040\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: 0.064\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: 0.064\n",
      "[epoch: 96, i:   999] avg mini-batch loss: 0.037\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: 0.049\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: 0.044\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: 0.038\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: 0.067\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: 0.069\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: 0.076\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: 0.093\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: 0.075\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: 0.077\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: 0.064\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: 0.074\n",
      "[epoch: 97, i:   999] avg mini-batch loss: 0.060\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: 0.066\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: 0.055\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: 0.059\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: 0.077\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: 0.068\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: 0.071\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: 0.071\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: 0.047\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: 0.090\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: 0.067\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: 0.088\n",
      "[epoch: 98, i:   999] avg mini-batch loss: 0.052\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: 0.048\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: 0.060\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: 0.062\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: 0.062\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: 0.088\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: 0.072\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: 0.071\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: 0.060\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: 0.072\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: 0.088\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: 0.056\n",
      "[epoch: 99, i:   999] avg mini-batch loss: 0.059\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: 0.052\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: 0.050\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: 0.068\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: 0.074\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: 0.089\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: 0.055\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: 0.094\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: 0.064\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: 0.063\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: 0.075\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: 0.073\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "# epochs = 20      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = modelA(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2da90a",
   "metadata": {},
   "source": [
    "## Model 1 Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88739588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725cbac",
   "metadata": {},
   "source": [
    "## Evaluate Model 1 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f8821aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 46 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = modelA(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae63ccd",
   "metadata": {},
   "source": [
    "## Evaluate Model 1 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "121b4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 71 %\n",
      "Accuracy of aquarium_fish : 61 %\n",
      "Accuracy of  baby : 39 %\n",
      "Accuracy of  bear : 23 %\n",
      "Accuracy of beaver : 38 %\n",
      "Accuracy of   bed : 54 %\n",
      "Accuracy of   bee : 53 %\n",
      "Accuracy of beetle : 48 %\n",
      "Accuracy of bicycle : 62 %\n",
      "Accuracy of bottle : 71 %\n",
      "Accuracy of  bowl : 27 %\n",
      "Accuracy of   boy : 24 %\n",
      "Accuracy of bridge : 51 %\n",
      "Accuracy of   bus : 44 %\n",
      "Accuracy of butterfly : 29 %\n",
      "Accuracy of camel : 41 %\n",
      "Accuracy of   can : 44 %\n",
      "Accuracy of castle : 63 %\n",
      "Accuracy of caterpillar : 33 %\n",
      "Accuracy of cattle : 31 %\n",
      "Accuracy of chair : 74 %\n",
      "Accuracy of chimpanzee : 58 %\n",
      "Accuracy of clock : 52 %\n",
      "Accuracy of cloud : 54 %\n",
      "Accuracy of cockroach : 74 %\n",
      "Accuracy of couch : 26 %\n",
      "Accuracy of  crab : 39 %\n",
      "Accuracy of crocodile : 32 %\n",
      "Accuracy of   cup : 70 %\n",
      "Accuracy of dinosaur : 45 %\n",
      "Accuracy of dolphin : 44 %\n",
      "Accuracy of elephant : 32 %\n",
      "Accuracy of flatfish : 46 %\n",
      "Accuracy of forest : 42 %\n",
      "Accuracy of   fox : 38 %\n",
      "Accuracy of  girl : 21 %\n",
      "Accuracy of hamster : 48 %\n",
      "Accuracy of house : 31 %\n",
      "Accuracy of kangaroo : 29 %\n",
      "Accuracy of keyboard : 73 %\n",
      "Accuracy of  lamp : 38 %\n",
      "Accuracy of lawn_mower : 62 %\n",
      "Accuracy of leopard : 50 %\n",
      "Accuracy of  lion : 42 %\n",
      "Accuracy of lizard : 23 %\n",
      "Accuracy of lobster : 29 %\n",
      "Accuracy of   man : 27 %\n",
      "Accuracy of maple_tree : 63 %\n",
      "Accuracy of motorcycle : 76 %\n",
      "Accuracy of mountain : 59 %\n",
      "Accuracy of mouse : 29 %\n",
      "Accuracy of mushroom : 57 %\n",
      "Accuracy of oak_tree : 55 %\n",
      "Accuracy of orange : 69 %\n",
      "Accuracy of orchid : 46 %\n",
      "Accuracy of otter : 19 %\n",
      "Accuracy of palm_tree : 70 %\n",
      "Accuracy of  pear : 57 %\n",
      "Accuracy of pickup_truck : 68 %\n",
      "Accuracy of pine_tree : 45 %\n",
      "Accuracy of plain : 74 %\n",
      "Accuracy of plate : 55 %\n",
      "Accuracy of poppy : 52 %\n",
      "Accuracy of porcupine : 42 %\n",
      "Accuracy of possum : 29 %\n",
      "Accuracy of rabbit : 17 %\n",
      "Accuracy of raccoon : 44 %\n",
      "Accuracy of   ray : 30 %\n",
      "Accuracy of  road : 70 %\n",
      "Accuracy of rocket : 72 %\n",
      "Accuracy of  rose : 42 %\n",
      "Accuracy of   sea : 71 %\n",
      "Accuracy of  seal : 13 %\n",
      "Accuracy of shark : 37 %\n",
      "Accuracy of shrew : 26 %\n",
      "Accuracy of skunk : 69 %\n",
      "Accuracy of skyscraper : 72 %\n",
      "Accuracy of snail : 33 %\n",
      "Accuracy of snake : 37 %\n",
      "Accuracy of spider : 52 %\n",
      "Accuracy of squirrel : 24 %\n",
      "Accuracy of streetcar : 61 %\n",
      "Accuracy of sunflower : 82 %\n",
      "Accuracy of sweet_pepper : 32 %\n",
      "Accuracy of table : 42 %\n",
      "Accuracy of  tank : 57 %\n",
      "Accuracy of telephone : 36 %\n",
      "Accuracy of television : 46 %\n",
      "Accuracy of tiger : 45 %\n",
      "Accuracy of tractor : 52 %\n",
      "Accuracy of train : 50 %\n",
      "Accuracy of trout : 58 %\n",
      "Accuracy of tulip : 34 %\n",
      "Accuracy of turtle : 30 %\n",
      "Accuracy of wardrobe : 82 %\n",
      "Accuracy of whale : 45 %\n",
      "Accuracy of willow_tree : 40 %\n",
      "Accuracy of  wolf : 40 %\n",
      "Accuracy of woman : 12 %\n",
      "Accuracy of  worm : 56 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = modelA(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679cc8ab",
   "metadata": {},
   "source": [
    "## Model 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c0939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelB = ConvutionModelAddedLayersBatchNorm()\n",
    "modelB.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(modelB.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0211a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.580\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.393\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.240\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.121\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 3.995\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 3.926\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 3.803\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 3.794\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 3.673\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 3.641\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 3.567\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 3.445\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 3.326\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 3.270\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 3.222\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 3.137\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 3.098\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 3.034\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 3.055\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 2.920\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 2.923\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 2.920\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 2.899\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 2.810\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 2.641\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 2.635\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 2.599\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 2.577\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 2.595\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 2.563\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 2.587\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 2.536\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 2.467\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 2.470\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 2.501\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 2.439\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 2.186\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 2.205\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 2.174\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 2.215\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 2.232\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 2.227\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 2.199\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 2.204\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 2.170\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 2.124\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 2.191\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 2.192\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 1.783\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 1.854\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 1.829\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 1.843\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 1.875\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 1.829\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 1.920\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 1.878\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 1.856\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 1.902\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 1.897\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 1.894\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 1.376\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 1.462\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 1.437\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 1.540\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 1.568\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 1.554\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 1.584\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 1.607\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 1.656\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 1.606\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 1.586\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 1.653\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 1.044\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 1.141\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 1.160\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 1.204\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 1.308\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 1.322\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 1.320\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 1.308\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 1.320\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 1.330\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 1.400\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 1.391\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 0.820\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 0.902\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 0.969\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 0.968\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 0.994\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 1.045\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 1.104\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 1.048\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 1.113\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 1.128\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 1.070\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 1.147\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 0.645\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 0.755\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 0.745\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 0.787\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 0.785\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 0.835\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 0.866\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 0.890\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 0.872\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 0.930\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 0.950\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 0.933\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 0.481\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 0.543\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 0.583\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 0.663\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 0.619\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 0.644\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 0.710\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 0.694\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 0.699\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 0.754\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 0.718\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 0.788\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 0.397\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 0.443\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 0.503\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 0.497\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 0.510\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 0.562\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 0.583\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 0.534\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 0.581\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 0.638\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 0.586\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 0.655\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 0.344\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 0.396\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 0.406\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 0.417\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 0.432\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 0.470\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 0.451\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 0.462\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 0.494\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 0.489\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 0.514\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 0.489\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 0.288\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 0.295\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 0.338\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 0.333\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 0.423\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 0.355\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 0.346\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 0.396\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 0.408\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 0.393\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 0.388\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 0.427\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 0.226\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 0.228\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 0.240\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 0.288\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 0.316\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 0.306\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 0.349\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 0.337\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 0.376\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 0.330\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 0.347\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 0.360\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 0.213\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 0.208\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 0.203\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 0.240\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 0.248\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 0.285\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 0.287\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 0.289\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 0.287\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 0.263\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 0.261\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 0.273\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 0.182\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 0.175\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 0.178\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 0.202\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 0.197\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 0.227\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 0.206\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 0.225\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 0.224\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 0.233\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 0.238\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 0.254\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 0.151\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 0.152\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 0.191\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 0.190\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 0.186\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 0.234\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 0.215\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 0.181\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 0.222\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 0.186\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 0.191\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 0.225\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 0.136\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 0.137\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 0.160\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 0.150\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 0.175\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 0.156\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 0.165\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 0.189\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 0.181\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 0.204\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 0.220\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 0.187\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 0.118\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 0.102\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 0.113\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 0.146\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 0.159\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 0.152\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 0.143\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 0.173\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 0.164\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 0.173\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 0.142\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 0.149\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 0.072\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 0.122\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 0.115\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 0.114\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 0.164\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 0.121\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 0.159\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 0.135\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 0.135\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 0.149\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 0.136\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 0.165\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 0.095\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 0.099\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 0.116\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 0.104\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 0.102\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 0.139\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 0.136\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 0.155\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 0.143\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 0.127\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 0.119\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 0.128\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 0.109\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 0.067\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 0.093\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 0.103\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 0.130\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 0.112\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 0.130\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 0.113\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 0.147\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 0.106\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 0.109\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 0.111\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 0.079\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 0.072\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 0.074\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 0.099\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 0.090\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 0.093\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 0.087\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 0.103\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 0.111\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 0.096\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 0.105\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 0.129\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 0.065\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 0.063\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 0.084\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 0.071\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 0.096\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 0.076\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 0.082\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 0.106\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 0.118\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 0.118\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 0.104\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 0.125\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 0.067\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 0.090\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 0.074\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 0.094\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 0.101\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 0.088\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 0.093\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 0.055\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 0.107\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 0.086\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 0.086\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 0.125\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 0.068\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 0.078\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 0.083\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 0.089\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 0.082\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 0.105\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 0.092\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 0.106\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 0.078\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 0.106\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 0.101\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 0.093\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 0.061\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 0.062\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 0.073\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 0.086\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 0.081\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 0.086\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 0.099\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 0.089\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 0.086\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 0.087\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 0.090\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 0.085\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 0.060\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 0.061\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 0.080\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 0.060\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 0.060\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 0.064\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 0.058\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 0.064\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 0.059\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 0.069\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 0.104\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 0.092\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 0.044\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 0.049\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 0.050\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 0.044\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 0.042\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 0.063\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 0.057\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 0.072\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 0.099\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 0.068\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 0.057\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 0.088\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 0.041\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 0.047\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 0.054\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 0.062\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 0.070\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 0.092\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 0.083\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 0.087\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 0.058\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 0.090\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 0.087\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 0.074\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 0.058\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 0.061\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 0.032\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 0.035\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 0.063\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 0.059\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 0.074\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 0.083\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 0.061\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 0.050\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 0.070\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 0.081\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 0.034\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 0.053\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 0.051\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 0.060\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 0.059\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 0.056\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 0.074\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 0.061\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 0.087\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 0.072\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 0.072\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 0.089\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 0.054\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 0.051\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 0.046\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 0.053\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 0.058\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 0.060\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 0.063\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 0.066\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 0.082\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 0.076\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 0.061\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 0.055\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 0.036\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 0.043\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 0.053\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 0.029\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 0.039\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 0.048\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 0.061\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 0.051\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 0.072\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 0.056\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 0.049\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 0.060\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 0.042\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 0.043\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 0.043\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 0.040\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 0.058\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 0.041\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 0.039\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 0.033\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 0.069\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 0.081\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 0.055\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 0.052\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 0.057\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 0.033\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 0.037\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 0.039\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 0.050\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 0.024\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 0.041\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 0.052\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 0.055\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 0.043\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 0.043\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 0.043\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 0.038\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 0.017\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 0.028\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 0.029\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 0.031\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 0.047\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 0.042\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 0.051\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 0.039\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 0.040\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 0.071\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 0.047\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 0.034\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 0.025\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 0.055\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 0.039\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 0.037\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 0.041\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 0.031\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 0.035\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 0.049\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 0.048\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 0.055\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 0.018\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 0.032\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 0.028\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 0.028\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 0.044\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 0.037\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 0.033\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 0.041\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 0.034\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 0.046\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 0.058\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 0.052\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 0.028\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 0.036\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 0.032\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 0.026\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 0.030\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 0.038\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 0.027\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 0.039\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 0.042\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 0.043\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 0.026\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 0.049\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 0.045\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 0.031\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 0.039\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 0.034\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 0.031\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 0.042\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 0.047\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 0.042\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 0.048\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 0.043\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 0.035\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 0.029\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 0.043\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 0.051\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 0.038\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 0.029\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 0.038\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 0.035\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 0.035\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 0.027\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 0.021\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 0.039\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 0.028\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 0.030\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 0.043\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 0.036\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 0.032\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 0.040\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 0.044\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 0.017\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 0.028\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 0.035\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 0.028\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 0.033\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 0.026\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 0.035\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 0.021\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 0.027\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 0.020\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 0.021\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 0.021\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 0.012\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 0.038\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 0.044\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 0.043\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 0.042\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 0.045\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 0.017\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 0.014\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 0.025\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 0.032\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 0.039\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 0.052\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 0.037\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 0.037\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 0.030\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 0.052\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 0.050\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 0.035\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 0.030\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 0.023\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 0.029\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 0.020\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 0.019\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 0.025\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 0.032\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 0.028\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 0.015\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 0.025\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 0.030\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 0.033\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 0.030\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 0.045\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 0.034\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 0.018\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 0.015\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 0.035\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 0.040\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 0.041\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 0.047\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 0.034\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 0.037\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 0.021\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 0.038\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 0.032\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 0.030\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 0.042\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 0.032\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 0.036\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 0.034\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 0.041\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 0.040\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 0.041\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 0.028\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 0.014\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 0.014\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 0.024\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 0.030\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 0.034\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 0.040\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 0.027\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 0.027\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 0.033\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 0.031\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 0.036\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 0.036\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 0.022\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 0.022\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 0.031\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 0.039\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 0.037\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 0.058\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 0.036\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 0.032\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 0.027\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 0.055\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 0.042\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 0.035\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 0.043\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 0.027\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 0.025\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 0.023\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 0.025\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 0.035\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 0.019\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 0.026\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 0.038\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 0.052\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 0.031\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 0.023\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 0.019\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 0.023\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 0.025\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 0.024\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 0.024\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 0.025\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 0.027\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 0.023\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 0.029\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 0.035\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 0.034\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 0.022\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 0.014\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 0.016\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 0.030\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 0.027\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 0.026\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 0.022\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 0.032\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 0.020\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 0.037\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 0.032\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 0.036\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 0.033\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 0.030\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 0.017\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 0.018\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 0.017\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 0.026\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 0.016\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 0.022\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 0.037\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 0.028\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 0.039\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 0.014\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 0.028\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 0.033\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 0.022\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 0.026\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 0.029\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 0.026\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 0.026\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 0.023\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 0.027\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 0.016\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 0.022\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 0.018\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 0.028\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 0.027\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 0.014\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 0.032\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 0.030\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 0.033\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 0.021\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 0.029\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 0.015\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 0.022\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 0.020\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 0.028\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 0.022\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 0.019\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 0.044\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 0.018\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 0.025\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 0.042\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 0.046\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 0.033\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 0.026\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 0.024\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 0.025\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 0.031\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 0.012\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 0.031\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 0.018\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 0.032\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 0.025\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 0.014\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 0.017\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 0.029\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 0.026\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 0.012\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 0.011\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 0.024\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 0.016\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 0.016\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 0.011\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 0.023\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 0.032\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 0.016\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 0.030\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 0.019\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 0.024\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 0.014\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 0.018\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 0.017\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 0.013\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 0.014\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 0.024\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 0.031\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 0.027\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 0.029\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 0.030\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 0.025\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 0.026\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 0.023\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 0.015\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 0.011\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 0.021\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 0.016\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 0.029\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 0.017\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 0.024\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 0.024\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 0.014\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 0.022\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 0.029\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 0.020\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 0.019\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 0.010\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 0.028\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 0.022\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 0.017\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 0.028\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 0.019\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 0.019\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 0.037\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 0.026\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 0.026\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 0.016\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 0.014\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 0.009\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 0.021\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 0.026\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 0.034\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 0.031\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 0.037\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 0.023\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 0.028\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 0.023\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 0.015\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 0.019\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 0.015\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 0.022\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 0.032\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 0.020\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 0.014\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 0.021\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 0.018\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 0.030\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 0.019\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 0.016\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 0.010\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 0.009\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 0.016\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 0.026\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 0.024\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 0.021\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 0.009\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 0.009\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 0.028\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 0.014\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 0.017\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 0.021\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 0.006\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 0.020\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 0.014\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 0.025\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 0.041\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 0.019\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 0.018\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 0.024\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 0.016\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 0.014\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 0.012\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 0.021\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 0.018\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 0.036\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 0.029\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 0.030\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 0.022\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 0.024\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 0.017\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 0.023\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 0.010\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 0.023\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 0.008\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 0.012\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 0.011\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 0.018\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 0.014\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 0.010\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 0.013\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 0.019\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 0.019\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 0.006\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 0.007\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 0.013\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 0.014\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 0.025\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 0.018\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 0.013\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 0.010\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 0.012\n",
      "[epoch: 68, i: 10999] avg mini-batch loss: 0.007\n",
      "[epoch: 68, i: 11999] avg mini-batch loss: 0.009\n",
      "[epoch: 69, i:   999] avg mini-batch loss: 0.011\n",
      "[epoch: 69, i:  1999] avg mini-batch loss: 0.007\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 0.003\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 0.003\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 0.011\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 0.009\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 0.012\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 0.006\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 0.017\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 0.007\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 0.008\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 0.014\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 0.014\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 0.011\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 0.012\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 0.012\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 0.011\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 0.021\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 0.021\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 0.036\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 0.039\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 0.026\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 0.029\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 0.007\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 0.006\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 0.020\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 0.009\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 0.017\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 0.009\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 0.020\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 0.011\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 0.015\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 0.018\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 0.018\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 0.019\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 0.018\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 0.017\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 0.013\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 0.022\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 0.012\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 0.013\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 0.022\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 0.017\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 0.017\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 0.018\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 0.020\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 0.018\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 0.029\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 0.017\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 0.022\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 0.016\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 0.014\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 0.011\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 0.031\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 0.014\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 0.039\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 0.011\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 0.016\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 0.012\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 0.020\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 0.014\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 0.009\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 0.008\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 0.023\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 0.013\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 0.021\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 0.025\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 0.017\n",
      "[epoch: 75, i:   999] avg mini-batch loss: 0.016\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: 0.018\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: 0.019\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: 0.017\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: 0.027\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: 0.018\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: 0.027\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: 0.020\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: 0.032\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: 0.028\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: 0.027\n",
      "[epoch: 76, i:   999] avg mini-batch loss: 0.009\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: 0.028\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: 0.028\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: 0.022\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: 0.017\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: 0.011\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: 0.029\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: 0.016\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: 0.011\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: 0.018\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: 0.020\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: 0.013\n",
      "[epoch: 77, i:   999] avg mini-batch loss: 0.017\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: 0.005\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: 0.007\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: 0.008\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: 0.011\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: 0.008\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: 0.013\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: 0.008\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: 0.004\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: 0.013\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: 0.016\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: 0.020\n",
      "[epoch: 78, i:   999] avg mini-batch loss: 0.007\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: 0.012\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: 0.012\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: 0.005\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: 0.009\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: 0.006\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: 0.007\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: 0.008\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: 0.003\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: 0.007\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: 0.008\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: 0.021\n",
      "[epoch: 79, i:   999] avg mini-batch loss: 0.006\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: 0.010\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: 0.013\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: 0.026\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: 0.033\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: 0.023\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: 0.015\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: 0.014\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: 0.014\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: 0.018\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: 0.037\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: 0.029\n",
      "[epoch: 80, i:   999] avg mini-batch loss: 0.019\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: 0.012\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: 0.012\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: 0.009\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: 0.007\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: 0.010\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: 0.017\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: 0.007\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: 0.014\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: 0.032\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: 0.016\n",
      "[epoch: 81, i:   999] avg mini-batch loss: 0.013\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: 0.015\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: 0.011\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: 0.019\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: 0.026\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: 0.016\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: 0.007\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: 0.014\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: 0.005\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: 0.009\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: 0.025\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: 0.025\n",
      "[epoch: 82, i:   999] avg mini-batch loss: 0.012\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: 0.015\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: 0.008\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: 0.024\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: 0.010\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: 0.013\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: 0.007\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: 0.010\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: 0.006\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: 0.017\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: 0.011\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: 0.017\n",
      "[epoch: 83, i:   999] avg mini-batch loss: 0.018\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: 0.016\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: 0.019\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: 0.015\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: 0.019\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: 0.014\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: 0.017\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: 0.012\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: 0.015\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: 0.023\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: 0.015\n",
      "[epoch: 84, i:   999] avg mini-batch loss: 0.034\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: 0.016\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: 0.014\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: 0.015\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: 0.005\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: 0.013\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: 0.027\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: 0.023\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: 0.010\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: 0.005\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: 0.014\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: 0.011\n",
      "[epoch: 85, i:   999] avg mini-batch loss: 0.005\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: 0.007\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: 0.011\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: 0.009\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: 0.014\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: 0.015\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: 0.022\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: 0.010\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: 0.016\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: 0.015\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: 0.010\n",
      "[epoch: 86, i:   999] avg mini-batch loss: 0.004\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: 0.028\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: 0.018\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: 0.014\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: 0.021\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: 0.025\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: 0.014\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: 0.019\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: 0.014\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: 0.019\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: 0.014\n",
      "[epoch: 87, i:   999] avg mini-batch loss: 0.012\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: 0.021\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: 0.018\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: 0.032\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: 0.015\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: 0.024\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: 0.012\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: 0.020\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: 0.031\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: 0.017\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: 0.027\n",
      "[epoch: 88, i:   999] avg mini-batch loss: 0.015\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: 0.020\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: 0.006\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: 0.007\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: 0.013\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: 0.020\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: 0.011\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: 0.019\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: 0.026\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: 0.016\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: 0.011\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: 0.010\n",
      "[epoch: 89, i:   999] avg mini-batch loss: 0.011\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: 0.009\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: 0.010\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: 0.018\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: 0.019\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: 0.011\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: 0.011\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: 0.021\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: 0.012\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: 0.021\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: 0.009\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: 0.019\n",
      "[epoch: 90, i:   999] avg mini-batch loss: 0.031\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: 0.028\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: 0.010\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: 0.016\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: 0.028\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: 0.029\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: 0.015\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: 0.029\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: 0.020\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: 0.036\n",
      "[epoch: 91, i:   999] avg mini-batch loss: 0.008\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: 0.015\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: 0.011\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: 0.016\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: 0.022\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: 0.015\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: 0.009\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: 0.019\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: 0.010\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: 0.015\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: 0.010\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: 0.009\n",
      "[epoch: 92, i:   999] avg mini-batch loss: 0.017\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: 0.006\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: 0.010\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: 0.007\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: 0.007\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: 0.024\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: 0.010\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: 0.013\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: 0.012\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: 0.012\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: 0.013\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: 0.013\n",
      "[epoch: 93, i:   999] avg mini-batch loss: 0.018\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: 0.016\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: 0.016\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: 0.007\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: 0.008\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: 0.008\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: 0.009\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: 0.008\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: 0.016\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: 0.008\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: 0.006\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: 0.009\n",
      "[epoch: 94, i:   999] avg mini-batch loss: 0.011\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: 0.008\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: 0.016\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: 0.012\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: 0.006\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: 0.007\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: 0.009\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: 0.007\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: 0.008\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: 0.017\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: 0.007\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: 0.013\n",
      "[epoch: 95, i:   999] avg mini-batch loss: 0.002\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: 0.007\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: 0.011\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: 0.013\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: 0.014\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: 0.013\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: 0.019\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: 0.018\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: 0.010\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: 0.014\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: 0.022\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: 0.019\n",
      "[epoch: 96, i:   999] avg mini-batch loss: 0.009\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: 0.017\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: 0.010\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: 0.018\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: 0.018\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: 0.016\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: 0.005\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: 0.009\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: 0.015\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: 0.014\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: 0.025\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: 0.027\n",
      "[epoch: 97, i:   999] avg mini-batch loss: 0.023\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: 0.008\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: 0.004\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: 0.012\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: 0.018\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: 0.011\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: 0.022\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: 0.024\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: 0.016\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: 0.020\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: 0.021\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: 0.024\n",
      "[epoch: 98, i:   999] avg mini-batch loss: 0.011\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: 0.017\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: 0.014\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: 0.023\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: 0.022\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: 0.011\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: 0.017\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: 0.018\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: 0.017\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: 0.012\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: 0.011\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: 0.018\n",
      "[epoch: 99, i:   999] avg mini-batch loss: 0.016\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: 0.004\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: 0.006\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: 0.007\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: 0.004\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: 0.006\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: 0.008\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: 0.015\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: 0.007\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: 0.017\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: 0.013\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: 0.010\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "# epochs = 20      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = modelB(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17924a",
   "metadata": {},
   "source": [
    "## Model 2 Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cab5eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAn/ElEQVR4nO3dd3hc1bX38e+a0ahLtiRX3MHY9OpQQ68JLXDTAyEFeHJzk5tKXrghjbRLcm96IYSQEG5CSAgkgRAI1ZCAAzbYYLANNhjcLVf1MjPr/eMcyeMiaWRpNJqj3+d59GjmzDlz1pblpT377LO2uTsiIhI9sXwHICIiuaEELyISUUrwIiIRpQQvIhJRSvAiIhFVlO8AMo0ZM8anT5+e7zBERArGggULNrn72D29NqwS/PTp05k/f36+wxARKRhm9npPr2mIRkQkopTgRUQiSgleRCSilOBFRCJKCV5EJKKU4EVEIkoJXkQkoiKR4H/w8CvMfbk+32GIiAwrkUjwP5u7gieU4EVEdhKJBF+aiNOWTOU7DBGRYSUSCb6kKEZbZzrfYYiIDCuRSPCliThtnerBi4hkikSCL0nE1YMXEdlFJBJ8aSJGu8bgRUR2Eo0EXxSnXT14EZGdRCLBlyRimkUjIrKLSCT40iJdZBUR2VU0EnxC0yRFRHYVkQSvHryIyK6U4EVEIioSCb6kKEZbUkM0IiKZopHgE3E6kmnSac93KCIiw0YkEnxpImhGu3rxIiLdIpHgK4qLAGjpSOY5EhGR4SMSCb6yJEjwTe1K8CIiXSKR4CvCBN/YpgQvItIlEgm+qjRI8M3qwYuIdItEgtcQjYjI7qKR4EuV4EVEdhWNBK8xeBGR3eQ8wZtZ3MyeM7N7c3WOrgSvMXgRkR2Gogf/CWBJLk9QXhzHTEM0IiKZcprgzWwycB5wc47PQ2VJkYZoREQy5LoH/z3gc0CPNQTM7Cozm29m8+vr6/f6RJUlRerBi4hkyFmCN7PzgY3uvqC3/dz9Jnef4+5zxo4du9fnqywp0hi8iEiGXPbgTwQuNLOVwO+A083s/3J1sspS9eBFRDLlLMG7+7XuPtndpwPvBh5x90tzdT6NwYuI7CwS8+BBY/AiIrsqGoqTuPtjwGO5PEdlSRFN6sGLiHSLTg++VBdZRUQyRSbBV5UU0dSR1LJ9IiKhyCT4ytIi3GF9Q1u+QxERGRYik+CPmloDwLNvbM1zJCIiw0NkEvyEUaUAtLSn8hyJiMjwEJkEr0U/RER2FpkEX14cJPiWDiV4ERGIUIIvLopRHI/R3KEhGhERiFCCB6guK2JLU0e+wxARGRYileD3H1fFsg2N+Q5DRGRYiFSC32d0GRs0D15EBIhYgq+rLGZLcwfuuptVRCRSCb62opj2ZJoWXWgVEYlWgq8uTQDQ0NaZ50hERPIvUgm+oiQOQLPuZhURiViCD292UtlgEZGoJfiwXEGz7mYVEYlagg+GaFRwTEQkcglePXgRkS79SvBmVmNmh+UqmIHaMQavHryISJ8J3sweM7NqM6sFFgG/NLPv5D60/tsxi0Y9eBGRbHrwo9y9AbgE+KW7Hw2cmduw9k5XyWAN0YiIZJfgi8xsIvBO4N4cxzMg8ZhRloirBy8iQnYJ/nrgAWC5uz9jZvsCr+Q2rL1XWVpEQ6sSvIhIUV87uPsfgD9kPH8V+LdcBjUQ46pKqG9qz3cYIiJ5l81F1m+FF1kTZvawmW0ys0uHIri9Mb66VCWDRUTIbojm7PAi6/nAamAWcHVOoxqA8dUlbGhQD15EJJsEnwi/vxW43d235DCeARtbVcrm5naSqXS+QxERyatsEvw9ZrYUmAM8bGZjgWE7BjK+ugR32KS1WUVkhOszwbv7NcDxwBx37wSagYtyHdjeGl9VCqBxeBEZ8fqcRWNmCeAy4GQzA5gL3JjjuPba+GoleBERyCLBAz8lGIf/Sfj8snDbFbkKaiDGVZcAsKFRF1pFZGTLJsG/yd0Pz3j+iJktylVAA1VXUUzMoF49eBEZ4bK5yJoys/26noR3sg7bco1F8RhjKjVVUkQkmwR/NfBoWFVyLvAI8Jm+DjKzUjN72swWmdmLZvaVgQabrbTDHfNXkU77UJ1SRGTYyaZUwcNmtj8wGzBgqbtn0z1uB05396bwQu0/zOxv7j5vYCH3bVNYquD1LS3MGFOR69OJiAxLPSZ4M7ukh5f2MzPc/a7e3tjdHWgKnybCryHpUl985CTufm4N7clhO5IkIpJzvfXgL+jlNQd6TfAAZhYHFgAzgR+7+7/2sM9VwFUAU6dO7ests3Lh4ftw93NraOlQgheRkavHBO/uHxzom7t7CjjCzEYDd5vZIe6+eJd9bgJuApgzZ86g9PDLi4OVnVqV4EVkBBuSRbfdfRvwGHDuUJyva2Un9eBFZCTLWYI3s7Fhzx0zKyNY5m9prs6XqSzswX/13peG4nQiIsNSLnvwEwmmVz4PPAM86O5DsuTfjDEVHDKpmje2tLC9pXMoTikiMuxkcycrZnYCMD1zf3f/dW/HuPvzwJEDCW5vxWPGR07Zj4/99jnWN7QxqjzR90EiIhGTTbGx24D9gIXsuIPVgV4TfL6NLisGYHurevAiMjJl04OfAxwUzmsvGKPKgl67EryIjFTZjMEvBibkOpDBpgQvIiNdb3ey3kMwFFMFvGRmTxOUHwDA3S/MfXh7r7Q4+NvV2qmpkiIyMvU2RPM/QxZFDpQmgqmS7UrwIjJC9XYn61wAM5sBrHP3tvB5GTB+aMLbe6VFQYJvU4IXkREqmzH4PwDpjOepcNuwlogbMYO2znTfO4uIRFA2Cb7I3Tu6noSPi3MX0uAwM0oTcfXgRWTEyibB15tZ9wVVM7sI2JS7kAZPaSJOm0oGi8gIlc08+I8AvzGzH4XPVxMsvD3slRbFWL9dS/eJyMiUTYJPu/txZlYJmLs3hhdeh72jptXwyNKNpNJOPGb5DkdEZEhlM0TzRwB3b3L3xnDbnbkLafCcOnscLR0pzvruXArsRlwRkQHr7UanA4CDgVG7LN9XDZTmOrDBcOikUQC8Wt/MotXbOWLK6PwGJCIyhHobopkNnA+MZufl+xqBK3MY06CZNb6y+/G2lo5e9hQRiZ7ebnT6M/BnMzve3Z8awpgGjZkx9+pTOeXbj7GxURdbRWRkyeYi63Nm9h8EwzXdQzPu/qGcRTWIxlUFIX/uzue58PB9uksYiIhEXTYXWW8jqCZ5DjAXmEwwTFMQupbvA3hyRUFM3xcRGRTZJPiZ7v4FoNndbwXOAw7NbVi50ZHUTBoRGTmySfBdBdW3mdkhwCiC5fsKxv2fPAmA+iaNw4vIyJHNGPxNZlYDfAH4C1AZPi4Yk0aXASodLCIjS58J3t1vDh/OBfbNbTi5kYgHH1Q6UqosKSIjR59DNGZWZ2Y/NLNnzWyBmX3PzOqGIrjB0pXgOzUGLyIjSDZj8L8DNgL/BrydoJLkHbkMarDFY0Y8ZiTT6sGLyMiRzRh8rbt/NeP518zsbTmKJ2cScdMQjYiMKNn04B81s3ebWSz8eifw11wHNtgS8ZiGaERkROmt2Fgj4IABnya44ckI/ig0AV8aigAHS3E8Rqd68CIygvRWi6ZqKAPJtUQ8RktHCnfHTLXhRST6shmi6WZmX85RHDmXKDL++OxqLvzRP/MdiojIkOhXggcu7HuX4WnVllYAXliznU/dsZDtLZ19HCEiUtj6m+AjMbZx93Nr+NnjK/IdhohITvU3wR+dkyiGwC8/+CZmjKnofl6v+vAiEnG9zaL5nLt/y8x+SDCbpms7AO7+n7kPb/CcNnscp80ex/RrghmeNRXFeY5IRCS3ervRaUn4ff5QBDJU/vOM/fnBw69QHO/vhxcRkcLS2zTJe8Lvtw5dOLn36bNmcePcFSTTuulJRKKtz1IFZjYL+CxBDfju/d399D6OmwL8mmA1qDRwk7t/fyDBDpa4GSnVpRGRiMumFs0fgBuBm4H+FFRPAp9x92fNrApYYGYPuvtLexHnoCqKGbqpVUSiLpsEn3T3n/b3jd19HbAufNxoZkuASUDeE3w8rh68iERfNlca7zGzj5rZRDOr7frqz0nMbDpwJPCvvQlysMXNNAYvIpGXTQ/+8vD71RnbnCxXdzKzSuCPwCfdvWEPr18FXAUwderUbN5ywOIxI+1K8CISbdks2Tdjb9/czBIEyf037n5XD+9/E3ATwJw5c4Yk6xbFjGRKCV5Eoq23G51Od/dHzOySPb3eU8LOON6AXwBL3P07AwtzcMViRlN7khvuX8rHT59JeXE2H2RERApLb5ntFOAR4II9vOZArwkeOBG4DHjBzBaG2/7L3e/rb5CDrShm/G3xegB++tgKnvjcaUypLc9zVCIig6u3G52+FH7/4N68sbv/g2FanCwe2zms389fxWfOnp2naEREciObG51GA+9n9xudCqoWTaYV9c07PddarSISRdkMPt8HzANeILgjNXLSmjIpIhGUTYIvdfdP5zySPDjn4PH8c/lmlN9FJIqyudHpNjO7ciA3Og1XP7tsDmaQUoYXkQjKpgffAXwb+Dw76sJnfaPTcPS1tx1CIh5caNVNTyISVdkk+E8DM919U66DGSqXHjet+3FQWVIJXkSiJ5shmheBllwHki8x9eBFJKKy6cGngIVm9ijQvZBpIU+TzKQevIhEVTYJ/k/hVyTFw9rwjW2dJFOutVpFJDKyKTYWqSX7dhWLQdqdk7/1KFtbOnnnnMl86+2H5zssEZEBG/ErT3cN0Wxt6QTg9/NX5zkiEZHBMeITfCxmbG/tzHcYIiKDbsQn+LgZc1+u735eVarSwSISDXuV4MNVmCIhZjtXljzv0Il5ikREZHDtbQ9+WJYB3huxjNLBE6pLNSdeRCJjrxK8u/9ssAPJl1Q6KJD52bNnhWUL8hyQiMggyaYe/J4qSW4HFrj7wkGPaIi1dKQAqCpNYKbSwSISHdn04OcAHwEmhV9XAacCPzezz+UutKGxemsrAGWJuAqPiUikZDNlpA44yt2bAMzsS8CdwMnAAuBbuQtv6DS0dRIzI6X8LiIRkU0PfipByeAuncA0d28lozZNoXvH0VOIGd09+I0Nbbh68yJSwLJJ8L8F5pnZl8Le+z+B282sAngpp9ENoVHlCWJmpNPOhoY2jvnGw3zzb0vzHZaIyF7rM8G7+1eBK4FtBBdXP+Lu17t7s7u/L8fxDamuMfjVW4PqyH96bk2eIxIR2XvZzKL5PnCHu39/COIZcvOuPYNUOBRjFlSW3NwUjEjVlKuypIgUrmwusj4LXGdms4C7CZL9/NyGNXQmjCrtfhyPwTMrt/DQkg0A1FUqwYtI4cpmiOZWd38rcAzwMnCDmb2S88jyIGY7Fx4bU1mSx2hERAamP5W1ZgIHANOJ0MXVTBbWpamtKA4WAtEsGhEpYH324M2sq8d+PcH6rEe7+wU5jywP4mFZmoP3qWZ0WUJ3tYpIQcumB/8acLy7b8p1MPnWVVmyqrSIjQ26q1VECls2S/bdaGY1ZnYMUJqx/fGcRpYHXZUlq0oSxMK1WkVEClU20ySvAD4BTAYWAscBTwGn5zSyfAg77HWVxcRj6E5WESlo2dzJ+gngTcDr7n4acCRQ3/shhWldQ1B4bFpdeViXZkfZAi3rJyKFJpsE3+bubQBmVuLuS4HZuQ0rP1ZtCRL8YZNHBwk+7bR0JDnmGw9zxv8+lt/gRET6KZuLrKvNbDTwJ+BBM9sKrM1lUPm279gK4jHDHeZ87SEANjV19HGUiMjwks1F1ovDh182s0eBUcD9OY0qz0qK4sQMlq5v7F4QRESk0PTnRifcfW6uAhluYmZsaopMNWQRGYH2dtHtPpnZLWa20cwW5+ocuRSP7byu+BFTRucnEBGRvdSvHnw//Qr4EfDrHJ5jUN36oWPY0hz02rtuegIwg+qyRL7CEhHZKzlL8O7+uJlNz9X758Ips8Z2P+666amiOM7sCVUqWyAiBSdnQzTZMrOrzGy+mc2vrx8+0+u7+u/T6oIZNSkleBEpMHlP8O5+k7vPcfc5Y8eO7fuAIfL65mYAJteUdd/0lEylufoPi1i4alt+gxMRyULeE/xwtS28c/X9x08PlvJLOwtXbeMPC1bz0f9bkOfoRET6pgTfg20tQYKvqUh014ZfvGY7AKXF8XyGJiKSlVxOk7ydoCjZbDNbbWYfztW5cqm2opiYBT34L98TrHMypkIrPYnI8JfLWTTvydV7D6Wa8mB1p8xiY0Vx6+UIEZHhQUM0fShNxImZsXJzS/c2zagRkUKgBN+DMZXF3Y/jGT+lsVUlSvAiUhByeSdrQXvwU6fQ1J4Edi5bMGNMBZ3hUk+3PrmSorjxvmOn5SVGEZHeKMH3oKaimJqKoBefzli6r7w4ztbmFO7Ol/7yIoASvIgMSxqiycL9L64H4OIjJxE3I5l27nthfZ6jEhHpnRJ8P3zghOndZQt++/Tr+Q5HRKRXGqLph66Vnpo7kixd3pjvcEREeqUefD9UFBcRi1n32q0AB0yoymNEIiI9U4Lvh1jMKMqYUVOaiHVPmZzztQc5+7sjZsErESkASvBZuO68Azl6Wg0A8YyFQKbXVZBKO/94ZRObmjp4eUNTvkIUEdmNEnwWrjhpX/747ycETzKqFMwaX0Uy7SzboPF4ERl+lOD7aXtYZfL6iw4mEY+RTKX56r0v5TkqEZHdKcH30+bmDgDGVZVQFDPWbm/rfq00oR+niAwfykj9tCVM8GOrSojvUlWyprx4T4eIiOSFEnw/bW5qB2BcVelOM2oAOlPBjJorbn2G2+bpRigRyS8l+H5q7kgBu1eVfP/x00im0/x54RoeWrKRL/xpcb5CFBEBlOD77csXHMTEUaWUJuJsCnvznzt3NkWxGNtaOvnE7xYCUF2qm4RFJL+U4PvpAyfO4KlrzwBgY2OQ4A+dNGq3VZ7Ki4ME39aZ2mk1KBGRoaJu5gBsbAgS/MRRO4/Hn3vwBO5/cT3Tr/lr97aV/33ekMcnIiObEvwAJMJe+4RRZTS0Bb30i4+cRG2FZtOISP5piGYAfvXBY/jGxYdSWVLEK2GZgkuOmkRJ0e4/1nmvbmb6NX/lqRWbhzpMERmhlOAHYPqYCt577FQA6sMLruOrSyneJcEn4sa7b5oHwHt+Pm9ogxSREUsJfpB84byDmDGmgml15ZQUxbu3zxxX2T0/XkRkKCnBD5LTDhjHo589lZKiOG2dwVz5k/YfwyVHTdpt301N7by+uXmoQxSREUYJPgf2GV0KBEv8Zfbmrzp5XwDmfO0hTvn2Y3z7gaV5iU9ERgYl+By4+MjJ3P/JkzjjwPHd4/GjyxNMrS3fab8fP7qCVNpZtGob37xvCe4ayhGRwaNpkjlQXBTjgAnVAJTEgwQ/tbac8uIdvfmpteW8saWFG+5fyk2PvwrAivpmqsuKOG32OC44fB8a2jppaU8xYVTp0DdCRAqeevA5NmtCFXUVxXzm7Nk7JfiPnz4ToDu5Azy0ZAN3PbuGj9/+HBsb2njL957guG8+POQxi0g0qAefY0dMGc2CL5wFQGNbJ8fMqOW4fesYVZbo9bgb7l/Gmm3B4t7bWzt5bVMz+4+rpKJE/2Qikh0bTuO+c+bM8fnz5+c7jCGxaksLZ/zvXA6bPIqPn7E/l9/ydI/7FsdjdKTSjK8uoaK4iET4/HdXHceTKzZx/mH7kIjH6EylKYoZZtbje4lItJjZAnefs6fX1B3Mkym15Tx17elUlyVYuSmYMnnZcdO4+KhJXPKTJ4GgJHF9YzsdqTQAGxragfbu9zj2G8Hwzf2L13PZcdO59Bf/4vLjp/GViw4B4Lk3toaF0GJsb+2kurQIM2N7aydVJUXEYvpDIBJl6sEPE69tamZKTRnNHSne+/N5nHHgeC44bCJnffdxAGaMqeC1TdnNnX/xK+fw7QeW8asnV3LtWw7g4aUbefq1LVx/0cEcMKGad/7sKa48aQYHTKhmxtgKjppas9t7LN/YxH5jK/RpQGSY660HrwQ/jLUnU8y+7n5qyhPc9uFjOf+H/wDgJ+87io/+5lkAzj9sIvc+v25A5zlhvzqeXLGZtxwygY+csh9lxXHO/u7jfOKM/dne2smvnlzJj997FF/482Km1Jbz8/cfTUt7iuaOJJNHl9PckeT7D73CWw+byIn71bFycwszx1Xudp4V9U1Mqy2nKK5r+yKDRQm+gC1b30hNeYLWzhSnfPsxDp8ymj//x4nc/MSrJNPOpcdN45AvPQAEPfeDw8eDoWsqZ3+cd9hE/vr8On57xbEsr2/i+w+9wi0feBOvbmriU3cs4kMnzuCLFxxEa0eKsnBW0ZPLN/HHZ9fw9YsPoaQoRlN7kqrSBE3tSdo6U4ypLBm0NvVHMpVmzbZWxlaVdNf3Fxlu8pbgzexc4PtAHLjZ3f+7t/2V4Hvm7vx07gredsQk9hldttNrP3/8VdqTKT52+v48umwjL61t4KOn7sd7fj6Pea9u4bvvOpzn3tjGfS+sY1NTsGj4lSfN4KIjJnH1nc+zZF0Dj3zmFL5x3xIeWrJxyNp05UkzSMRj/OSxFbu9dt6hE/nrC8Enk/95x+H8/plVPL1yC/uPq2T99jaaOpLMqKvg42fM5ImXN9HameLS46ZRmogxe0I1tz65kpWbmmlPpvnLorVMrS3nno+9mUSRsWZrKzfOfZUptWX87YX1nDp7LKu3tXLghCrOPWQCL65toCwR51sPLGP5xqBKaG1FMT96z5FcfefzfPOSQzl51liWb2xi4qhSSopi3PXsGo6aVsO46hJ+8cRrnH/YRGaOq8TMcHfWbW9jTGUJ8Zhx3Z8Ws2JjE1+84CCm1JZ3Xw9Jp52mjiRVJcG1kvZkaqc7od0dMyOZSrNw1TaOnlbD3JfrOWBCdXeJ6njMiGdcW+lMpfnNvNd5+5wptHak2NrSwazxVVn/G63f3saSdQ2cdsC4XvdbtGobFSVxZo7b+b0zr/30ZXtLJw1tnZjB5JpyOlNp3NmteN+u1m1vZXxVaZ/XlJrak5Ql4rj7Hj9FdiSDSQqxmLGivol9RpV1d0KGs7wkeDOLAy8DZwGrgWeA97j7Sz0dowSfe8+9sZUDJ1ZTmgh+cZOpNMm0U5qI05FMs257K5ubO3jujW38+qmVjCpLsN/YSpaub+TWD76JlZtbeOfPnqK4KEZHMrj4e915B3LREZP42l9f4tTZY9na3Mn19/b4zxwJ46pKulf0yqWimJFMZ/9/NBE3yhJxxleX8kr4x2lPzODoqTVUlBTRnkzR1J7kw2+ewZFTavjH8k20J9N0JNPccP+Ochp1FcWce8gEVm1t5bVNTRwzvY4/Prt6p/c9eJ9qXlzbwOzxVSzb0Ljbed977FTecfRkXlzbwPKNTbz/+Gms397GdX9ezKv1O64xff3iQ/j83cG6xtPqyilLxFm6vpHSRIzLT5jOG5tbaO1MsWZra3c79x1bwSfPnEV9Yzt3Pbualo5Uj9etptWV8/rmFv7z9JnEYsaNc1fQ1pnebb/aimJOnT2WMZUl3PfCOq4+ZzYNbUn2HVNBU3uStdtaOWzyKJ5asZmjptXw5PLNPPDielZvbeX9x0/jiVc2EYsF17Q6kmnSDhXFcc45ZALHTK/lxJlj+NnjK7jizfsyfUxFL/+yPctXgj8e+LK7nxM+vxbA3b/Z0zFK8IVhwetbmDGmktJEjPrGdqbV7fyLmU47T70a1L0/ceYYALY0d7CluYOOZJpb/vkalx03jYt+/E8A5l93Jv94ZRPrG9oYXZbgmrte4JRZY/nQm2dw3/PrWLhqGwdOrGJMZQlT68o588DxPLx0I01tSe5csIpJNeWs2NjUfd9Al2l15cTM+Oip+/HsG9u4/ek3gGC93IqSImJm7Du2gje2tPDps2Zx21Ovs2j1Ns4+aEJ3LGMqS2juCHp+f1iwmqm15cRjxvjqElZvbWX11h3nrCiOdy/KDkFC3Nzcscef4fS6clZu7nv4qysRSbTFY8YLXz57r4YC85Xg3w6c6+5XhM8vA45194/tst9VwFUAU6dOPfr111/PSTwycqTTzvqGYEgk7d79aQWCYY5k2imKGan0zh/V2zpTdKTSVJf2fhNapoa2zp32d3c6UmnSaSgrjnefLxWeM/N8rR0pGto6KU3EWbuttfsPTltn0KOeOKqMzlSaNVtbmVJbTltn0CM9YEIV8Zjx9GtbOHzKaOIxwx1SaWdTUzsbG9sYV1XK8vom6hvbOeegCZQkYmxsaGdqXTnL1jfyxCv1zBxXSWtHCjOoLEmwZF0D9U3tLFnXwJFTRnPMjDpmjqtka0sHt817ncMnj+K02eMoLY6zblsbs8YHQ1Cvb26mI5mmvrGd+qZ2jpxSw5TaMsyM1o4UyzYEPe9kynls2UZqK0rY1NROZyrNluYOKkqKiMeMK0/al7Q77ck0D720gZKiGEdOrWHttlbuWbSWMw4cz7INjbR3prjkqMlMHF3KlqYOxleXcu/za+lIpfnXq1t488wxTKktZ9n6Bt40o5bpdRVsaGhjXHUpW5s7KC+Oc+/z6zhiymga25Ik02lKE3EaWjuZVFPGvmMqeWNLCy9vaKQoZizb0EhrZ4rJo8s4bPJo1mxrZe22Vra3duIeTGd+aMkGptWVc+lx02hoTbKhoY3trZ3sP66SF9ZsZ5/RZZx54HiWb2yioiTOluYO7lm0jphBPG7MmVbLWQeN36vf93wl+HcA5+yS4I9x94/3dIx68CIi/dNbgs/lfLXVwJSM55OBtTk8n4iIZMhlgn8G2N/MZphZMfBu4C85PJ+IiGTI2eRed0+a2ceABwimSd7i7i/m6nwiIrKznN694e73Affl8hwiIrJnumdcRCSilOBFRCJKCV5EJKKU4EVEImpYVZM0s3pgb29lHQNsGsRw8ikqbYlKO0BtGa6i0paBtGOau4/d0wvDKsEPhJnN7+lurkITlbZEpR2gtgxXUWlLrtqhIRoRkYhSghcRiagoJfib8h3AIIpKW6LSDlBbhquotCUn7YjMGLyIiOwsSj14ERHJoAQvIhJRBZ/gzexcM1tmZsvN7Jp8x9MXM5tiZo+a2RIze9HMPhFurzWzB83slfB7TcYx14btW2Zm5+Qv+t2ZWdzMnjOze8PnhdqO0WZ2p5ktDf9tji/gtnwq/N1abGa3m1lpobTFzG4xs41mtjhjW79jN7OjzeyF8LUfWDarfg9NW74d/o49b2Z3m9nojNcGvy3uXrBfBGWIVwD7AsXAIuCgfMfVR8wTgaPCx1UEC5MfBHwLuCbcfg1wQ/j4oLBdJcCMsL3xfLcjoz2fBn4L3Bs+L9R23ApcET4uBkYXYluAScBrQFn4/PfABwqlLcDJwFHA4oxt/Y4deBo4HjDgb8BbhklbzgaKwsc35Lothd6DPwZY7u6vunsH8DvgojzH1Ct3X+fuz4aPG4ElBP8pLyJIMoTf3xY+vgj4nbu3u/trwHKCduedmU0GzgNuzthciO2oJvjP+AsAd+9w920UYFtCRUCZmRUB5QQrqRVEW9z9cWDLLpv7FbuZTQSq3f0pDzLkrzOOGTJ7aou7/93dk+HTeQQr3UGO2lLoCX4SsCrj+epwW0Ews+nAkcC/gPHuvg6CPwLAuHC34dzG7wGfA9IZ2wqxHfsC9cAvw+Gmm82sggJsi7uvAf4HeANYB2x3979TgG3J0N/YJ4WPd90+3HyIoEcOOWpLoSf4PY1FFcS8TzOrBP4IfNLdG3rbdQ/b8t5GMzsf2OjuC7I9ZA/b8t6OUBHBR+mfuvuRQDPBUEBPhm1bwvHpiwg+5u8DVJjZpb0dsodtw6ItWegp9mHfJjP7PJAEftO1aQ+7DbgthZ7gC3JhbzNLECT337j7XeHmDeHHMcLvG8Ptw7WNJwIXmtlKgqGx083s/yi8dkAQ22p3/1f4/E6ChF+IbTkTeM3d6929E7gLOIHCbEuX/sa+mh1DH5nbhwUzuxw4H3hfOOwCOWpLoSf4glvYO7wC/gtgibt/J+OlvwCXh48vB/6csf3dZlZiZjOA/QkuuuSVu1/r7pPdfTrBz/0Rd7+UAmsHgLuvB1aZ2exw0xnASxRgWwiGZo4zs/Lwd+0Mgus8hdiWLv2KPRzGaTSz48KfwfszjskrMzsX+H/Ahe7ekvFSbtoy1FeWc3Cl+q0EM1FWAJ/PdzxZxPtmgo9YzwMLw6+3AnXAw8Ar4ffajGM+H7ZvGXmYDZBFm05lxyyagmwHcAQwP/x3+RNQU8Bt+QqwFFgM3EYwM6Mg2gLcTnDtoJOg9/rhvYkdmBO2fwXwI8K79odBW5YTjLV3/d+/MZdtUakCEZGIKvQhGhER6YESvIhIRCnBi4hElBK8iEhEKcGLiESUErzklJldaH1U+TSzfczszh5ee8zMsl6M2MyOMLO3ZrFfUxb79Bn7Ho75lZm9vT/H9PJe7wnveMzcVmdBNdImM/vRLq/tsepgOLf6jnD7v8ISGV3HXB5WaXwlvAFHIkQJXnLK3f/i7v/dxz5r3X1QkiLBfPY+E3w2sok9x84F7t9lWxvwBeCze9j/p8BVBDfJ7B8eD8H8663uPhP4LkEVQ8ysFvgScCxBgbEvZZbilcKnBC97xcymh3Wtb7ag7vhvzOxMM/tn2Bs8JtzvA109zbB3+wMze9LMXu3q6YbvtbiX010aHrM4432PCbc9F36fHd7NfD3wLjNbaGbvMrNKM/tl2LN93sz+LaMNXzezRWY2z8zG76GN2cRuZvYjM3vJzP7KjkJYXT3quWa2wMweMLOJZjbKgnrfs8N9bjezK/dwbiP4Y/Vs5nZ3b3b3fxAk+sz9e6s6mFmN8U7gjPD9zwEedPct7r4VeJAdfxQkApTgZSBmAt8HDgMOAN5LcKfuZ4H/6uGYieE+5wPZ9o4r3P0E4KPALeG2pcDJHhQH+yLwDQ9KRn8RuMPdj3D3Owh6u9vd/VB3Pwx4pOs9gXnufjjwOLBbks0y9ouB2cCh4XucAN31hn4IvN3djw7j/rq7bwc+BvzKzN4N1Lj7z/dwriOBRZ79nYi9VR3srlToQana7QR3hxZCJUkZgKJ8ByAF7TV3fwHAzF4EHnZ3N7MXgOk9HPMnd08DL+2p19yD2yGor21m1RasglMF3Gpm+xOUfkj0cOyZBLVyCN9ja/iwA7g3fLwAOCuLOPYU+8nA7e6eAtaaWdcfkNnAIcCD4VB4nOC2ddz9QTN7B/Bj4PAeznUuO0rJZqO3qoMFW3VRBkY9eBmI9ozH6YznaXruPGQes1uCCYdTFprZfRmbd006DnwVeNTdDwEuAEp7OJ/t4XiAzozecaqXeLOJfU/vb8CL4SeJI8JPEGcDmFkMOBBoBWp7ONfZwN+ziKlLb1UHuysVWrAIyCiChSgKoZKkDIASvAwr7v7BMCFmXih9F4CZvZlguGU7QZJaE77+gYx9Gwl6913+TjAkQvgeg30R8XGCKoDxcBz8tHD7MmCsmR0fnjdhZgeHr32KoMLje4BbwuGcbmY2imBZt83ZBuG9Vx3MrMb4doLKnw48AJxtZjXhz+XscJtEhBK8FIKtZvYkcCPBjBAI1un8ppn9k2D4o8ujwEFdF1mBrwE14QXaRexIwIPlboIqhy8QzGKZC8GyfwTJ9IbwvAuBE8xsFnAF8Bl3f4LgD8R1u7znWcBDPZ3Qghr83wE+YGarzeyg8KV/J1g+cTlB5cGuIZ5fAHVmtpxgDd1rwhi3EHwSeib8uj7cJhGhapIiw4yZ3Qzc7O7z8h2LFDYleBGRiNIQjYhIRCnBi4hElBK8iEhEKcGLiESUEryISEQpwYuIRNT/B5iQpJlyZzgSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccd94f",
   "metadata": {},
   "source": [
    "## Evaluate Model 2 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d70d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = modelB(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220ec71",
   "metadata": {},
   "source": [
    "## Evaluate Model 2 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5011e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 70 %\n",
      "Accuracy of aquarium_fish : 62 %\n",
      "Accuracy of  baby : 41 %\n",
      "Accuracy of  bear : 16 %\n",
      "Accuracy of beaver : 38 %\n",
      "Accuracy of   bed : 35 %\n",
      "Accuracy of   bee : 37 %\n",
      "Accuracy of beetle : 38 %\n",
      "Accuracy of bicycle : 61 %\n",
      "Accuracy of bottle : 69 %\n",
      "Accuracy of  bowl : 38 %\n",
      "Accuracy of   boy : 32 %\n",
      "Accuracy of bridge : 49 %\n",
      "Accuracy of   bus : 61 %\n",
      "Accuracy of butterfly : 36 %\n",
      "Accuracy of camel : 31 %\n",
      "Accuracy of   can : 49 %\n",
      "Accuracy of castle : 58 %\n",
      "Accuracy of caterpillar : 17 %\n",
      "Accuracy of cattle : 39 %\n",
      "Accuracy of chair : 74 %\n",
      "Accuracy of chimpanzee : 59 %\n",
      "Accuracy of clock : 48 %\n",
      "Accuracy of cloud : 61 %\n",
      "Accuracy of cockroach : 60 %\n",
      "Accuracy of couch : 29 %\n",
      "Accuracy of  crab : 47 %\n",
      "Accuracy of crocodile : 18 %\n",
      "Accuracy of   cup : 73 %\n",
      "Accuracy of dinosaur : 43 %\n",
      "Accuracy of dolphin : 38 %\n",
      "Accuracy of elephant : 34 %\n",
      "Accuracy of flatfish : 24 %\n",
      "Accuracy of forest : 46 %\n",
      "Accuracy of   fox : 52 %\n",
      "Accuracy of  girl : 27 %\n",
      "Accuracy of hamster : 43 %\n",
      "Accuracy of house : 47 %\n",
      "Accuracy of kangaroo : 26 %\n",
      "Accuracy of keyboard : 68 %\n",
      "Accuracy of  lamp : 43 %\n",
      "Accuracy of lawn_mower : 68 %\n",
      "Accuracy of leopard : 51 %\n",
      "Accuracy of  lion : 46 %\n",
      "Accuracy of lizard : 19 %\n",
      "Accuracy of lobster : 34 %\n",
      "Accuracy of   man : 26 %\n",
      "Accuracy of maple_tree : 50 %\n",
      "Accuracy of motorcycle : 82 %\n",
      "Accuracy of mountain : 55 %\n",
      "Accuracy of mouse : 28 %\n",
      "Accuracy of mushroom : 36 %\n",
      "Accuracy of oak_tree : 63 %\n",
      "Accuracy of orange : 65 %\n",
      "Accuracy of orchid : 46 %\n",
      "Accuracy of otter : 22 %\n",
      "Accuracy of palm_tree : 69 %\n",
      "Accuracy of  pear : 49 %\n",
      "Accuracy of pickup_truck : 64 %\n",
      "Accuracy of pine_tree : 37 %\n",
      "Accuracy of plain : 54 %\n",
      "Accuracy of plate : 57 %\n",
      "Accuracy of poppy : 64 %\n",
      "Accuracy of porcupine : 39 %\n",
      "Accuracy of possum : 36 %\n",
      "Accuracy of rabbit : 22 %\n",
      "Accuracy of raccoon : 41 %\n",
      "Accuracy of   ray : 39 %\n",
      "Accuracy of  road : 62 %\n",
      "Accuracy of rocket : 69 %\n",
      "Accuracy of  rose : 51 %\n",
      "Accuracy of   sea : 80 %\n",
      "Accuracy of  seal : 19 %\n",
      "Accuracy of shark : 30 %\n",
      "Accuracy of shrew : 34 %\n",
      "Accuracy of skunk : 64 %\n",
      "Accuracy of skyscraper : 74 %\n",
      "Accuracy of snail : 34 %\n",
      "Accuracy of snake : 38 %\n",
      "Accuracy of spider : 41 %\n",
      "Accuracy of squirrel : 24 %\n",
      "Accuracy of streetcar : 40 %\n",
      "Accuracy of sunflower : 63 %\n",
      "Accuracy of sweet_pepper : 30 %\n",
      "Accuracy of table : 41 %\n",
      "Accuracy of  tank : 48 %\n",
      "Accuracy of telephone : 51 %\n",
      "Accuracy of television : 62 %\n",
      "Accuracy of tiger : 36 %\n",
      "Accuracy of tractor : 39 %\n",
      "Accuracy of train : 52 %\n",
      "Accuracy of trout : 60 %\n",
      "Accuracy of tulip : 37 %\n",
      "Accuracy of turtle : 20 %\n",
      "Accuracy of wardrobe : 75 %\n",
      "Accuracy of whale : 48 %\n",
      "Accuracy of willow_tree : 42 %\n",
      "Accuracy of  wolf : 45 %\n",
      "Accuracy of woman : 14 %\n",
      "Accuracy of  worm : 58 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = modelB(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f6819",
   "metadata": {},
   "source": [
    "## Construct Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4749790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalEnsemble(\n",
       "  (modelA): BaselineConvolutionModel(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (modelB): ConvutionModelAddedLayersBatchNorm(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_seven): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc5): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc6): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalEnsemble(modelA,modelB)\n",
    "\n",
    "# Freeze these models\n",
    "for param in modelA.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in modelB.parameters():\n",
    "    param.requires_grad_(False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06ab13",
   "metadata": {},
   "source": [
    "###  Evaluate Ensemble on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d168d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 48 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361094ca",
   "metadata": {},
   "source": [
    "### Evaluate Ensemble Across Classes on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "155dbe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 73 %\n",
      "Accuracy of aquarium_fish : 67 %\n",
      "Accuracy of  baby : 45 %\n",
      "Accuracy of  bear : 24 %\n",
      "Accuracy of beaver : 38 %\n",
      "Accuracy of   bed : 55 %\n",
      "Accuracy of   bee : 56 %\n",
      "Accuracy of beetle : 51 %\n",
      "Accuracy of bicycle : 60 %\n",
      "Accuracy of bottle : 75 %\n",
      "Accuracy of  bowl : 33 %\n",
      "Accuracy of   boy : 25 %\n",
      "Accuracy of bridge : 51 %\n",
      "Accuracy of   bus : 52 %\n",
      "Accuracy of butterfly : 29 %\n",
      "Accuracy of camel : 42 %\n",
      "Accuracy of   can : 50 %\n",
      "Accuracy of castle : 64 %\n",
      "Accuracy of caterpillar : 31 %\n",
      "Accuracy of cattle : 31 %\n",
      "Accuracy of chair : 76 %\n",
      "Accuracy of chimpanzee : 60 %\n",
      "Accuracy of clock : 53 %\n",
      "Accuracy of cloud : 60 %\n",
      "Accuracy of cockroach : 73 %\n",
      "Accuracy of couch : 27 %\n",
      "Accuracy of  crab : 41 %\n",
      "Accuracy of crocodile : 31 %\n",
      "Accuracy of   cup : 73 %\n",
      "Accuracy of dinosaur : 46 %\n",
      "Accuracy of dolphin : 47 %\n",
      "Accuracy of elephant : 33 %\n",
      "Accuracy of flatfish : 46 %\n",
      "Accuracy of forest : 51 %\n",
      "Accuracy of   fox : 44 %\n",
      "Accuracy of  girl : 20 %\n",
      "Accuracy of hamster : 49 %\n",
      "Accuracy of house : 37 %\n",
      "Accuracy of kangaroo : 30 %\n",
      "Accuracy of keyboard : 77 %\n",
      "Accuracy of  lamp : 41 %\n",
      "Accuracy of lawn_mower : 65 %\n",
      "Accuracy of leopard : 51 %\n",
      "Accuracy of  lion : 48 %\n",
      "Accuracy of lizard : 21 %\n",
      "Accuracy of lobster : 29 %\n",
      "Accuracy of   man : 28 %\n",
      "Accuracy of maple_tree : 64 %\n",
      "Accuracy of motorcycle : 80 %\n",
      "Accuracy of mountain : 63 %\n",
      "Accuracy of mouse : 31 %\n",
      "Accuracy of mushroom : 61 %\n",
      "Accuracy of oak_tree : 56 %\n",
      "Accuracy of orange : 72 %\n",
      "Accuracy of orchid : 48 %\n",
      "Accuracy of otter : 18 %\n",
      "Accuracy of palm_tree : 73 %\n",
      "Accuracy of  pear : 57 %\n",
      "Accuracy of pickup_truck : 71 %\n",
      "Accuracy of pine_tree : 44 %\n",
      "Accuracy of plain : 69 %\n",
      "Accuracy of plate : 57 %\n",
      "Accuracy of poppy : 57 %\n",
      "Accuracy of porcupine : 44 %\n",
      "Accuracy of possum : 31 %\n",
      "Accuracy of rabbit : 21 %\n",
      "Accuracy of raccoon : 45 %\n",
      "Accuracy of   ray : 35 %\n",
      "Accuracy of  road : 71 %\n",
      "Accuracy of rocket : 74 %\n",
      "Accuracy of  rose : 41 %\n",
      "Accuracy of   sea : 79 %\n",
      "Accuracy of  seal : 18 %\n",
      "Accuracy of shark : 41 %\n",
      "Accuracy of shrew : 28 %\n",
      "Accuracy of skunk : 70 %\n",
      "Accuracy of skyscraper : 75 %\n",
      "Accuracy of snail : 34 %\n",
      "Accuracy of snake : 37 %\n",
      "Accuracy of spider : 55 %\n",
      "Accuracy of squirrel : 24 %\n",
      "Accuracy of streetcar : 59 %\n",
      "Accuracy of sunflower : 82 %\n",
      "Accuracy of sweet_pepper : 30 %\n",
      "Accuracy of table : 48 %\n",
      "Accuracy of  tank : 57 %\n",
      "Accuracy of telephone : 45 %\n",
      "Accuracy of television : 50 %\n",
      "Accuracy of tiger : 45 %\n",
      "Accuracy of tractor : 52 %\n",
      "Accuracy of train : 54 %\n",
      "Accuracy of trout : 64 %\n",
      "Accuracy of tulip : 40 %\n",
      "Accuracy of turtle : 29 %\n",
      "Accuracy of wardrobe : 82 %\n",
      "Accuracy of whale : 47 %\n",
      "Accuracy of willow_tree : 45 %\n",
      "Accuracy of  wolf : 43 %\n",
      "Accuracy of woman : 13 %\n",
      "Accuracy of  worm : 59 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6cc5ce",
   "metadata": {},
   "source": [
    "We took the original baseline model followed by a variation of the baseline model with added layers and added batch layers. I created a classifier that took the weights from the model that maxmized prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054e2ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
