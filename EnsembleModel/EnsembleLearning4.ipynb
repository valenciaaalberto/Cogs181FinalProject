{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0954e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e37623",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0180c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d9c01f",
   "metadata": {},
   "source": [
    "## Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d2c9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4534e4dd",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da1270f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineConvolutionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaselineConvolutionModel,self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 100)             \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d7f13",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c64648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvutionModelAddedLayersBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvutionModelAddedLayersBatchNorm, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_seven = nn.Conv2d(in_channels=1024,out_channels=2048,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm3 = nn.BatchNorm2d(3)\n",
    "        self.batchNorm512 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8192, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, 256)\n",
    "        self.fc6 = nn.Linear(256, 100)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.batchNorm512(x)\n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        \n",
    "        x = self.conv_layer_seven(x)\n",
    "        x = F.elu(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2ea87",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d232b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, in_dim, heads):\n",
    "        super(MultiHeadAttentionWrapper, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=in_dim, num_heads=heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store original shape\n",
    "        original_shape = x.shape  # New line to store original shape\n",
    "\n",
    "        # Reshape x for attention\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        x = x.flatten(2).permute(0, 2, 1)\n",
    "\n",
    "        # Apply attention\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "\n",
    "        # Restore attn_output to original shape\n",
    "        attn_output = attn_output.permute(0, 2, 1).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, channels, height, width)\n",
    "\n",
    "        # Restore x to its original shape\n",
    "        x = x.permute(0, 2, 1).contiguous().view(original_shape)  # New line to restore x's shape\n",
    "\n",
    "        return attn_output + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a3adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetMultiheadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetMultiheadAttention, self).__init__()\n",
    "        # Load pretrained ResNet-18\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 100)\n",
    "\n",
    "        # Modify layers to include multi-head attention\n",
    "        self.layer1 = self._add_attention_to_layer(self.resnet.layer1, 64, 2)\n",
    "        self.layer2 = self._add_attention_to_layer(self.resnet.layer2, 128, 4)\n",
    "        self.layer3 = self._add_attention_to_layer(self.resnet.layer3, 256, 8)\n",
    "        self.layer4 = self._add_attention_to_layer(self.resnet.layer4, 512, 8)\n",
    "\n",
    "    def _add_attention_to_layer(self, layer, in_dim, heads):\n",
    "        new_layer = []\n",
    "        for block in layer:\n",
    "            new_layer.append(block)\n",
    "            new_layer.append(MultiHeadAttentionWrapper(in_dim, heads))\n",
    "        return nn.Sequential(*new_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.resnet.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.resnet.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c054a84",
   "metadata": {},
   "source": [
    "## Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffc86f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModified(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNetModified, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11 , stride=4, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5, stride=1, padding=2)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Compared to the original architecture, I had to add more linear\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 500)\n",
    "        self.fc5 = nn.Linear(500, 100)\n",
    "        \n",
    "        self.batchNorm1 = nn.BatchNorm2d(3)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(384)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Since the images are 32,32 we map it to 224,224\n",
    "        ## Test whether \n",
    "        transform = transforms.Compose([transforms.RandomResizedCrop(224,antialias=True)])\n",
    "        x = transform(x)\n",
    "        \n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x) \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da92f8",
   "metadata": {},
   "source": [
    "## Maximum Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07ba97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4\n",
    "class MaximumVotingConvolutionalEnsemble(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, model4, nb_classes=100):\n",
    "        super(MaximumVotingConvolutionalEnsemble, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.model4 = model4\n",
    "        # Remove last linear layer\n",
    "        self.model1.fc = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "        self.model3.fc = nn.Identity()\n",
    "        self.model4.fc = nn.Identity()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x.clone())  \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = self.model2(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "        x3 = self.model3(x)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        \n",
    "        x4 = self.model4(x)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "\n",
    "        x = torch.max(x1,torch.max(torch.max(x2,x3),x4))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaab8d8",
   "metadata": {},
   "source": [
    "## Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c365b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4\n",
    "class VotingConvolutionalEnsemble(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, model4, nb_classes=100):\n",
    "        super(VotingConvolutionalEnsemble, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.model4 = model4\n",
    "        # Remove last linear layer\n",
    "        self.model1.fc = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "        self.model3.fc = nn.Identity()\n",
    "        self.model4.fc = nn.Identity()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x.clone())  \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = self.model2(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "        x3 = self.model3(x)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        \n",
    "        x4 = self.model4(x)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        \n",
    "        x = (x1 + x2 + x3 + x4)/4\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26576691",
   "metadata": {},
   "source": [
    "## Bagging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d477d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from: https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4\n",
    "class BaggingConvolutionalEnsemble(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, model4, nb_classes=100):\n",
    "        super(BaggingConvolutionalEnsemble, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.model4 = model4\n",
    "        # Remove last linear layer\n",
    "        self.model1.fc = nn.Identity()\n",
    "        self.model2.fc = nn.Identity()\n",
    "        self.model3.fc = nn.Identity()\n",
    "        self.model4.fc = nn.Identity()\n",
    "        \n",
    "        self.classifier = nn.Linear(400, nb_classes)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x.clone())  \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        x2 = self.model2(x)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        \n",
    "        x3 = self.model3(x)\n",
    "        x3 = x3.view(x3.size(0), -1)\n",
    "        \n",
    "        x4 = self.model4(x)\n",
    "        x4 = x4.view(x4.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = self.classifier(F.elu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269cb0ff",
   "metadata": {},
   "source": [
    "## Model 1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb085fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = BaselineConvolutionModel()\n",
    "model_one.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(model_one.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a40c72e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.608\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.601\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.562\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.487\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.430\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.363\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.335\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.316\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.308\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.274\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.233\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.177\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.160\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.128\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.071\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.051\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.000\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 4.002\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.983\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.919\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.889\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.895\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.844\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.783\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.752\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.749\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.666\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.672\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.634\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.559\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.558\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.537\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.496\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.481\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.493\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.449\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.435\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.425\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.393\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.306\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.242\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.254\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.211\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.156\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.136\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.098\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.085\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.053\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.035\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.007\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 2.925\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 2.930\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.821\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.851\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 2.810\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 2.779\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.810\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 2.749\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 2.776\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.763\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.756\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.674\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.677\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.688\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.548\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.588\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.531\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.584\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.498\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.519\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.500\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.486\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.472\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.450\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.495\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.457\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.357\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.333\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.284\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.332\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.353\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.275\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.330\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.314\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.292\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.245\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.250\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.263\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.068\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.123\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.125\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.148\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.081\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.108\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.141\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.145\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.126\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.132\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.168\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.070\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 1.855\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 1.943\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 1.958\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 1.965\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 1.952\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 1.979\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 1.964\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 1.968\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 1.965\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 1.960\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 1.987\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 1.913\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 1.736\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 1.794\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 1.737\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 1.805\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 1.817\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 1.824\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 1.832\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 1.790\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 1.767\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 1.815\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 1.818\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 1.814\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 1.583\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 1.631\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 1.594\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 1.657\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 1.629\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 1.668\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 1.646\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 1.608\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 1.664\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 1.622\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 1.707\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 1.658\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 1.408\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 1.398\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 1.509\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 1.475\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 1.495\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 1.526\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 1.464\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 1.566\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 1.516\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 1.498\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 1.524\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 1.587\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 1.248\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 1.278\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 1.260\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 1.341\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 1.394\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 1.379\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 1.404\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 1.393\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 1.369\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 1.398\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 1.415\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 1.422\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 1.103\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 1.089\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 1.213\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 1.152\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 1.219\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 1.190\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 1.243\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 1.295\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 1.229\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 1.239\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 1.233\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 1.290\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 0.952\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 0.947\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 1.003\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 1.017\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 1.046\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 1.108\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 1.073\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 1.136\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 1.141\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 1.094\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 1.157\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 1.162\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 0.781\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 0.835\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 0.845\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 0.916\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 0.938\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 1.020\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 0.964\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 0.985\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 0.985\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 1.018\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 1.044\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 1.000\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 0.657\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 0.691\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 0.736\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 0.786\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 0.795\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 0.847\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 0.880\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 0.876\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 0.891\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 0.932\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 0.941\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 0.890\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 0.563\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 0.610\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 0.638\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 0.680\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 0.693\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 0.720\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 0.742\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 0.738\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 0.771\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 0.827\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 0.784\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 0.871\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 0.516\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 0.530\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 0.555\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 0.604\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 0.643\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 0.650\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 0.712\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 0.709\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 0.712\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 0.705\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 0.725\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 0.716\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 0.460\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 0.450\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 0.513\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 0.547\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 0.546\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 0.525\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 0.639\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 0.572\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 0.630\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 0.634\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 0.694\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 0.594\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 0.386\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 0.443\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 0.414\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 0.445\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 0.513\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 0.506\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 0.541\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 0.542\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 0.554\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 0.580\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 0.551\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 0.609\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 0.319\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 0.352\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 0.417\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 0.444\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 0.416\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 0.470\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 0.468\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 0.475\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 0.498\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 0.527\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 0.546\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 0.531\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 0.312\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 0.319\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 0.379\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 0.377\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 0.393\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 0.437\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 0.463\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 0.420\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 0.469\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 0.430\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 0.460\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 0.482\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 0.338\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 0.296\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 0.320\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 0.346\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 0.351\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 0.383\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 0.411\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 0.428\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 0.411\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 0.408\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 0.419\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 0.425\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 25      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = model_one(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2da90a",
   "metadata": {},
   "source": [
    "## Model 1 Training Loss Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88739588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAzUUlEQVR4nO3dd3wc1bXA8d+RdtV7tdwt997kCtgEDJji0HsLCRCSlwckEB4vkNBCKCFAgBBieoAHBhzANIOxjU0zttzlLstFsq1m9b5a3ffHjmTZVlnbWq12db6fjz5azd6dPeOxju7euXOuGGNQSinlfwK8HYBSSinP0ASvlFJ+ShO8Ukr5KU3wSinlpzTBK6WUn7J5O4DmEhISTP/+/b0dhlJK+YzVq1cXGmMSW3quSyX4/v37k56e7u0wlFLKZ4jIntae0yEapZTyU5rglVLKT2mCV0opP6UJXiml/JQmeKWU8lOa4JVSyk9pgldKKT/VpebBH69307OpcTipq2+gtr6BuvoG7IFCeLCNEHsggQFCYmQwUwfEExoU6O1wlVKqU/hFgr/vo01UO5zttpvUP5Z3fzkNEemEqJRSyrv8IsEvvfNUbIFCkC2AoEDXl6OhgcpaV6/e4Wzgs40HeOTzrSxYv5/zx/XydshKKeVxfjEG3yM6hISIYKJC7ITYAwkIEIJtgcSFB9EjOoQ+cWHcdEoqw3pE8uI3Wd4OVymlOoVfJHh3BAQIc8b2JGNfGfllNd4ORymlPK7bJHiA04cnAbB0W76XI1FKKc/rVgl+aHIkPaNDWLRZE7xSyv91qwQvIpw3tidfb8unoLzW2+EopZRHdasED3BZWh/qGwz/WZPj7VCUUsqjul2CH5QUwcR+sczXBK+U8nPdLsEDzBmTwva8CjLzy70dilJKeUy3TPCzR6UA8NnGXC9HopRSntMtE3yP6BAmD4jjjRV7yNM58UopP9UtEzzAQ+ePorK2nhteXUVuqSZ5pZT/6bYJfmiPSJ6/egK7D1Yy869LefTzrd4OSSmlOlS3TfAApw5NYsFvTmbWiGReWLaTRZvzvB2SUkp1mG6d4ME1bfKpy8YxrEck932UQY0bZYeVUsoXdPsEDxBkC+C+OSPZX1rDa9/v9nY4SinVITTBW6YNjOfUoYn8a9lOauu1F6+U8n2a4Jv5xckDKK5ysDBD58crpXyfJvhmThqYQN+4MF78JovSaoe3w1FKqROiCb6ZgADh92cNZeuBci594Xuq63SoRinluzTBH2HO2J68dH0a2/MqeGyhzo1XSvkuTfAtOHVoEldP6csbK/aQX653uSqlfJMm+Fb84uQBOBsM81fv83YoSil1XDTBtyI1MYLJ/eN444fdWpBMKeWTPJ7gRSRQRNaKyCeefq+OdtfsoZRWO7hy7goczgZvh6OUUsekM3rwtwFbOuF9Olxa/zj+dtk4sgordW68UsrneDTBi0hv4FzgJU++jyedOSKZ/vFhPPzpFu6ev0HnxyulfIane/BPA3cBrY5viMjNIpIuIukFBQUeDufYuebGDyMmzM78NTlc/q8fqNfhGqWUD/BYgheR84B8Y8zqttoZY+YaY9KMMWmJiYmeCueEnDsmhYW3z+DRi8awNbecH7IOejskpZRqlyd78CcBPxWR3cA7wGki8qYH38/jzh2TQkSwjQXr9ns7FKWUapfHErwx5n+NMb2NMf2BK4AlxphrPPV+nSHEHsiZI5NZmJHLwYpab4ejlFJt0nnwx+jmGanUOhu49F8/cOPrq3SBEKVUl9UpCd4Y87Ux5rzOeC9PG9Yjij+fP4qKmnq+2pLPx+t1uEYp1TVpD/44XDapDz/+4XSGJkfy6ne7McZ4OySllDqKJvjjJCJcN70fmw+UsT6n1NvhKKXUUTTBn4DzxvQkyBbAB2tyvB2KUkodRRP8CYgOtXPGiGQWrN+vF1uVUl2OJvgTdN3UfhRXObjvo01adVIp1aVogj9BU1LjufHkAcxLz+akR5eQU1zl7ZCUUgrQBN8h/nDOcP559QTqGwzLtxd6OxyllAI0wXeIgABh9qgepESH8G1m1yuYppTqnjTBdxAR4aRBCXy2MZcnv9ymF12VUl6nCb4DnTEiGYBnlmTy4vIsL0ejlOruNMF3oDNHJLPyntM5e1QPnv96JwXlWpBMKeU9muA7kIiQFBnCbbMGU+1w8tWWPG+HpJTqxjTBe8DQ5Eh6x4ayeEu+t0NRSnVjmuA9QEQ4fVgS32YWsDAjF2eDFiNTSnU+TfAecs7oFGocDdzy5mpufWetruOqlOp0muA9ZEpqPOv+dAZ3zR7KpxsO8OaKPd4OSSnVzRxTgheRWBEZ46lg/E1MWBC/mjmQ6QPjeWZJJnsPVvHV5jwc2ptXSnWCdhO8iHwtIlEiEgesB14VkSc9H5p/EBH+cM5wKmvrmfHXpdz473Re/naXt8NSSnUD7vTgo40xZcBFwKvGmInALM+G5V9G9Yrm01tP4eopfRnbJ4a5y7Moq3F4OyyllJ9zJ8HbRCQFuAz4xMPx+K1BSRE8fOFo7p8zgpKqOs56ajlvrNijF1+VUh7jToJ/EPgCyDTGrBKRVGCHZ8PyX+P7xvLeLdNIjgrhjx9m8NjCrd4OSSnlp9pN8MaY94wxY4wxv7Z+zjLGXOz50PzXxH5xfPDr6Vw1pS8vfbuLddkl3g5JKeWH3LnI+rh1kdUuIotFpFBErumM4PxZ48VXe0AAn2884O1wlFJ+yJ0hmjOti6znATnAEOD3Ho2qm4gItjGubww/ZB30dihKKT/kToK3W9/PAd42xhR5MJ5uZ1pqPBn7Simt1lk1SqmO5U6C/1hEtgJpwGIRSQR0dekOMm1gPA0GftRevFKqg7lzkfVuYBqQZoxxAJXA+Z4OrLuY0DeWqBAbCzflejsUpZSfceciqx24FpgnIu8DvwC0u9lBgmwBzB7Vg082HOCDtTl6A5RSqsO4M0TzT2Ai8Lz1NcHapjrInLE9qatv4Lfz1vPwJ1u8HY5Syk/Y3GgzyRgzttnPS0RkvacC6o6mD0zgjjOGsD6nlPfX5HDzzFQGJkZ4OyyllI9zpwfvFJGBjT9Yd7I6PRdS9xMYIPz36YN59OLR2AKEV7QYmVKqA7iT4H8PLLWqSi4DlgB3eDas7ikhIphzx6Tw0br9VNbWezscpZSPc2cWzWJgMHCr9TXUGLPU04F1V1dN7ktFbT3PLsnEGF3qTyl1/FodgxeRi1p5aqCIYIz5j4di6tYm9ovlkom9eWHZTuLDg7hpRqq3Q1JK+ai2LrLOaeM5A2iC9wAR4a+XjKGkqo6nvtrOeWNTSIkO9XZYSikfJF1pGCAtLc2kp6d7O4wuYe/BKs54ahnhwTaeuWI8Jw9O8HZISqkuSERWG2PSWnpOF93uovrGh/GfX08nLjyI2+etpaiyztshKaV8jMcSvIiEiMhKEVkvIptE5AFPvZe/GtkzmmevHE9ptYOrXlxBdlGVt0NSSvkQT/bga4HTrJukxgGzRWSqB9/PLw1PieKl6yexv6San7+2igqdPqmUcpNbCV5EpovIVSJyXeNXe68xLhXWj3brq+sM+PuQmUMSeeGaiewsqOC5JZneDkcp5SPcKTb2BvAEcDIwyfpqcUC/hdcGisg6IB9YZIz5sYU2N4tIuoikFxQUHEvs3cr0QQmcNCiBJVvzvB2KUspHuFOLJg0YYY5juo0xxgmME5EY4AMRGWWMyTiizVxgLrhm0Rzre3QnpwxO4C+fbSW3tIYe0SHeDkcp1cW5M0STAfQ4kTcxxpQAXwOzT2Q/3d3JgxIBeODjTWTsK/VyNEqprq6tO1k/xjVmHglsFpGVuC6cAmCM+WlbO7ZWfnIYY0pEJBSYBTzWIVF3U8N6RNI3LozPM3JxNhjmXufWSJlSqptqa4jmiRPcdwrwuogE4vqk8K4x5pMT3Ge3FhAgfPnbGdz+zjrW55R4OxylVBfXaoI3xiwDEJEBwAFjTI31cyiQ3N6OjTEbgPEdFKeyhNgDSesfy8JNueSX15AUqWPxSqmWuTMG/x7Q0Oxnp7VNecmY3jEATH54MU9/td27wSiluix3ErzNGNN0n7z1OMhzIan2jOwZ1fT42SWZZBVU4HA24GzQSUhKqUPcmSZZICI/NcYsABCR84FCz4al2hIebOPcMSkkRgTzbno293+8mfyyGob2iOTvV+iomFLKxZ0Efwvwlog8Z/2cA1zruZCUO/5x1QQA+seHcf/HmwHYWVDBacOSCLEHcuaIZETEmyEqpbzMnQTfYIyZKiIRuMoLl1sXXlUXcP30/uwsqMThbOCdVdnc9s46wDWl8vZZQ5g96oRuYVBK+TB3xuDnAxhjKowx5da29z0XkjoWIsJDF4zikYtGM7ZPDIOTInjw/JFUO5zctyBDl/1Tqhtr60anYcBIIPqI5fuiAJ2b18WICG/dOIWgwACCbAEEBQZw9382kplfweDkSG+Hp5TygraGaIYC5wExHL58XzlwkwdjUscpIvjQ6WxcAWr5jkKyi6uICLYzeUCct0JTSnlBWzc6fQR8JCLTjDE/dGJMqgP0jg1jQEI4f/50M8ZAVIiNDfef5e2wlFKdyJ2LrGtF5L9wDdc0Dc0YY37usahUh7jnnOF8s6OAb3YUklVYSY3DSYg90NthKaU6iTsXWd/AVU3yLGAZ0BvXMI3q4maNSOaB80fx2zOGALCrsNLLESmlOpM7CX6QMeaPQKUx5nXgXGC0Z8NSHWlgYgQAmfkV7bRUSvkTd4ZoHNb3EhEZBeQC/T0WkepwqYnhiMCd761n2fYCzh2dwra8cm6ZOdDboSmlPMidBD9XRGKBPwILgAjrsfIRIfZAjIHa+gbeX53D0q35lFY7uGZqv8Nm3iil/Eu7QzTGmJeMMcXGmGXGmFRjTJIx5l+dEZzqOD2tJf5sAcLByjrqGwwrdx30clRKKU9yZ9HteBF5VkTWiMhqEXlaROI7IzjVcd66aSof/Ho6s0f1oEdUCEG2AO54dz33fLCRitp6b4enlPIAdy6yvgPkAxcDl+CqJDnPk0GpjjcgIZzxfWN5/JIxfHrryQxMjKC4ysFbP+7lwn98x/rsEgC255Xzj6WZNGjpYaV8njsDsHHGmIea/fxnEbnAQ/EoDwsLshEWZOO+OSP4MauIMX2iuXv+Bi58/jtumpHKyl1FrN1bQkp0CBdN6O3tcJVSJ0DaK0YlIk8A6cC71qZLgJHGmPs6Opi0tDSTnp7e0btV7SircfCXT7fwzqpswFXyIDw4kO/+5zRsge58yFNKeYuIrDbGpLX0XFvFxsoBAwjwO1w3PAmuYZ0KoMMTvPKOqBA7j148hkFJEXybWci5o1P4/fsb2Jpbzqhe0d4OTyl1nNqqRaMlCLuZG09J5cZTUskuqgJgzd5iTfBK+bBj+vwtIvd7KA7VhfSODSUxMpg1e4q9HYpS6gQc6wDrTz0ShepSRIQJfWNYs7fksO01Dqd3AlJKHZdjTfC6yGc3MTU1nr1FVbyzci/OBsPt76xlzANfsr+kWqdQKuUjjvU+9YkeiUJ1OddM7cfX2wq458MMthwo48N1+wG4b8EmvtlRwG2nD+GWmam6sLdSXVir0yRF5C5jzOMi8iyu2TSHMcbc2tHB6DTJriW/rIaTH1tKnbOBqalx5BRXk1NcjS1AqG8wXDu1Hw+eP1KTvFJe1NY0ybaGaLZY39OB1S18KT+XFBXCBeN7AnDLzIGcOjQRgN+eMYSbThnAGyv28OXmPG+GqJRqQ1vTJD+2vr/eeeGoruau2cNI6xfHzCGJRIXaWb2nhMsn9SEm1M4Xm/J4dskOfth5kM8zDvDeL6fTNz7M2yErpSztjsGLyBDgTlw14JvaG2NO81xYqqtIiAjmskl9AJjQN5bPbzul6blbZg7kDx9sJGNfGQAb9pVogleqC3HnIut7wAvAS4DOk1NNrpjUhwEJ4SREBHHGU8vZX1Lt7ZCUUs24k+DrjTH/9HgkyucEBAjTBroqR0cE29hfUuPliJRSzbkzD/5jEfm1iKSISFzjl8cjUz4lJTqEA6Xt9+DrnQ08vnArmfm6brtSnuZOD/566/vvm20zQGrHh6N8VUpMaFMPPre0hr8v3sFtpw8mOSqYBev3sy23nIsn9mbp1nye/3one4uqeO6qCV6OWin/1m6CN8YM6IxAlG/rFRPC6t1FPPjxZhZtySW7qJr48CB6xYbyv//ZCMDr3+/G4XTdUpFdrOP1SnlaW+WCTzPGLBGRi1p63hjzH8+FpXxNSnQolXVOXvluF8NTohiYGMD8NTmU19QzLTWexy4ew2MLt5IQEURVnZMP1u6jxuEkxB7o7dCV8ltt9eBnAkuAOS08ZwBN8KpJirWod1SIjc9vO4W3ftzDPR9kEBcexF8vHUPv2DD+cbVrSOarzXm8tzqHDTmlTB6gl3OU8pS2bnS6z/p+Q+eFo3xVQmQwANdP7w/AeaN7smxbATfNSKV37OFz4yf0iwVgfXaJJnilPMidG51igOs4+kanNmvRiEgf4N9AD6ABmGuM+fsJxKq6sFOHJPLKz9KYOSQJgOgwO3Ova7E8BnHhQUQE2zhQqtMqlfIkd2bRfAasADbiStTuqgfuMMasEZFIYLWILDLGbD6OOFUXJyKcNizZ7fZJUcHklWmCV8qT3EnwIcaY3x3rjo0xB4AD1uNyEdkC9AI0wSuSI0PcTvCb9pcSExZEr5hQD0ellH9xJ8G/ISI3AZ8AtY0bjTFF7r6JiPQHxgM/tvDczcDNAH379nV3l8rH9YgOYdVu13+hqrp6Vu8pJjEymNvfWceAhHAm9oslxB5IfHgQt89bx+QBcbzxiylejlop3+JOgq8D/grcw6G68G7f6CQiEcB84HZjTNmRzxtj5gJzwVUP3p19Kt+XFBVMflktn208wB8+2EhJlQOAIFsAWQWVfJ6Re1j7FVkHKa9xEBli90a4SvkkdxL874BBxpjCY925iNhxJfe3dN68ai45MoQ6ZwO/fmsNY3pH8/jFg1i8JZ/ThycxoV8sxrjWgN20v5SKWid3vreeb3YUcs7oFG+HrpTPcCfBbwKqjnXH4lrm52VgizHmyWN9vfJvPax58wD3nDOcKanxnDmyx1Ht+sSFUe9s4KFPNrN0a74meKWOgTsJ3gmsE5GlHD4G396SfScB1wIbRWSdte0PxpjPjidQ5V+So4KbHo/tE9NmW1tgACNSothZUOHhqJTyL+4k+A+tr2NijPkW0MU6VYuSIg/14N0pV9AnLpSvtxV4MiSl/I47xcZ0yT7V4ZKsHvylE3u71b53bBj55bUt1q/ZX1KNLVAO+6OhlHKvB69Uhwu2BZJ+7yxiQt2bFdM71jUHfn9JNamJETgbDB+u3cfbK/eSvqeYXjGhLLlzJvuKq5mXno2j3nDq0ERmDEn05GEo1aVpgldekxAR3H4jS2M9mw/W7mPW8GRe+W4XH63bz+CkCK6Z2pc3V+zlzRV7eXdVNlmFFQSIsGD9ftLvneWp8JXq8jTBK5/Q2IN/dkkm/1mzj/zyGq6e0pc/XzAKgKyCSh75bAv1DYYnLxvL3qIqnv5qBw5nA/ZAdxYuU8r/HNf/fOvuU6U6TXLUofH1fSXVOJyGy9L6ICKICE9fMY5+8WH0jQtjztieJFrVLQsralvbpVJ+73h78Do7RnWqwIBD/+ViwuyEB9kY0zu6aVtSZAif3noKtQ5Xj73xgmtBeS0p0VrDRnVPx5XgjTH/6uhAlGrPc1eNJzzYhjEGW0AArnvpDgmxBzbNsGnsweeXtd+DL6qsY/GWPC6Z2PuofSrly9ypB99SJclSYLUxZl2HR6RUK84b09PttklWgi9wY4jmte9388ziHSRGBnPq0KTjjk+prsadMfg04BZcpX574ar8eCrwoojc5bnQlDp+jTN0Gnvw9c4GDlbUkr67iD9+mEFlbT3ZRVW89E0W3+5w3UD1wrKdXotXKU9wZ4gmHphgjKkAEJH7gPeBGcBq4HHPhafU8QmyBRAbZie/vAZng+GxhVv59w97iA0LIreshm255fSIDmHB+v2Aa5WpFVlF5JbWHFYnRylf5k6C74urZHAjB9DPGFMtIjpFQXVZSZEhvPXjXpbvKKCq1kltfQO5ZTVcNaUv//fj3sPanj+uJ69+t5v8ck3wyn+4k+D/D1ghIh9ZP88B3haRcHR1JtWF2W2uC6bZRdUA3H32MJIig7lwfC/2Hqzi28xC7jlnOMt3FHDWyB68+t3uY5pWaYzRi7KqS3OnFs1DIvIZcDKu6ZG3GGPSraev9mRwSp2I4krXIiLTB8azPa+cG07qT7DNNcvmiUvH8kNWIReO781NM1LJLnJVxC4sr2t1f80T+tbcMmY//Q3zfzWNif3iPHwkSh0fd2bR/B2YZ4z5eyfEo1SHee6q8WTsK+XKyX2prHU2JXdw1aO/cPyhQmfxEUEAFFa23IPfllvORc9/x5s3TmF831jmLs8CYEVWkSZ41WW5M4tmDXCviGSKyF9FJM3TQSnVEcb3jeXaaf2xBQYQHdZ2UbOwIBthQYFH9eCNMewqrOSbHQVU1jl5YdlOjDEss0oXB9u0DILqutr932mMed0Ycw4wGdgOPCYiOzwemVKdLCEimMKKWqrrnHyyYT/OBsNL3+ziJ098zWvf7wbgy815/O3L7RysdP0hKKt2eDFipdp2LHeyDgKGAf3Ri6vKDyVEBHGwspZ3Vu3lgY83c/aoA3y1JQ+AnOJqpg+MJ6e4mueWZjKpfywb95VSVlPv5aiVal27PXgRaeyxP4hrfdaJxpg5Ho9MqU4WHxFMYXkd2/NcSwN+npFLakIEZ41MBmDW8GQW/OYk7jlnOC9el0ZiZDCl2oNXXZg7PfhdwDRjTKGng1HKmxIiglmzp5iMfaUMTY7knNEp/Gx6fzILKli6rYCTBycQExbETTNSAYgOtWuCV12aO9MkXxCRWBGZDIQ0277co5Ep1ckSI4I4WFlHSbWDG08ewG2zBgMwsV8smx84C9sRdeWjQtxP8PnlNXyzvZDpg+K1uqXqNO4M0dwILAe+AB6wvt/v2bCU6nwJVoEyZ4NhVK/ow547MrlDyz340moHNQ4njy3cyicb9tPQYJi/OocZjy/ljvfW88q3uzx3AEodwZ0hmtuAScAKY8xPRGQYrkSvlF85a2QP5i7PIq+shon9YtttHx1qP2wWTXZRFWc8tYzwIFvTLJspA/bw464ipqbGsSKriKJKHdJRncedBF9jjKmxVs4JNsZsFZGhHo9MqU6WHBXCN3f9hPLaeqJC2l8M/Mge/Js/7sHhdN3tevfZw6iqreeZJZmMSInitRsmc+Hz31Na3fqdskp1NHcSfI6IxAAfAotEpBjY78mglPIWEXEruQNEhdqprW+gxuFEBN5dlc0Zw5P55zUTEBGMMYzoGcWEvrGE2AOJCbVTUqU9eNV53LnIeqH18H4RWQpEAws9GpVSPiAq1PWHoKzawba8coqrHFyadmhVKBFh9qiUpvYxYXYy8yu8Eqvqno5pyT5jzDJPBaKUr4m2EnxptYNvdhRiDxSmDYxvtX1MmJ2SNmbdfLh2H3HhQcwYkgjAjrxyAAYnR3Zg1Ko7Od5Ft5Xq9hoT/BlPuWYMT0uNJyyo9V+pqFA7pVWOFssMF1XWcdf8DfSOCWXxHTMx5tB+dz96roeOQPk7TfBKHaeokMN/fU4enNBm+5jQIOqcDVQ7nEf9IZi3Kpu6+gayCivZtL+Mokq9GKtOnCZ4pY5Tv/hwEiKCuePMIYQFBXLmiB5tto8JOzSk05jgy2sc2AMDeHPFHkb1iiJjXxnnPftt02saPyUodTw0wSt1nOLCg0i/d5bb7WOsZF1S5eDxhduYPCCOF5dnUVvfwL6Sav543gi+31nInoNV9IsPY2FGrpZCUCdEE7xSnaSxJv3+kmo+XLePLzblUlXnBKBndAizhicxe9ShTwFJkcE88eV2ahxOQuyBLe5TqbZogleqk8SEulaNWrm7CGOgqs6JPVC49bTBjO4dfVQ5hOhm0zDbS/BVdfV8sHYfEcE2zh/XyzMHoHyOJnilOkljD35FVhEAtgDh5EEJ/Pfpg1ts3zTPvsZBUlTIUc8XV9bx3upsYsOCWLW7iHfTcwD46dieuhi4AjTBK9VpGsfg12eXEBFs4/WfTyK5hcTdqPk8+yM1NBj+Z/4Gvtych8jhSwdW1TkJD9ZfbeXemqxKqQ4QFhSIPdDVsx6QEM7EfnH0jg1rtX1rCf7+BZs46+nlLN2Wz0UTehFsC6DG0cA5o13j9+W6ypSyaIJXqpOICDed4losZFL/uHbbt5TgaxxO3l+dw478ChxOw89PGsDPTxpA//iwpmmaFbU680a5eOxznIi8ApwH5BtjRnnqfZTyJXfNHsblk/qQEBHcbtvGMfjSZgXKvtqSR0VtPWn9YhGBkT2jGNkzijvPHMqyHQUAbq8Tq7Nz/J8ne/CvAbM9uH+lfFK/+HC3xsibZtFYCbvG4eSfX+8kOSqYeb+cxnu3TMcq401AgDTdWevOEM0/v97J+AcXkbGv9ASORHV1Hkvw1pJ+RZ7av1L+zh4YQFhQIKXVDvLKavj1W2vYtL+MP18wmsCAo2fJRFpljstrDh+iWZiRy9S/LKbYKn+waHMejy3cSrXDyVOLtnv+QJTX6KV2pbqwxkVF/vvttazPLuG+OSM4Y0Ryi20jW+nBf7x+P7llNbz6/W56Rofwt0XbGZESxenDk3h2SSZ7D1bRN771i73Kd3k9wYvIzcDNAH379vVyNEp1LdGhdtbsLSaroJL/PXsYN5w0oNW2LfXgnQ2GbzMLAXhm8Q4AIoJtPH7JGEqqHDxLJgdKqzXB+ymvz6Ixxsw1xqQZY9ISExO9HY5SXUpUqJ2sgkpsAcLFE3u32TY8KJAAObwHv3FfKaXVDuaM7UlSZDCv3jCJVffMYlSv6KPG+NtSXuNgYUYuNQ7niR2Q6lRe78ErpVr3yxmphAUFMq5PTLszb0SEiGBbU4KvrK3n4U83Yw8U7p8zgrjwoMPucI0Kdf36t1fQLLuoivOe/ZbSage/+ckg7jxLl2T2FZ6cJvk2cCqQICI5wH3GmJc99X5K+aPThydz+vCWx9xbEhlip8waopm7PIv0PcU8e+V44lv449C81k1bnvhyG7X1TtL6xfJ/K/fym9MG6fRKH+GxBG+MudJT+1ZKtSwyxEaF1YP/YlMuk/rHcd6Ynq20bb0UQkVtPX//ajtl1fV8tG4/vzp1IKcMTuCqF39k0eY85oxteZ9HKqtxEGwLINimfxC8QYdolPIjkSGuIZq9B6vYmlvOvecOb7VtYIAQGWxr6vE39/Cnm3l7ZTYBAldM6sOtpw3GaQzgKnfsDmMMY+7/ktOHJfHyzyYd3wGpE6IJXik/EhliZ9XuIn777jqAdleZirKmYTa3Zm8xb6/M5pczU/n9mUObyhgbY7AFiFuLkBRV1pFfXgPA4q35x3EkqiNoglfKjzT24LfllvOXC0e3O/0xKtR+1Bj8q9/tJjLExq2nDT6sRr2IEB1qp6SdBF/jcHLmU8sxVo+/V0zocR6NOlGa4JXyI5W1rvH3a6b246op7d9XEh1qo6za9RpjDPPX7OPzjQe4blr/FsspRLfQ4z/SF5tyKayobfo5NlzXlfUWr8+DV0p1HLvV477ajeQOEBVyKGF/suEAd763nkFJEdw0o+UbqqLD7IcVP2vJ2yv30jM6hMhg92vjKM/QHrxSfuShC0Zx3bT+9Ilz787U6FDXtEpjDC99u4vUhHA+u/UUAlqoddPY/mBFXYvP7SqsZPn2AlZkFXHvucM5c0QPnly0jWXbC477eNSJ0QSvlB9JiAh2qxRxo8aLrKv3FLM+u4SHzh/ZanIHV4LPKqg8avuC9fu5/Z21NBgY2zuan03vjy0wgN6xYZTV1GOMaXcZwZW7ithbVMXsUT2I0BWpOoT+KyrVjUWH2qmqc/Lkou3Ehwe1Ww4hJtROSdXhPfgfdh7kd/PWkdY/jqun9GXKgPimi7NRoTacDYbKOudRSfveDzfy9bYCbAFCiD2QHfkVOBsM767K5t1bprUbuzGG+gbTNCyljqb/Mkp1Y413s36/8yA3zUglLKjtPl90qJ3y2nqcDa4ZMm/8sJub/51O/4RwXr4+jfPH9aJH9KF1ZqNCWr5btqiyjndWZhMRbGNkr2iiQu2cOzqFc0enkFV49CeElvz7hz1M+ctirY/TBu3BK9WNhdgP9fGundqv3fbRYUEY4yo+tj2vgj9+tImTBsXz6EVjmu6MbS6qqaCZg54cmi756cYD1DcYnrxsHCN6RjVt/9uX2/g84wANDabNoSJjDP/+YTdFlXVs3Ffq1hKI3ZH24JXqxob2cCXXf1078ZhWmSqpcvD4wq0kRQbz0nWTWr2oe6gHf2gmzTOLd/DIZ1sYkhzB8JTIw9rHhwfRYKC46ugLudtyy5m3ai/GGD7ZcICd1rWANXuK3TjS7kl78Ep1Y+P6xLDtz7PdrhXTmOAf+XwL6XuKeezi0YQGtf7axoqVjUM067NLeHLRdn4yNJF7zh1+1IXXxqJoByvrjiqQNnd5FvPX5LBmTwnz0rOJCw/CHiis3VviVuyN8strSIoMab+hH9AevFLd3LEUAosJcyX4Lzbl8dOxPbksrU+b7Zt68Fa9m0c+30JCRBDPXDmeQUmRR7WPjwgCOOxGqUab9rvWj52Xns0pgxP46nczmZYaz5q9xU13zbbnkw37mfzwYlbt7h6riWqCV0q5rbEHD/CnOSPanfoY1awk8brsElZkFXHLzIEtjtcDTVM8j5xrX+NwsiO/ounna6f2Iy48iFG9oskvr6W4nZuvVu8p5tIXvufu+RsByCmuarO9v9AhGqWU2+LCXT3sESlRbs23b1wntrS6nueWZBIZYuOKya3fZXsowbt68MWVdSzfUUCtowFng2HGkEQOVtTyk2FJAPS06tzkltY0xdZccWUd63NKuPO99RQ2+6NRUevezJstB8roFx/W7uyirso3o1ZKeUVCRDCv3TDJ7Vkr9sAAggIDeOqr7QDcNXtomzcxxYTaCRDXGLzD2cDVL/3I5gNlTc8/fMGowy7oJke5xtJzy6oPm40D8NG6ffx23joaDPSMDuGRi0bz0jdZ7CyopLiy5btxGxljeOLLbfxj6U76xIXy759PYUBCuFvH3JXoEI1S6picOjTJrRk3jeqcDYBrWOVXMwe22TYgQIgLD6awopZXv9vF5gNlPHLRaAYnRdA7NpTesYdXpkyx5tznlh4as9+WW861L//Inz7axOhe0bxwzQSW/v5Urpzcl8V3nEpksK3FWTrNvfnjXv6xdCfnjUlhf0kN81fnuH28XYn24JVSHvXidWkECG4vPZgQEURBeR3f7zzI1NQ4rpzclysm9aG2vuGoMf/EyGBEILespmnbGyt2811mIdGhdp64dCyDkw+/mBsTbm+zB//NjgIe+ngzPxmayDNXjGf6o0vIa7b/9uzIKycs2NYlyiRrgldKedQZI9xfUxZcM2m+yyyk2uHkv04dBLhq0be0Dqw9MICEiGDySl0J2NlgWJiRx+xRPXj+6okt7j8uLIiiIy7KltU4uOn1dCpq69maW86Q5Eievnw8AQFCclQweeVHz+oBWLwlj9TECN5ZuZeBSRGkRIdw7csrSesXy/u/mn5Mx+0JmuCVUl3KsB5RfJd5kKDAAM4a1faKVAA9okLYsK+UV77dxaCkCAorajl7VEqr7WPDgyiyevCVtfW89M0uqurq+XFXEdMHxvPLGanceEoq0daU0KSoELKLjp518+Hafdw+bx0p0SEcKK0hLCiQxs8XW5pdN/AmTfBKqS7l92cNZUhyBMG2wMOmZbamR3QIizbn8eAnmwmxB5AUGcxp1iyblsSGBZGZX0FDg+G389bx5eY8AMb3jeH/bpp6VPvkqGDSj5g3X1hRyx8+2HhYcq93GsKCA7l8TE/eXZ1NjcN51KeOitp6Hvp4M4UVtdw3ZyQ5JVV8sGYfByvreMUD69ZqgldKdSkh9kAun+TegiVwaCpmZLCNirp6/nbZ2DYvAseGBVFS5eC7nYV8udl1w9bCTbncfEpqi+2TI0MornJQW+9suils7vIsahxOFvzmJB74eDNTBsQxtk8M8eHB7CyoYF56NrsKKxmecmhmT72zgZ+9spI1e4sJsgUw469Lm+I/dWgS9c6Gw5ZI7Aia4JVSPk2sgZEXr0+jb1xY09z41sSF26morWf+6hwig208fskYHr9kTItj/HBoKmZ+WS3z1+SwPa+czzNyuXB8LwYlRfLGL6a0+LqdBRWHJfinv9pB+p5i/n7FOMb1iWHJ1nwigm3MGduz1fc+UZrglVI+7X/OHsrU1DimDIhr985agJgw1w1RH67bz0Xje7WbXJOiXDdffbOjkKe/2kF0qJ0rJvXl3nOHt9h+QEI4IrAz31UMrbrOyecZB/jH15lcltab88f1AuCGk1peFrEjaYJXSvm0pMgQLm2nJk5zze94vWB8r3bbN/bgn/86E1uAsOSOmUcVQmsuNCiQXjGhbM8rp6Sqjkte+IHM/AqGJEdw/09Huh1nR9AEr5TqVmKsC7f948M4ZXBCu+0bE3xOcTWzhie3mdwbnTI4kfmrc9hXUs3eg1U8e+V4Zg1PbrPypidogldKdSsT+8fyuzOGcO3Ufm4N6cSG2Tl5UAIBAcLdZw9z6z1+c9og5q/OYV12Cc9cOZ45Y3ueaNjHRdwts9kZ0tLSTHp6urfDUEqpE/bh2n2I0DTm7ikistoYk9bSc9qDV0opD3BnfN/TtNiYUkr5KU3wSinlpzTBK6WUn9IEr5RSfkoTvFJK+SlN8Eop5ac0wSullJ/SBK+UUn6qS93JKiIFwJ7jfHkCUNiB4XiTHkvX4y/HAXosXdXxHks/Y0xiS090qQR/IkQkvbXbdX2NHkvX4y/HAXosXZUnjkWHaJRSyk9pgldKKT/lTwl+rrcD6EB6LF2PvxwH6LF0VR1+LH4zBq+UUupw/tSDV0op1YwmeKWU8lM+n+BFZLaIbBORTBG529vxHCsR2S0iG0VknYikW9viRGSRiOywvsd6O86WiMgrIpIvIhnNtrUau4j8r3WetonIWd6JumWtHMv9IrLPOjfrROScZs915WPpIyJLRWSLiGwSkdus7T51bto4Dp87LyISIiIrRWS9dSwPWNs9e06MMT77BQQCO4FUIAhYD4zwdlzHeAy7gYQjtj0O3G09vht4zNtxthL7DGACkNFe7MAI6/wEAwOs8xbo7WNo51juB+5soW1XP5YUYIL1OBLYbsXsU+emjePwufMCCBBhPbYDPwJTPX1OfL0HPxnINMZkGWPqgHeA870cU0c4H3jdevw6cIH3QmmdMWY5UHTE5tZiPx94xxhTa4zZBWTiOn9dQivH0pqufiwHjDFrrMflwBagFz52bto4jtZ0yeMAMC4V1o9268vg4XPi6wm+F5Dd7Occ2v4P0BUZ4EsRWS0iN1vbko0xB8D1nxxI8lp0x6612H31XP1GRDZYQziNH5995lhEpD8wHleP0WfPzRHHAT54XkQkUETWAfnAImOMx8+Jryd4aWGbr837PMkYMwE4G/gvEZnh7YA8xBfP1T+BgcA44ADwN2u7TxyLiEQA84HbjTFlbTVtYVuXOZ4WjsMnz4sxxmmMGQf0BiaLyKg2mnfIsfh6gs8B+jT7uTew30uxHBdjzH7rez7wAa6PYXkikgJgfc/3XoTHrLXYfe5cGWPyrF/KBuBFDn1E7vLHIiJ2XEnxLWPMf6zNPnduWjoOXz4vAMaYEuBrYDYePie+nuBXAYNFZICIBAFXAAu8HJPbRCRcRCIbHwNnAhm4juF6q9n1wEfeifC4tBb7AuAKEQkWkQHAYGClF+JzW+MvnuVCXOcGuvixiIgALwNbjDFPNnvKp85Na8fhi+dFRBJFJMZ6HArMArbi6XPi7avLHXB1+hxcV9d3Avd4O55jjD0V15Xy9cCmxviBeGAxsMP6HuftWFuJ/21cH5EduHocv2grduAe6zxtA872dvxuHMsbwEZgg/ULl+Ijx3Iyro/zG4B11tc5vnZu2jgOnzsvwBhgrRVzBvAna7tHz4mWKlBKKT/l60M0SimlWqEJXiml/JQmeKWU8lOa4JVSyk9pgldKKT+lCV55lIj8VNqp8ikiPUXk/Vae+1pE3F6IWETGNa8u2Ea7CjfatBt7C695TUQuOZbXtLGvK0XkniO2xVsVFitE5LkjnpsorsqkmSLyjDWPHGsu9Txr+4/Wbf+Nr7neqmS4Q0SuR/kVTfDKo4wxC4wxj7bTZr8xpkOSIq7b19tN8O5wJ3YPmw0sPGJbDfBH4M4W2v8TuBnXTTGDrdeDa05/sTFmEPAU8Bi4StUC9wFTcN0Nep900dLU6vhoglfHRUT6i8hWEXlJRDJE5C0RmSUi31m9wclWu5819jSt3u0zIvK9iGQ19nStfWW08XbXWK/JaLbfyda2tdb3odbdzA8Cl4urTvjlIhIhIq9aPdsNInJxs2N4WFz1uVeISHILx+hO7CIiz4nIZhH5lGaF4awe9TJxFZL7QkRSRCRaXPW9h1pt3haRm1p4b8H1x2pN8+3GmEpjzLe4En3z9ilAlDHmB+O6ueXfHF6ZsLFi4fvA6db+z8JV9KrIGFMMLOLQHwXlBzTBqxMxCPg7rrv0hgFX4br78E7gD628JsVqcx7gbu843BgzHfg18Iq1bSswwxgzHvgT8BfjKhn9J2CeMWacMWYert5uqTFmtDFmDLCkcZ/ACmPMWGA5cFSSdTP2C4GhwGhrH9OhqYbKs8AlxpiJVtwPG2NKgd8Ar4nIFUCsMebFFt5rPLDeuH8nYi9cd+A2al59sKkyoTGmHijFdQdll66+qE6czdsBKJ+2yxizEUBENgGLjTFGRDYC/Vt5zYfGVSRqc0u95la8Da6a7SISZdX0iAReF5HBuG5nt7fy2lm4ahRh7aPYelgHfGI9Xg2c4UYcLcU+A3jbGOME9otI4x+QocAoYJE1FB6IqxQCxphFInIp8A9gbCvvNRv43I2YGrVVfbC157p09UV14rQHr05EbbPHDc1+bqD1zkPz1xyVYKzhlHUi8lmzzUcmHQM8BCw1xowC5gAhrbyftPB6AEez3rGzjXjdib2l/QuwyfokMc76BHEmgIgEAMOBaiCulfc6E/jSjZga5eCqONioefXBpsqEImIDonEtbuIT1RfV8dMEr7oUY8wNVkJsfqH0cgARORnXcEspriS1z3r+Z83aluPq3Tf6EteQCNY+Ovoi4nJcVf8CrXHwn1jbtwGJIjLNel+7iIy0nvstrtWJrgResYZzmohINGAzxhx0NwjjWiyiXESmWuPr13F4ZcLGGTKXAEusP25fAGeKSKz173KmtU35CU3wyhcUi8j3wAu4ZoSAay3LR0TkO1zDH42WAiMaL7ICfwZirQu06zmUgDvKB7gqAW7ENYtlGYB1PeAS4DHrfdcB00VkCHAjcIcx5htcfyDuPWKfZwBftfaGIrIbeBL4mYjkiMgI66lfAS/hWt5tJ4eGeF4G4kUkE/gdrrU/McYU4foktMr6etDapvyEVpNUqosRkZeAl4wxK7wdi/JtmuCVUspP6RCNUkr5KU3wSinlpzTBK6WUn9IEr5RSfkoTvFJK+SlN8Eop5af+HwpiXhlJ/mnAAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725cbac",
   "metadata": {},
   "source": [
    "## Evaluate Model 1 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f8821aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 46 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_one(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae63ccd",
   "metadata": {},
   "source": [
    "## Evaluate Model 1 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "121b4ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 59 %\n",
      "Accuracy of aquarium_fish : 64 %\n",
      "Accuracy of  baby : 24 %\n",
      "Accuracy of  bear : 26 %\n",
      "Accuracy of beaver : 34 %\n",
      "Accuracy of   bed : 54 %\n",
      "Accuracy of   bee : 45 %\n",
      "Accuracy of beetle : 58 %\n",
      "Accuracy of bicycle : 57 %\n",
      "Accuracy of bottle : 58 %\n",
      "Accuracy of  bowl : 17 %\n",
      "Accuracy of   boy : 33 %\n",
      "Accuracy of bridge : 56 %\n",
      "Accuracy of   bus : 33 %\n",
      "Accuracy of butterfly : 25 %\n",
      "Accuracy of camel : 32 %\n",
      "Accuracy of   can : 31 %\n",
      "Accuracy of castle : 67 %\n",
      "Accuracy of caterpillar : 38 %\n",
      "Accuracy of cattle : 33 %\n",
      "Accuracy of chair : 78 %\n",
      "Accuracy of chimpanzee : 55 %\n",
      "Accuracy of clock : 48 %\n",
      "Accuracy of cloud : 52 %\n",
      "Accuracy of cockroach : 53 %\n",
      "Accuracy of couch : 44 %\n",
      "Accuracy of  crab : 47 %\n",
      "Accuracy of crocodile : 24 %\n",
      "Accuracy of   cup : 58 %\n",
      "Accuracy of dinosaur : 56 %\n",
      "Accuracy of dolphin : 34 %\n",
      "Accuracy of elephant : 52 %\n",
      "Accuracy of flatfish : 50 %\n",
      "Accuracy of forest : 62 %\n",
      "Accuracy of   fox : 49 %\n",
      "Accuracy of  girl : 33 %\n",
      "Accuracy of hamster : 44 %\n",
      "Accuracy of house : 42 %\n",
      "Accuracy of kangaroo : 28 %\n",
      "Accuracy of keyboard : 65 %\n",
      "Accuracy of  lamp : 51 %\n",
      "Accuracy of lawn_mower : 64 %\n",
      "Accuracy of leopard : 59 %\n",
      "Accuracy of  lion : 55 %\n",
      "Accuracy of lizard : 20 %\n",
      "Accuracy of lobster : 30 %\n",
      "Accuracy of   man : 21 %\n",
      "Accuracy of maple_tree : 44 %\n",
      "Accuracy of motorcycle : 79 %\n",
      "Accuracy of mountain : 51 %\n",
      "Accuracy of mouse : 36 %\n",
      "Accuracy of mushroom : 40 %\n",
      "Accuracy of oak_tree : 39 %\n",
      "Accuracy of orange : 65 %\n",
      "Accuracy of orchid : 53 %\n",
      "Accuracy of otter : 32 %\n",
      "Accuracy of palm_tree : 71 %\n",
      "Accuracy of  pear : 60 %\n",
      "Accuracy of pickup_truck : 73 %\n",
      "Accuracy of pine_tree : 56 %\n",
      "Accuracy of plain : 82 %\n",
      "Accuracy of plate : 57 %\n",
      "Accuracy of poppy : 59 %\n",
      "Accuracy of porcupine : 41 %\n",
      "Accuracy of possum : 15 %\n",
      "Accuracy of rabbit : 23 %\n",
      "Accuracy of raccoon : 47 %\n",
      "Accuracy of   ray : 33 %\n",
      "Accuracy of  road : 72 %\n",
      "Accuracy of rocket : 60 %\n",
      "Accuracy of  rose : 41 %\n",
      "Accuracy of   sea : 73 %\n",
      "Accuracy of  seal : 14 %\n",
      "Accuracy of shark : 34 %\n",
      "Accuracy of shrew : 23 %\n",
      "Accuracy of skunk : 70 %\n",
      "Accuracy of skyscraper : 70 %\n",
      "Accuracy of snail : 26 %\n",
      "Accuracy of snake : 49 %\n",
      "Accuracy of spider : 46 %\n",
      "Accuracy of squirrel : 22 %\n",
      "Accuracy of streetcar : 59 %\n",
      "Accuracy of sunflower : 80 %\n",
      "Accuracy of sweet_pepper : 28 %\n",
      "Accuracy of table : 28 %\n",
      "Accuracy of  tank : 55 %\n",
      "Accuracy of telephone : 73 %\n",
      "Accuracy of television : 46 %\n",
      "Accuracy of tiger : 32 %\n",
      "Accuracy of tractor : 56 %\n",
      "Accuracy of train : 49 %\n",
      "Accuracy of trout : 56 %\n",
      "Accuracy of tulip : 29 %\n",
      "Accuracy of turtle : 25 %\n",
      "Accuracy of wardrobe : 84 %\n",
      "Accuracy of whale : 62 %\n",
      "Accuracy of willow_tree : 45 %\n",
      "Accuracy of  wolf : 32 %\n",
      "Accuracy of woman : 27 %\n",
      "Accuracy of  worm : 52 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_one(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679cc8ab",
   "metadata": {},
   "source": [
    "## Model 2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62c0939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_two = ConvutionModelAddedLayersBatchNorm()\n",
    "model_two.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(model_two.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0211a928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.575\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.380\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.173\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.080\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 3.978\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 3.894\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 3.807\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 3.701\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 3.635\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 3.529\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 3.487\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 3.423\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 3.233\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 3.179\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 3.202\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 3.130\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 3.080\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 2.991\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 2.942\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 2.956\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 2.897\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 2.848\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 2.861\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 2.784\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 2.586\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 2.576\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 2.583\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 2.578\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 2.570\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 2.472\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 2.538\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 2.512\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 2.468\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 2.459\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 2.400\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 2.412\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 2.104\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 2.081\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 2.142\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 2.135\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 2.163\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 2.187\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 2.150\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 2.141\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 2.118\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 2.134\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 2.098\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 2.156\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 1.729\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 1.741\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 1.766\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 1.788\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 1.851\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 1.778\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 1.850\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 1.861\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 1.826\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 1.857\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 1.811\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 1.848\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 1.336\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 1.373\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 1.434\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 1.470\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 1.500\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 1.543\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 1.520\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 1.530\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 1.540\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 1.539\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 1.573\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 1.576\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 1.021\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 1.066\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 1.153\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 1.164\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 1.232\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 1.201\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 1.248\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 1.328\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 1.326\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 1.314\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 1.278\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 1.297\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 0.799\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 0.797\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 0.924\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 0.939\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 0.956\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 1.019\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 1.001\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 0.999\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 1.098\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 1.061\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 1.069\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 1.102\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 0.627\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 0.697\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 0.724\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 0.759\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 0.778\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 0.853\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 0.811\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 0.870\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 0.868\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 0.866\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 0.883\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 0.910\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 0.499\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 0.548\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 0.590\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 0.623\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 0.625\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 0.645\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 0.656\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 0.709\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 0.674\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 0.692\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 0.716\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 0.739\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 0.409\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 0.442\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 0.462\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 0.462\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 0.494\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 0.522\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 0.562\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 0.511\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 0.560\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 0.575\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 0.549\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 0.599\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 0.329\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 0.349\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 0.396\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 0.416\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 0.422\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 0.436\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 0.466\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 0.484\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 0.487\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 0.441\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 0.474\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 0.456\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 0.278\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 0.328\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 0.301\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 0.352\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 0.343\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 0.326\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 0.403\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 0.371\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 0.390\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 0.390\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 0.424\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 0.405\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 0.190\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 0.217\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 0.273\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 0.273\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 0.258\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 0.294\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 0.314\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 0.309\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 0.326\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 0.348\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 0.325\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 0.355\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 0.210\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 0.221\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 0.240\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 0.240\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 0.257\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 0.210\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 0.281\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 0.271\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 0.257\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 0.296\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 0.274\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 0.316\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 0.202\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 0.166\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 0.217\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 0.201\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 0.234\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 0.213\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 0.194\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 0.234\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 0.250\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 0.232\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 0.303\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 0.236\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 0.168\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 0.181\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 0.175\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 0.151\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 0.179\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 0.166\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 0.164\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 0.202\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 0.179\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 0.225\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 0.225\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 0.195\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 0.119\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 0.098\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 0.143\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 0.118\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 0.156\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 0.147\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 0.161\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 0.154\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 0.196\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 0.194\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 0.173\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 0.190\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 0.103\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 0.126\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 0.132\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 0.134\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 0.151\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 0.155\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 0.154\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 0.145\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 0.187\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 0.174\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 0.174\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 0.158\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 0.086\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 0.126\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 0.129\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 0.120\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 0.133\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 0.120\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 0.110\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 0.148\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 0.143\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 0.156\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 0.178\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 0.157\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 0.090\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 0.087\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 0.098\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 0.119\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 0.108\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 0.130\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 0.099\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 0.118\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 0.123\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 0.126\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 0.151\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 0.129\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 0.076\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 0.085\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 0.076\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 0.114\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 0.131\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 0.097\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 0.101\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 0.107\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 0.104\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 0.138\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 0.100\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 0.120\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 0.095\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 0.102\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 0.082\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 0.091\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 0.104\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 0.089\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 0.129\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 0.108\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 0.116\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 0.109\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 0.138\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 0.130\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 0.082\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 0.102\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 0.074\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 0.112\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 0.102\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 0.100\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 0.086\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 0.101\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 0.110\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 0.114\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 0.104\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 0.108\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 0.050\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 0.073\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 0.073\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 0.069\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 0.087\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 0.091\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 0.088\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 0.086\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 0.090\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 0.109\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 0.106\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 0.106\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 0.088\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 0.070\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 0.065\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 0.100\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 0.072\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 0.074\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 0.073\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 0.087\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 0.053\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 0.066\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 0.098\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 0.088\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 0.056\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 0.048\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 0.062\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 0.072\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 0.085\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 0.075\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 0.072\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 0.044\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 0.091\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 0.086\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 0.078\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 0.105\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 0.050\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 0.064\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 0.070\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 0.083\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 0.070\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 0.079\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 0.086\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 0.069\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 0.060\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 0.095\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 0.094\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 0.065\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 0.055\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 0.038\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 0.056\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 0.053\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 0.059\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 0.059\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 0.069\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 0.073\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 0.076\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 0.057\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 0.066\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 0.075\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 0.065\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 0.055\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 0.063\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 0.067\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 0.060\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 0.072\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 0.061\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 0.082\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 0.067\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 0.067\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 0.072\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 0.060\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 30      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = model_two(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17924a",
   "metadata": {},
   "source": [
    "## Model 2 Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cab5eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccd94f",
   "metadata": {},
   "source": [
    "## Evaluate Model 2 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d70d0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 44 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_two(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220ec71",
   "metadata": {},
   "source": [
    "##  Evaluate Model 2 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5011e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 73 %\n",
      "Accuracy of aquarium_fish : 56 %\n",
      "Accuracy of  baby : 35 %\n",
      "Accuracy of  bear : 23 %\n",
      "Accuracy of beaver : 24 %\n",
      "Accuracy of   bed : 39 %\n",
      "Accuracy of   bee : 47 %\n",
      "Accuracy of beetle : 41 %\n",
      "Accuracy of bicycle : 64 %\n",
      "Accuracy of bottle : 61 %\n",
      "Accuracy of  bowl : 42 %\n",
      "Accuracy of   boy : 28 %\n",
      "Accuracy of bridge : 15 %\n",
      "Accuracy of   bus : 39 %\n",
      "Accuracy of butterfly : 39 %\n",
      "Accuracy of camel : 33 %\n",
      "Accuracy of   can : 47 %\n",
      "Accuracy of castle : 68 %\n",
      "Accuracy of caterpillar : 28 %\n",
      "Accuracy of cattle : 39 %\n",
      "Accuracy of chair : 75 %\n",
      "Accuracy of chimpanzee : 51 %\n",
      "Accuracy of clock : 40 %\n",
      "Accuracy of cloud : 59 %\n",
      "Accuracy of cockroach : 73 %\n",
      "Accuracy of couch : 28 %\n",
      "Accuracy of  crab : 40 %\n",
      "Accuracy of crocodile : 22 %\n",
      "Accuracy of   cup : 67 %\n",
      "Accuracy of dinosaur : 34 %\n",
      "Accuracy of dolphin : 29 %\n",
      "Accuracy of elephant : 44 %\n",
      "Accuracy of flatfish : 35 %\n",
      "Accuracy of forest : 42 %\n",
      "Accuracy of   fox : 50 %\n",
      "Accuracy of  girl : 23 %\n",
      "Accuracy of hamster : 42 %\n",
      "Accuracy of house : 36 %\n",
      "Accuracy of kangaroo : 20 %\n",
      "Accuracy of keyboard : 59 %\n",
      "Accuracy of  lamp : 41 %\n",
      "Accuracy of lawn_mower : 65 %\n",
      "Accuracy of leopard : 32 %\n",
      "Accuracy of  lion : 50 %\n",
      "Accuracy of lizard : 19 %\n",
      "Accuracy of lobster : 23 %\n",
      "Accuracy of   man : 25 %\n",
      "Accuracy of maple_tree : 63 %\n",
      "Accuracy of motorcycle : 76 %\n",
      "Accuracy of mountain : 53 %\n",
      "Accuracy of mouse : 25 %\n",
      "Accuracy of mushroom : 39 %\n",
      "Accuracy of oak_tree : 54 %\n",
      "Accuracy of orange : 90 %\n",
      "Accuracy of orchid : 51 %\n",
      "Accuracy of otter : 23 %\n",
      "Accuracy of palm_tree : 62 %\n",
      "Accuracy of  pear : 40 %\n",
      "Accuracy of pickup_truck : 66 %\n",
      "Accuracy of pine_tree : 50 %\n",
      "Accuracy of plain : 79 %\n",
      "Accuracy of plate : 57 %\n",
      "Accuracy of poppy : 36 %\n",
      "Accuracy of porcupine : 37 %\n",
      "Accuracy of possum : 25 %\n",
      "Accuracy of rabbit : 23 %\n",
      "Accuracy of raccoon : 27 %\n",
      "Accuracy of   ray : 33 %\n",
      "Accuracy of  road : 70 %\n",
      "Accuracy of rocket : 67 %\n",
      "Accuracy of  rose : 62 %\n",
      "Accuracy of   sea : 43 %\n",
      "Accuracy of  seal : 13 %\n",
      "Accuracy of shark : 32 %\n",
      "Accuracy of shrew : 24 %\n",
      "Accuracy of skunk : 77 %\n",
      "Accuracy of skyscraper : 71 %\n",
      "Accuracy of snail : 31 %\n",
      "Accuracy of snake : 32 %\n",
      "Accuracy of spider : 36 %\n",
      "Accuracy of squirrel : 16 %\n",
      "Accuracy of streetcar : 36 %\n",
      "Accuracy of sunflower : 82 %\n",
      "Accuracy of sweet_pepper : 43 %\n",
      "Accuracy of table : 32 %\n",
      "Accuracy of  tank : 41 %\n",
      "Accuracy of telephone : 49 %\n",
      "Accuracy of television : 48 %\n",
      "Accuracy of tiger : 47 %\n",
      "Accuracy of tractor : 51 %\n",
      "Accuracy of train : 37 %\n",
      "Accuracy of trout : 58 %\n",
      "Accuracy of tulip : 48 %\n",
      "Accuracy of turtle : 21 %\n",
      "Accuracy of wardrobe : 81 %\n",
      "Accuracy of whale : 39 %\n",
      "Accuracy of willow_tree : 36 %\n",
      "Accuracy of  wolf : 57 %\n",
      "Accuracy of woman : 31 %\n",
      "Accuracy of  worm : 58 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_two(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602a1c7",
   "metadata": {},
   "source": [
    "## Model 3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b81a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_three = ResNetMultiheadAttention()\n",
    "model_three.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(model_three.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "689bcaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.690\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.461\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.379\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.255\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.204\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.189\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.121\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.111\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.021\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 3.987\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 3.970\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 3.919\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 3.869\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 3.838\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 3.819\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 3.828\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 3.793\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 3.786\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 3.730\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 3.722\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 3.677\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 3.663\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 3.639\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 3.659\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 3.581\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 3.580\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.590\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.525\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.550\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.536\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.518\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.482\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.434\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.433\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.431\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.419\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.333\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.354\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.309\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.298\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.292\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.275\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.283\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.244\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.252\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.265\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.276\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.286\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.151\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.169\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.176\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.132\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.137\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.137\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.112\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.121\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.130\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.077\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.105\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.084\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.980\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.990\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 3.059\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 3.015\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.983\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 3.016\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 3.021\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.928\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.912\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.986\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.985\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.978\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.827\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.809\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.850\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.858\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.862\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.844\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.907\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.861\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.866\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.869\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.868\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.844\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.787\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.714\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.743\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.775\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.750\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.764\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.767\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.774\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.783\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.728\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.762\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.767\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.641\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.682\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.657\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.666\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.608\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.691\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.691\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.682\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.660\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.650\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.673\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.664\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 2.551\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 2.564\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 2.527\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 2.497\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 2.552\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 2.567\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 2.604\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 2.572\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 2.586\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 2.609\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 2.594\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 2.563\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 2.430\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 2.457\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 2.537\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 2.514\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 2.491\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 2.491\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 2.449\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 2.470\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 2.517\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 2.508\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 2.524\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 2.484\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 2.371\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 2.379\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 2.392\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 2.407\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 2.445\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 2.383\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 2.434\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 2.407\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 2.447\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 2.400\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 2.445\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 2.456\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 2.319\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 2.306\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 2.352\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 2.375\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 2.323\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 2.346\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 2.351\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 2.298\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 2.302\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 2.354\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 2.386\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 2.344\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 2.192\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 2.237\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 2.189\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 2.283\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 2.276\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 2.259\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 2.258\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 2.283\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 2.246\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 2.311\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 2.290\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 2.334\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 2.177\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 2.204\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 2.183\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 2.206\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 2.175\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 2.192\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 2.269\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 2.206\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 2.266\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 2.215\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 2.229\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 2.202\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 2.111\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 2.088\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 2.138\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 2.161\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 2.124\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 2.156\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 2.147\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 2.139\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 2.152\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 2.186\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 2.152\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 2.183\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 1.989\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 2.043\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 2.070\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 2.068\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 2.094\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 2.090\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 2.110\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 2.134\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 2.059\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 2.143\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 2.126\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 2.132\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 1.960\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 2.001\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 1.971\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 2.009\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 2.039\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 2.045\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 2.015\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 2.059\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 2.073\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 2.056\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 2.046\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 2.052\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 1.907\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 1.962\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 1.939\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 1.921\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 1.962\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 2.036\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 1.981\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 1.974\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 2.002\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 2.022\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 2.061\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 2.024\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 1.803\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 1.895\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 1.882\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 1.941\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 1.909\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 1.890\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 1.920\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 1.953\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 1.943\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 1.954\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 1.965\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 1.984\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 1.803\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 1.854\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 1.819\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 1.850\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 1.922\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 1.903\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 1.857\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 1.883\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 1.901\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 1.893\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 1.935\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 1.910\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 1.725\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 1.790\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 1.801\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 1.749\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 1.858\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 1.810\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 1.876\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 1.882\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 1.881\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 1.827\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 1.909\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 1.873\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 1.709\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 1.709\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 1.774\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 1.716\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 1.724\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 1.810\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 1.804\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 1.791\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 1.824\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 1.857\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 1.832\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 1.868\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 1.670\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 1.679\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 1.655\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 1.766\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 1.740\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 1.757\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 1.745\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 1.753\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 1.770\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 1.818\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 1.776\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 1.764\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 1.592\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 1.623\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 1.679\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 1.705\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 1.690\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 1.623\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 1.701\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 1.709\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 1.761\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 1.698\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 1.773\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 1.753\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 1.572\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 1.602\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 1.622\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 1.581\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 1.645\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 1.653\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 1.702\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 1.741\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 1.636\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 1.639\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 1.752\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 1.736\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 1.556\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 1.544\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 1.622\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 1.605\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 1.611\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 1.601\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 1.660\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 1.628\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 1.655\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 1.626\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 1.686\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 1.660\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 1.473\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 1.497\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 1.510\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 1.539\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 1.548\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 1.571\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 1.625\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 1.588\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 1.569\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 1.613\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 1.669\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 1.618\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 1.427\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 1.495\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 1.472\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 1.611\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 1.535\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 1.564\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 1.542\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 1.547\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 1.553\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 1.597\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 1.571\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 1.561\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 1.410\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 1.434\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 1.420\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 1.422\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 1.499\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 1.491\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 1.558\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 1.525\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 1.522\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 1.561\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 1.564\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 1.568\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 1.392\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 1.409\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 1.426\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 1.463\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 1.440\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 1.474\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 1.463\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 1.511\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 1.546\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 1.518\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 1.525\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 1.518\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 1.323\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 1.331\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 1.360\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 1.369\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 1.404\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 1.439\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 1.461\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 1.456\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 1.456\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 1.501\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 1.469\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 1.477\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 1.265\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 1.310\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 1.350\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 1.380\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 1.369\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 1.365\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 1.423\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 1.445\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 1.454\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 1.391\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 1.447\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 1.465\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 1.240\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 1.313\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 1.306\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 1.379\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 1.313\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 1.390\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 1.358\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 1.348\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 1.371\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 1.400\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 1.442\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 1.436\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 1.197\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 1.256\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 1.269\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 1.291\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 1.349\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 1.307\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 1.321\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 1.351\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 1.358\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 1.347\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 1.388\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 1.431\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 1.212\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 1.212\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 1.267\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 1.247\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 1.242\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 1.253\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 1.327\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 1.340\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 1.320\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 1.301\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 1.325\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 1.339\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 1.126\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 1.159\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 1.227\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 1.190\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 1.250\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 1.249\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 1.279\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 1.257\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 1.270\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 1.337\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 1.322\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 1.317\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 1.174\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 1.118\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 1.228\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 1.231\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 1.182\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 1.222\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 1.186\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 1.243\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 1.227\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 1.239\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 1.282\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 1.238\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 1.090\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 1.102\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 1.149\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 1.171\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 1.184\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 1.160\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 1.216\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 1.194\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 1.185\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 1.251\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 1.226\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 1.276\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 1.099\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 1.108\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 1.199\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 1.158\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 1.163\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 1.179\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 1.131\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 1.179\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 1.216\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 1.221\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 1.188\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 1.231\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 1.044\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 1.049\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 1.078\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 1.120\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 1.117\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 1.176\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 1.094\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 1.194\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 1.148\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 1.226\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 1.203\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 1.181\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 0.990\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 1.035\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 1.077\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 1.093\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 1.099\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 1.085\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 1.124\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 1.162\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 1.115\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 1.140\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 1.098\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 1.160\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 0.974\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 0.989\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 1.004\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 1.049\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 1.078\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 1.114\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 1.059\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 1.122\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 1.107\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 1.180\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 1.127\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 1.128\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 0.976\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 1.040\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 1.009\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 1.026\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 1.000\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 1.050\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 1.075\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 1.079\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 1.094\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 1.084\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 1.106\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 1.133\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 0.924\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 0.975\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 0.955\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 1.034\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 1.052\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 0.972\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 1.074\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 1.064\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 1.026\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 1.035\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 1.054\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 1.076\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 0.889\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 0.955\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 0.951\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 0.981\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 0.991\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 1.000\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 1.030\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 1.019\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 1.033\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 1.085\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 1.046\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 1.083\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 0.886\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 0.919\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 0.929\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 0.930\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 1.018\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 0.959\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 0.957\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 1.040\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 1.010\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 1.019\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 1.033\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 1.041\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 0.882\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 0.871\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 0.914\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 0.943\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 0.967\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 0.986\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 0.974\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 0.958\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 1.023\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 0.996\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 1.016\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 1.043\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 0.799\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 0.887\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 0.934\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 0.894\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 0.924\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 0.927\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 0.972\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 0.981\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 0.946\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 0.973\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 0.984\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 0.985\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 0.801\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 0.815\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 0.827\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 0.856\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 0.860\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 0.953\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 0.950\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 0.920\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 0.934\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 0.947\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 0.981\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 0.946\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 0.786\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 0.785\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 0.809\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 0.849\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 0.899\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 0.923\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 0.878\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 0.923\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 0.937\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 0.940\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 0.961\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 0.924\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 0.784\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 0.746\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 0.816\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 0.819\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 0.862\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 0.867\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 0.908\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 0.857\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 0.920\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 0.896\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 0.905\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 0.874\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 0.766\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 0.738\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 0.764\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 0.861\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 0.805\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 0.847\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 0.813\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 0.847\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 0.844\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 0.841\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 0.881\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 0.909\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 0.735\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 0.747\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 0.778\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 0.758\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 0.812\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 0.822\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 0.839\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 0.844\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 0.876\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 0.875\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 0.900\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 0.855\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 0.707\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 0.755\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 0.724\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 0.829\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 0.805\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 0.779\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 0.834\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 0.869\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 0.803\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 0.832\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 0.840\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 0.849\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 0.705\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 0.670\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 0.699\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 0.742\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 0.761\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 0.787\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 0.795\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 0.815\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 0.822\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 0.822\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 0.810\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 0.847\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 0.696\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 0.645\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 0.700\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 0.713\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 0.718\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 0.767\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 0.789\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 0.776\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 0.753\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 0.759\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 0.776\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 0.839\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 0.650\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 0.681\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 0.709\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 0.732\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 0.756\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 0.737\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 0.727\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 0.758\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 0.732\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 0.749\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 0.783\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 0.814\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 0.645\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 0.596\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 0.675\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 0.695\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 0.707\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 0.712\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 0.712\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 0.770\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 0.749\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 0.754\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 0.744\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 0.745\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 0.651\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 0.653\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 0.640\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 0.707\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 0.676\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 0.715\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 0.700\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 0.703\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 0.738\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 0.730\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 0.755\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 0.730\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 0.591\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 0.646\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 0.670\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 0.640\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 0.685\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 0.695\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 0.652\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 0.657\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 0.717\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 0.703\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 0.763\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 0.757\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 0.565\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 0.592\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 0.601\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 0.664\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 0.656\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 0.665\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 0.681\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 0.684\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 0.707\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 0.672\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 0.724\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 0.704\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 0.565\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 0.538\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 0.553\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 0.595\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 0.627\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 0.619\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 0.704\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 0.692\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 0.694\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 0.664\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 0.683\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 0.719\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 0.559\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 0.553\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 0.565\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 0.612\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 0.648\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 0.588\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 0.614\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 0.602\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 0.662\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 0.643\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 0.703\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 0.707\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 0.537\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 0.516\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 0.537\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 0.561\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 0.631\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 0.594\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 0.661\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 0.629\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 0.652\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 0.650\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 0.652\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 0.643\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 0.567\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 0.563\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 0.646\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 0.572\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 0.556\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 0.587\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 0.582\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 0.636\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 0.608\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 0.598\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 0.639\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 0.684\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 0.567\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 0.513\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 0.537\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 0.564\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 0.526\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 0.535\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 0.623\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 0.611\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 0.607\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 0.593\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 0.618\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 0.610\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 0.488\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 0.524\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 0.527\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 0.535\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 0.573\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 0.555\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 0.567\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 0.600\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 0.596\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 0.605\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 0.617\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 0.582\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 0.462\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 0.496\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 0.495\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 0.574\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 0.564\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 0.515\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 0.548\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 0.579\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 0.580\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 0.597\n",
      "[epoch: 68, i: 10999] avg mini-batch loss: 0.583\n",
      "[epoch: 68, i: 11999] avg mini-batch loss: 0.633\n",
      "[epoch: 69, i:   999] avg mini-batch loss: 0.473\n",
      "[epoch: 69, i:  1999] avg mini-batch loss: 0.490\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 0.509\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 0.523\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 0.546\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 0.554\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 0.547\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 0.537\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 0.536\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 0.579\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 0.562\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 0.577\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 0.494\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 0.493\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 0.494\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 0.472\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 0.495\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 0.512\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 0.503\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 0.528\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 0.572\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 0.544\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 0.571\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 0.555\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 0.418\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 0.459\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 0.474\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 0.489\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 0.501\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 0.486\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 0.530\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 0.486\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 0.515\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 0.547\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 0.537\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 0.544\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 0.421\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 0.454\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 0.470\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 0.479\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 0.475\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 0.474\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 0.468\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 0.516\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 0.485\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 0.541\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 0.494\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 0.526\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 0.426\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 0.420\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 0.437\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 0.440\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 0.440\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 0.466\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 0.507\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 0.493\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 0.480\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 0.512\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 0.514\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 0.514\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 0.403\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 0.425\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 0.423\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 0.442\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 0.438\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 0.445\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 0.473\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 0.482\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 0.452\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 0.524\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 0.491\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 0.503\n",
      "[epoch: 75, i:   999] avg mini-batch loss: 0.418\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: 0.430\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: 0.397\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: 0.472\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: 0.434\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: 0.468\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: 0.458\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: 0.462\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: 0.498\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: 0.466\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: 0.493\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: 0.501\n",
      "[epoch: 76, i:   999] avg mini-batch loss: 0.394\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: 0.425\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: 0.413\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: 0.456\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: 0.408\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: 0.406\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: 0.436\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: 0.460\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: 0.478\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: 0.454\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: 0.457\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: 0.470\n",
      "[epoch: 77, i:   999] avg mini-batch loss: 0.367\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: 0.383\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: 0.402\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: 0.488\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: 0.415\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: 0.403\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: 0.476\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: 0.421\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: 0.421\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: 0.440\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: 0.443\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: 0.450\n",
      "[epoch: 78, i:   999] avg mini-batch loss: 0.352\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: 0.395\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: 0.389\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: 0.411\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: 0.404\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: 0.393\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: 0.416\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: 0.445\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: 0.410\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: 0.432\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: 0.428\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: 0.459\n",
      "[epoch: 79, i:   999] avg mini-batch loss: 0.363\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: 0.351\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: 0.342\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: 0.387\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: 0.386\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: 0.347\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: 0.403\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: 0.393\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: 0.445\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: 0.471\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: 0.431\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: 0.470\n",
      "[epoch: 80, i:   999] avg mini-batch loss: 0.402\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: 0.354\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: 0.383\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: 0.402\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: 0.377\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: 0.412\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: 0.383\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: 0.407\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: 0.416\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: 0.450\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: 0.393\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: 0.397\n",
      "[epoch: 81, i:   999] avg mini-batch loss: 0.343\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: 0.340\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: 0.394\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: 0.372\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: 0.345\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: 0.381\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: 0.395\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: 0.404\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: 0.392\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: 0.422\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: 0.418\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: 0.415\n",
      "[epoch: 82, i:   999] avg mini-batch loss: 0.368\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: 0.343\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: 0.365\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: 0.348\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: 0.375\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: 0.338\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: 0.324\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: 0.375\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: 0.368\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: 0.371\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: 0.399\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: 0.412\n",
      "[epoch: 83, i:   999] avg mini-batch loss: 0.306\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: 0.313\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: 0.306\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: 0.347\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: 0.348\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: 0.390\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: 0.380\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: 0.402\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: 0.384\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: 0.383\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: 0.379\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: 0.428\n",
      "[epoch: 84, i:   999] avg mini-batch loss: 0.343\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: 0.306\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: 0.353\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: 0.314\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: 0.395\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: 0.349\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: 0.372\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: 0.351\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: 0.367\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: 0.399\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: 0.372\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: 0.384\n",
      "[epoch: 85, i:   999] avg mini-batch loss: 0.316\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: 0.337\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: 0.348\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: 0.334\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: 0.337\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: 0.369\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: 0.342\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: 0.374\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: 0.356\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: 0.363\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: 0.361\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: 0.379\n",
      "[epoch: 86, i:   999] avg mini-batch loss: 0.310\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: 0.264\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: 0.334\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: 0.285\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: 0.353\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: 0.370\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: 0.340\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: 0.342\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: 0.368\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: 0.370\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: 0.344\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: 0.358\n",
      "[epoch: 87, i:   999] avg mini-batch loss: 0.271\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: 0.304\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: 0.282\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: 0.291\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: 0.324\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: 0.346\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: 0.354\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: 0.372\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: 0.351\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: 0.368\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: 0.353\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: 0.377\n",
      "[epoch: 88, i:   999] avg mini-batch loss: 0.300\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: 0.282\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: 0.320\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: 0.297\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: 0.310\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: 0.348\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: 0.327\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: 0.325\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: 0.349\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: 0.334\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: 0.372\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: 0.351\n",
      "[epoch: 89, i:   999] avg mini-batch loss: 0.300\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: 0.275\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: 0.267\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: 0.314\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: 0.328\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: 0.330\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: 0.337\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: 0.339\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: 0.341\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: 0.320\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: 0.329\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: 0.323\n",
      "[epoch: 90, i:   999] avg mini-batch loss: 0.268\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: 0.254\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: 0.295\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: 0.268\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: 0.284\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: 0.309\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: 0.293\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: 0.332\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: 0.319\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: 0.306\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: 0.355\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: 0.337\n",
      "[epoch: 91, i:   999] avg mini-batch loss: 0.288\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: 0.284\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: 0.269\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: 0.305\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: 0.276\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: 0.320\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: 0.286\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: 0.307\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: 0.338\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: 0.322\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: 0.313\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: 0.312\n",
      "[epoch: 92, i:   999] avg mini-batch loss: 0.275\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: 0.297\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: 0.247\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: 0.291\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: 0.257\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: 0.269\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: 0.290\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: 0.281\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: 0.321\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: 0.305\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: 0.318\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: 0.334\n",
      "[epoch: 93, i:   999] avg mini-batch loss: 0.241\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: 0.262\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: 0.257\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: 0.283\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: 0.312\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: 0.304\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: 0.305\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: 0.290\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: 0.319\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: 0.326\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: 0.308\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: 0.301\n",
      "[epoch: 94, i:   999] avg mini-batch loss: 0.301\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: 0.309\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: 0.303\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: 0.274\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: 0.309\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: 0.302\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: 0.291\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: 0.272\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: 0.298\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: 0.338\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: 0.338\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: 0.289\n",
      "[epoch: 95, i:   999] avg mini-batch loss: 0.227\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: 0.265\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: 0.286\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: 0.284\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: 0.273\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: 0.263\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: 0.276\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: 0.298\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: 0.276\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: 0.295\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: 0.303\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: 0.293\n",
      "[epoch: 96, i:   999] avg mini-batch loss: 0.274\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: 0.238\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: 0.261\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: 0.244\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: 0.257\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: 0.281\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: 0.276\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: 0.306\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: 0.277\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: 0.283\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: 0.280\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: 0.276\n",
      "[epoch: 97, i:   999] avg mini-batch loss: 0.210\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: 0.256\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: 0.220\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: 0.244\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: 0.231\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: 0.258\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: 0.242\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: 0.274\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: 0.277\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: 0.279\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: 0.275\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: 0.282\n",
      "[epoch: 98, i:   999] avg mini-batch loss: 0.214\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: 0.223\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: 0.237\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: 0.234\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: 0.275\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: 0.241\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: 0.274\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: 0.274\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: 0.261\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: 0.264\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: 0.292\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: 0.311\n",
      "[epoch: 99, i:   999] avg mini-batch loss: 0.230\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: 0.199\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: 0.227\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: 0.247\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: 0.221\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: 0.281\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: 0.266\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: 0.225\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: 0.230\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: 0.246\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: 0.287\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: 0.264\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 100      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = model_three(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d821ca4c",
   "metadata": {},
   "source": [
    "## Model 3 Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0159fcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db6631",
   "metadata": {},
   "source": [
    "## Evaluate Model 3 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "339f8f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_three(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a67dd92",
   "metadata": {},
   "source": [
    "## Evaluate Model 3 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c6f89c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 61 %\n",
      "Accuracy of aquarium_fish : 53 %\n",
      "Accuracy of  baby : 27 %\n",
      "Accuracy of  bear : 21 %\n",
      "Accuracy of beaver : 25 %\n",
      "Accuracy of   bed : 45 %\n",
      "Accuracy of   bee : 33 %\n",
      "Accuracy of beetle : 35 %\n",
      "Accuracy of bicycle : 46 %\n",
      "Accuracy of bottle : 48 %\n",
      "Accuracy of  bowl : 23 %\n",
      "Accuracy of   boy : 30 %\n",
      "Accuracy of bridge : 43 %\n",
      "Accuracy of   bus : 41 %\n",
      "Accuracy of butterfly : 30 %\n",
      "Accuracy of camel : 30 %\n",
      "Accuracy of   can : 42 %\n",
      "Accuracy of castle : 54 %\n",
      "Accuracy of caterpillar : 26 %\n",
      "Accuracy of cattle : 35 %\n",
      "Accuracy of chair : 69 %\n",
      "Accuracy of chimpanzee : 47 %\n",
      "Accuracy of clock : 46 %\n",
      "Accuracy of cloud : 55 %\n",
      "Accuracy of cockroach : 50 %\n",
      "Accuracy of couch : 26 %\n",
      "Accuracy of  crab : 33 %\n",
      "Accuracy of crocodile : 24 %\n",
      "Accuracy of   cup : 64 %\n",
      "Accuracy of dinosaur : 39 %\n",
      "Accuracy of dolphin : 52 %\n",
      "Accuracy of elephant : 37 %\n",
      "Accuracy of flatfish : 44 %\n",
      "Accuracy of forest : 32 %\n",
      "Accuracy of   fox : 23 %\n",
      "Accuracy of  girl : 25 %\n",
      "Accuracy of hamster : 28 %\n",
      "Accuracy of house : 38 %\n",
      "Accuracy of kangaroo : 12 %\n",
      "Accuracy of keyboard : 60 %\n",
      "Accuracy of  lamp : 36 %\n",
      "Accuracy of lawn_mower : 54 %\n",
      "Accuracy of leopard : 30 %\n",
      "Accuracy of  lion : 46 %\n",
      "Accuracy of lizard : 15 %\n",
      "Accuracy of lobster : 22 %\n",
      "Accuracy of   man : 24 %\n",
      "Accuracy of maple_tree : 41 %\n",
      "Accuracy of motorcycle : 74 %\n",
      "Accuracy of mountain : 54 %\n",
      "Accuracy of mouse : 19 %\n",
      "Accuracy of mushroom : 35 %\n",
      "Accuracy of oak_tree : 60 %\n",
      "Accuracy of orange : 71 %\n",
      "Accuracy of orchid : 58 %\n",
      "Accuracy of otter :  8 %\n",
      "Accuracy of palm_tree : 66 %\n",
      "Accuracy of  pear : 45 %\n",
      "Accuracy of pickup_truck : 48 %\n",
      "Accuracy of pine_tree : 35 %\n",
      "Accuracy of plain : 68 %\n",
      "Accuracy of plate : 53 %\n",
      "Accuracy of poppy : 38 %\n",
      "Accuracy of porcupine : 36 %\n",
      "Accuracy of possum : 16 %\n",
      "Accuracy of rabbit : 23 %\n",
      "Accuracy of raccoon : 27 %\n",
      "Accuracy of   ray : 30 %\n",
      "Accuracy of  road : 65 %\n",
      "Accuracy of rocket : 58 %\n",
      "Accuracy of  rose : 41 %\n",
      "Accuracy of   sea : 53 %\n",
      "Accuracy of  seal : 12 %\n",
      "Accuracy of shark : 32 %\n",
      "Accuracy of shrew : 28 %\n",
      "Accuracy of skunk : 65 %\n",
      "Accuracy of skyscraper : 62 %\n",
      "Accuracy of snail : 19 %\n",
      "Accuracy of snake : 35 %\n",
      "Accuracy of spider : 41 %\n",
      "Accuracy of squirrel : 16 %\n",
      "Accuracy of streetcar : 28 %\n",
      "Accuracy of sunflower : 59 %\n",
      "Accuracy of sweet_pepper : 29 %\n",
      "Accuracy of table : 31 %\n",
      "Accuracy of  tank : 49 %\n",
      "Accuracy of telephone : 38 %\n",
      "Accuracy of television : 50 %\n",
      "Accuracy of tiger : 23 %\n",
      "Accuracy of tractor : 38 %\n",
      "Accuracy of train : 32 %\n",
      "Accuracy of trout : 44 %\n",
      "Accuracy of tulip : 35 %\n",
      "Accuracy of turtle : 22 %\n",
      "Accuracy of wardrobe : 65 %\n",
      "Accuracy of whale : 37 %\n",
      "Accuracy of willow_tree : 30 %\n",
      "Accuracy of  wolf : 39 %\n",
      "Accuracy of woman : 20 %\n",
      "Accuracy of  worm : 47 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_three(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4ea02",
   "metadata": {},
   "source": [
    "## Model 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32afd244",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_four =AlexNetModified()\n",
    "model_four.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(model_four.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad4d89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.603\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.558\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.482\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.410\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.354\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.307\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.259\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.242\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.210\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.172\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.145\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.112\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.083\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.068\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.038\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.027\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 3.989\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 3.991\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 3.947\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 3.944\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 3.925\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 3.917\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 3.883\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 3.862\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 3.841\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.829\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.796\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.803\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.803\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.793\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.774\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.770\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.734\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.710\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.679\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.629\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.637\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.630\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.636\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.593\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.603\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.608\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.569\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.572\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.519\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.519\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.512\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.441\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.427\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.452\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.390\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.420\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.419\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.473\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.367\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.398\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.359\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.353\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.344\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 3.279\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 3.265\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 3.261\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 3.252\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 3.286\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 3.256\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 3.270\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 3.244\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 3.179\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 3.204\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 3.180\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 3.225\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 3.121\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 3.104\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 3.042\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 3.057\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 3.093\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 3.115\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 3.093\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 3.114\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 3.067\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 3.044\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 3.070\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 3.005\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.950\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.948\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.923\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.957\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.936\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.976\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.936\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.889\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.929\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.932\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.923\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.951\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.879\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.848\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.867\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.844\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.797\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.822\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.800\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.789\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.805\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.809\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.810\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.729\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 2.666\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 2.699\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 2.697\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 2.736\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 2.751\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 2.689\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 2.659\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 2.736\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 2.694\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 2.707\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 2.702\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 2.724\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 2.593\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 2.568\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 2.662\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 2.606\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 2.582\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 2.645\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 2.600\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 2.609\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 2.574\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 2.593\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 2.604\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 2.652\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 2.591\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 2.523\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 2.514\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 2.565\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 2.493\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 2.505\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 2.528\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 2.539\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 2.550\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 2.508\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 2.560\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 2.509\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 2.472\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 2.418\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 2.453\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 2.457\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 2.429\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 2.444\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 2.428\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 2.416\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 2.458\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 2.424\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 2.471\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 2.428\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 2.345\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 2.282\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 2.344\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 2.388\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 2.361\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 2.384\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 2.363\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 2.367\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 2.369\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 2.406\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 2.347\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 2.406\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 2.277\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 2.215\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 2.300\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 2.309\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 2.319\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 2.350\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 2.287\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 2.258\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 2.299\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 2.313\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 2.319\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 2.295\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 2.189\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 2.223\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 2.281\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 2.230\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 2.253\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 2.229\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 2.252\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 2.212\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 2.217\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 2.270\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 2.223\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 2.249\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 2.113\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 2.181\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 2.193\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 2.152\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 2.174\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 2.181\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 2.208\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 2.155\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 2.182\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 2.225\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 2.270\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 2.175\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 2.098\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 2.101\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 2.106\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 2.099\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 2.123\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 2.067\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 2.116\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 2.102\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 2.120\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 2.137\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 2.092\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 2.118\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 2.031\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 2.085\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 2.023\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 2.023\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 2.072\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 2.090\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 2.084\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 2.081\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 2.096\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 2.064\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 2.121\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 2.040\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 1.986\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 1.945\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 2.052\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 2.052\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 2.027\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 1.974\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 2.027\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 2.041\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 2.016\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 2.069\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 2.043\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 2.045\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 2.001\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 1.935\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 1.893\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 1.948\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 1.974\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 1.900\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 1.969\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 1.969\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 1.986\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 2.006\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 1.945\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 1.983\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 1.816\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 1.876\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 1.911\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 1.946\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 1.852\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 1.934\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 1.906\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 1.913\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 1.884\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 2.050\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 1.920\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 1.953\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 1.911\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 1.903\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 1.843\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 1.897\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 1.887\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 1.869\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 1.880\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 1.920\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 1.887\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 1.938\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 1.890\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 1.929\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 1.783\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 1.850\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 1.819\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 1.864\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 1.816\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 1.801\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 1.854\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 1.857\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 1.831\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 1.871\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 1.854\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 1.904\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 1.807\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 1.778\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 1.761\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 1.785\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 1.787\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 1.802\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 1.828\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 1.773\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 1.780\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 1.808\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 1.880\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 1.810\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 1.787\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 1.793\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 1.803\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 1.686\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 1.722\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 1.806\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 1.797\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 1.797\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 1.790\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 1.762\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 1.729\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 1.748\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 1.658\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 1.701\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 1.734\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 1.745\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 1.705\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 1.736\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 1.752\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 1.712\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 1.730\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 1.705\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 1.732\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 1.796\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 1.633\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 1.667\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 1.689\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 1.715\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 1.745\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 1.752\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 1.665\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 1.713\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 1.728\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 1.671\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 1.666\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 1.717\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 1.714\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 1.637\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 1.617\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 1.679\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 1.638\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 1.647\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 1.735\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 1.649\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 1.718\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 1.699\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 1.675\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 1.644\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 1.590\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 1.586\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 1.618\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 1.678\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 1.637\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 1.615\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 1.620\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 1.612\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 1.638\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 1.698\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 1.594\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 1.580\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 30      \n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = model_four(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c675a3d0",
   "metadata": {},
   "source": [
    "## Model 4 Training Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92e51ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de24e80",
   "metadata": {},
   "source": [
    "## Evaluate Model 4 on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa184947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 42 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_four(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbdb7d",
   "metadata": {},
   "source": [
    "## Evaluate Model 4 Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60e144a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 75 %\n",
      "Accuracy of aquarium_fish : 53 %\n",
      "Accuracy of  baby : 35 %\n",
      "Accuracy of  bear : 16 %\n",
      "Accuracy of beaver : 18 %\n",
      "Accuracy of   bed : 35 %\n",
      "Accuracy of   bee : 48 %\n",
      "Accuracy of beetle : 42 %\n",
      "Accuracy of bicycle : 46 %\n",
      "Accuracy of bottle : 49 %\n",
      "Accuracy of  bowl : 29 %\n",
      "Accuracy of   boy : 25 %\n",
      "Accuracy of bridge : 44 %\n",
      "Accuracy of   bus : 33 %\n",
      "Accuracy of butterfly : 38 %\n",
      "Accuracy of camel : 39 %\n",
      "Accuracy of   can : 52 %\n",
      "Accuracy of castle : 68 %\n",
      "Accuracy of caterpillar : 50 %\n",
      "Accuracy of cattle : 27 %\n",
      "Accuracy of chair : 62 %\n",
      "Accuracy of chimpanzee : 65 %\n",
      "Accuracy of clock : 37 %\n",
      "Accuracy of cloud : 63 %\n",
      "Accuracy of cockroach : 63 %\n",
      "Accuracy of couch : 14 %\n",
      "Accuracy of  crab : 26 %\n",
      "Accuracy of crocodile : 29 %\n",
      "Accuracy of   cup : 47 %\n",
      "Accuracy of dinosaur : 33 %\n",
      "Accuracy of dolphin : 32 %\n",
      "Accuracy of elephant : 36 %\n",
      "Accuracy of flatfish : 29 %\n",
      "Accuracy of forest : 38 %\n",
      "Accuracy of   fox : 43 %\n",
      "Accuracy of  girl : 22 %\n",
      "Accuracy of hamster : 51 %\n",
      "Accuracy of house : 31 %\n",
      "Accuracy of kangaroo : 23 %\n",
      "Accuracy of keyboard : 47 %\n",
      "Accuracy of  lamp : 31 %\n",
      "Accuracy of lawn_mower : 55 %\n",
      "Accuracy of leopard : 40 %\n",
      "Accuracy of  lion : 43 %\n",
      "Accuracy of lizard : 17 %\n",
      "Accuracy of lobster : 21 %\n",
      "Accuracy of   man : 22 %\n",
      "Accuracy of maple_tree : 50 %\n",
      "Accuracy of motorcycle : 68 %\n",
      "Accuracy of mountain : 38 %\n",
      "Accuracy of mouse : 24 %\n",
      "Accuracy of mushroom : 42 %\n",
      "Accuracy of oak_tree : 78 %\n",
      "Accuracy of orange : 66 %\n",
      "Accuracy of orchid : 73 %\n",
      "Accuracy of otter : 11 %\n",
      "Accuracy of palm_tree : 60 %\n",
      "Accuracy of  pear : 45 %\n",
      "Accuracy of pickup_truck : 66 %\n",
      "Accuracy of pine_tree : 34 %\n",
      "Accuracy of plain : 61 %\n",
      "Accuracy of plate : 34 %\n",
      "Accuracy of poppy : 55 %\n",
      "Accuracy of porcupine : 43 %\n",
      "Accuracy of possum : 34 %\n",
      "Accuracy of rabbit : 22 %\n",
      "Accuracy of raccoon : 49 %\n",
      "Accuracy of   ray : 39 %\n",
      "Accuracy of  road : 67 %\n",
      "Accuracy of rocket : 61 %\n",
      "Accuracy of  rose : 45 %\n",
      "Accuracy of   sea : 74 %\n",
      "Accuracy of  seal : 17 %\n",
      "Accuracy of shark : 47 %\n",
      "Accuracy of shrew : 33 %\n",
      "Accuracy of skunk : 56 %\n",
      "Accuracy of skyscraper : 62 %\n",
      "Accuracy of snail : 28 %\n",
      "Accuracy of snake : 28 %\n",
      "Accuracy of spider : 46 %\n",
      "Accuracy of squirrel : 32 %\n",
      "Accuracy of streetcar : 48 %\n",
      "Accuracy of sunflower : 80 %\n",
      "Accuracy of sweet_pepper : 47 %\n",
      "Accuracy of table : 39 %\n",
      "Accuracy of  tank : 44 %\n",
      "Accuracy of telephone : 41 %\n",
      "Accuracy of television : 61 %\n",
      "Accuracy of tiger : 43 %\n",
      "Accuracy of tractor : 48 %\n",
      "Accuracy of train : 41 %\n",
      "Accuracy of trout : 47 %\n",
      "Accuracy of tulip : 29 %\n",
      "Accuracy of turtle : 20 %\n",
      "Accuracy of wardrobe : 73 %\n",
      "Accuracy of whale : 43 %\n",
      "Accuracy of willow_tree : 41 %\n",
      "Accuracy of  wolf : 35 %\n",
      "Accuracy of woman : 27 %\n",
      "Accuracy of  worm : 63 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_four(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f6819",
   "metadata": {},
   "source": [
    "# Construct Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc0ed6",
   "metadata": {},
   "source": [
    "## Maximum Vote Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4749790f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaximumVotingConvolutionalEnsemble(\n",
       "  (model1): BaselineConvolutionModel(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model2): ConvutionModelAddedLayersBatchNorm(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_seven): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc5): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc6): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model3): ResNetMultiheadAttention(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model4): AlexNetModified(\n",
       "    (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (fc4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (fc5): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (batchNorm1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vote_convolutional_ensemble = MaximumVotingConvolutionalEnsemble(model_one,model_two,model_three, model_four)\n",
    "\n",
    "# Freeze these models\n",
    "for param in model_one.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in model_two.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "for param in model_three.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "max_vote_convolutional_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06ab13",
   "metadata": {},
   "source": [
    "##  Evaluate Maximum Voting Ensemble on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d168d4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = max_vote_convolutional_ensemble(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361094ca",
   "metadata": {},
   "source": [
    "## Evaluate Maximum Voting Ensemble Across Classes on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "155dbe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 60 %\n",
      "Accuracy of aquarium_fish : 66 %\n",
      "Accuracy of  baby : 37 %\n",
      "Accuracy of  bear : 29 %\n",
      "Accuracy of beaver : 38 %\n",
      "Accuracy of   bed : 58 %\n",
      "Accuracy of   bee : 49 %\n",
      "Accuracy of beetle : 60 %\n",
      "Accuracy of bicycle : 59 %\n",
      "Accuracy of bottle : 58 %\n",
      "Accuracy of  bowl : 25 %\n",
      "Accuracy of   boy : 36 %\n",
      "Accuracy of bridge : 55 %\n",
      "Accuracy of   bus : 35 %\n",
      "Accuracy of butterfly : 38 %\n",
      "Accuracy of camel : 37 %\n",
      "Accuracy of   can : 39 %\n",
      "Accuracy of castle : 73 %\n",
      "Accuracy of caterpillar : 43 %\n",
      "Accuracy of cattle : 41 %\n",
      "Accuracy of chair : 80 %\n",
      "Accuracy of chimpanzee : 60 %\n",
      "Accuracy of clock : 53 %\n",
      "Accuracy of cloud : 63 %\n",
      "Accuracy of cockroach : 68 %\n",
      "Accuracy of couch : 45 %\n",
      "Accuracy of  crab : 47 %\n",
      "Accuracy of crocodile : 30 %\n",
      "Accuracy of   cup : 67 %\n",
      "Accuracy of dinosaur : 53 %\n",
      "Accuracy of dolphin : 37 %\n",
      "Accuracy of elephant : 53 %\n",
      "Accuracy of flatfish : 50 %\n",
      "Accuracy of forest : 58 %\n",
      "Accuracy of   fox : 61 %\n",
      "Accuracy of  girl : 35 %\n",
      "Accuracy of hamster : 50 %\n",
      "Accuracy of house : 43 %\n",
      "Accuracy of kangaroo : 33 %\n",
      "Accuracy of keyboard : 74 %\n",
      "Accuracy of  lamp : 49 %\n",
      "Accuracy of lawn_mower : 68 %\n",
      "Accuracy of leopard : 57 %\n",
      "Accuracy of  lion : 58 %\n",
      "Accuracy of lizard : 23 %\n",
      "Accuracy of lobster : 23 %\n",
      "Accuracy of   man : 27 %\n",
      "Accuracy of maple_tree : 51 %\n",
      "Accuracy of motorcycle : 81 %\n",
      "Accuracy of mountain : 57 %\n",
      "Accuracy of mouse : 35 %\n",
      "Accuracy of mushroom : 46 %\n",
      "Accuracy of oak_tree : 63 %\n",
      "Accuracy of orange : 83 %\n",
      "Accuracy of orchid : 63 %\n",
      "Accuracy of otter : 27 %\n",
      "Accuracy of palm_tree : 75 %\n",
      "Accuracy of  pear : 59 %\n",
      "Accuracy of pickup_truck : 76 %\n",
      "Accuracy of pine_tree : 52 %\n",
      "Accuracy of plain : 88 %\n",
      "Accuracy of plate : 60 %\n",
      "Accuracy of poppy : 59 %\n",
      "Accuracy of porcupine : 41 %\n",
      "Accuracy of possum : 27 %\n",
      "Accuracy of rabbit : 27 %\n",
      "Accuracy of raccoon : 44 %\n",
      "Accuracy of   ray : 32 %\n",
      "Accuracy of  road : 78 %\n",
      "Accuracy of rocket : 68 %\n",
      "Accuracy of  rose : 46 %\n",
      "Accuracy of   sea : 63 %\n",
      "Accuracy of  seal : 19 %\n",
      "Accuracy of shark : 36 %\n",
      "Accuracy of shrew : 31 %\n",
      "Accuracy of skunk : 76 %\n",
      "Accuracy of skyscraper : 76 %\n",
      "Accuracy of snail : 32 %\n",
      "Accuracy of snake : 48 %\n",
      "Accuracy of spider : 48 %\n",
      "Accuracy of squirrel : 25 %\n",
      "Accuracy of streetcar : 57 %\n",
      "Accuracy of sunflower : 81 %\n",
      "Accuracy of sweet_pepper : 40 %\n",
      "Accuracy of table : 32 %\n",
      "Accuracy of  tank : 58 %\n",
      "Accuracy of telephone : 73 %\n",
      "Accuracy of television : 52 %\n",
      "Accuracy of tiger : 38 %\n",
      "Accuracy of tractor : 60 %\n",
      "Accuracy of train : 52 %\n",
      "Accuracy of trout : 68 %\n",
      "Accuracy of tulip : 38 %\n",
      "Accuracy of turtle : 26 %\n",
      "Accuracy of wardrobe : 83 %\n",
      "Accuracy of whale : 62 %\n",
      "Accuracy of willow_tree : 39 %\n",
      "Accuracy of  wolf : 44 %\n",
      "Accuracy of woman : 27 %\n",
      "Accuracy of  worm : 57 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = max_vote_convolutional_ensemble(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0f3fd",
   "metadata": {},
   "source": [
    "## Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3c930b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingConvolutionalEnsemble(\n",
       "  (model1): BaselineConvolutionModel(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model2): ConvutionModelAddedLayersBatchNorm(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_seven): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc5): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc6): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model3): ResNetMultiheadAttention(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model4): AlexNetModified(\n",
       "    (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (fc4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (fc5): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (batchNorm1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_vote_conv_ensemble = VotingConvolutionalEnsemble(model_one,model_two,model_three, model_four)\n",
    "\n",
    "# Freeze these models\n",
    "for param in model_one.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in model_two.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "for param in model_three.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "avg_vote_conv_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284199a",
   "metadata": {},
   "source": [
    "## Evaluate Voting Ensemble on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5aaebd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = avg_vote_conv_ensemble(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab80144",
   "metadata": {},
   "source": [
    "## Evaluate Voting Ensemble on Test Set Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e69e8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 75 %\n",
      "Accuracy of aquarium_fish : 68 %\n",
      "Accuracy of  baby : 42 %\n",
      "Accuracy of  bear : 39 %\n",
      "Accuracy of beaver : 43 %\n",
      "Accuracy of   bed : 57 %\n",
      "Accuracy of   bee : 65 %\n",
      "Accuracy of beetle : 58 %\n",
      "Accuracy of bicycle : 66 %\n",
      "Accuracy of bottle : 67 %\n",
      "Accuracy of  bowl : 46 %\n",
      "Accuracy of   boy : 38 %\n",
      "Accuracy of bridge : 55 %\n",
      "Accuracy of   bus : 58 %\n",
      "Accuracy of butterfly : 56 %\n",
      "Accuracy of camel : 46 %\n",
      "Accuracy of   can : 56 %\n",
      "Accuracy of castle : 79 %\n",
      "Accuracy of caterpillar : 55 %\n",
      "Accuracy of cattle : 48 %\n",
      "Accuracy of chair : 83 %\n",
      "Accuracy of chimpanzee : 72 %\n",
      "Accuracy of clock : 57 %\n",
      "Accuracy of cloud : 76 %\n",
      "Accuracy of cockroach : 75 %\n",
      "Accuracy of couch : 46 %\n",
      "Accuracy of  crab : 54 %\n",
      "Accuracy of crocodile : 44 %\n",
      "Accuracy of   cup : 76 %\n",
      "Accuracy of dinosaur : 57 %\n",
      "Accuracy of dolphin : 48 %\n",
      "Accuracy of elephant : 54 %\n",
      "Accuracy of flatfish : 54 %\n",
      "Accuracy of forest : 66 %\n",
      "Accuracy of   fox : 62 %\n",
      "Accuracy of  girl : 39 %\n",
      "Accuracy of hamster : 60 %\n",
      "Accuracy of house : 58 %\n",
      "Accuracy of kangaroo : 30 %\n",
      "Accuracy of keyboard : 78 %\n",
      "Accuracy of  lamp : 58 %\n",
      "Accuracy of lawn_mower : 67 %\n",
      "Accuracy of leopard : 53 %\n",
      "Accuracy of  lion : 65 %\n",
      "Accuracy of lizard : 31 %\n",
      "Accuracy of lobster : 38 %\n",
      "Accuracy of   man : 34 %\n",
      "Accuracy of maple_tree : 58 %\n",
      "Accuracy of motorcycle : 87 %\n",
      "Accuracy of mountain : 71 %\n",
      "Accuracy of mouse : 45 %\n",
      "Accuracy of mushroom : 55 %\n",
      "Accuracy of oak_tree : 67 %\n",
      "Accuracy of orange : 92 %\n",
      "Accuracy of orchid : 75 %\n",
      "Accuracy of otter : 28 %\n",
      "Accuracy of palm_tree : 78 %\n",
      "Accuracy of  pear : 62 %\n",
      "Accuracy of pickup_truck : 75 %\n",
      "Accuracy of pine_tree : 63 %\n",
      "Accuracy of plain : 86 %\n",
      "Accuracy of plate : 62 %\n",
      "Accuracy of poppy : 57 %\n",
      "Accuracy of porcupine : 52 %\n",
      "Accuracy of possum : 33 %\n",
      "Accuracy of rabbit : 42 %\n",
      "Accuracy of raccoon : 52 %\n",
      "Accuracy of   ray : 44 %\n",
      "Accuracy of  road : 77 %\n",
      "Accuracy of rocket : 73 %\n",
      "Accuracy of  rose : 61 %\n",
      "Accuracy of   sea : 70 %\n",
      "Accuracy of  seal : 25 %\n",
      "Accuracy of shark : 47 %\n",
      "Accuracy of shrew : 38 %\n",
      "Accuracy of skunk : 83 %\n",
      "Accuracy of skyscraper : 78 %\n",
      "Accuracy of snail : 46 %\n",
      "Accuracy of snake : 51 %\n",
      "Accuracy of spider : 58 %\n",
      "Accuracy of squirrel : 39 %\n",
      "Accuracy of streetcar : 57 %\n",
      "Accuracy of sunflower : 85 %\n",
      "Accuracy of sweet_pepper : 47 %\n",
      "Accuracy of table : 47 %\n",
      "Accuracy of  tank : 66 %\n",
      "Accuracy of telephone : 68 %\n",
      "Accuracy of television : 63 %\n",
      "Accuracy of tiger : 57 %\n",
      "Accuracy of tractor : 73 %\n",
      "Accuracy of train : 64 %\n",
      "Accuracy of trout : 67 %\n",
      "Accuracy of tulip : 55 %\n",
      "Accuracy of turtle : 39 %\n",
      "Accuracy of wardrobe : 88 %\n",
      "Accuracy of whale : 63 %\n",
      "Accuracy of willow_tree : 52 %\n",
      "Accuracy of  wolf : 64 %\n",
      "Accuracy of woman : 38 %\n",
      "Accuracy of  worm : 67 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = avg_vote_conv_ensemble(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acdb44",
   "metadata": {},
   "source": [
    "# Bagging Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6cbd96ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingConvolutionalEnsemble(\n",
       "  (model1): BaselineConvolutionModel(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model2): ConvutionModelAddedLayersBatchNorm(\n",
       "    (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_seven): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchNorm3): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc1): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc5): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc6): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model3): ResNetMultiheadAttention(\n",
       "    (resnet): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=512, out_features=100, bias=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): MultiHeadAttentionWrapper(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (model4): AlexNetModified(\n",
       "    (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "    (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "    (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (fc3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "    (fc4): Linear(in_features=1000, out_features=500, bias=True)\n",
       "    (fc5): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (batchNorm1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (batchNorm2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (classifier): Linear(in_features=400, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_conv_ensemble = BaggingConvolutionalEnsemble(model_one,model_two,model_three,model_four)\n",
    "\n",
    "# Freeze these models\n",
    "for param in model_one.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "for param in model_two.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "for param in model_three.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "bagging_conv_ensemble.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068a023",
   "metadata": {},
   "source": [
    "## Evaluate Bagging Ensemble on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b37d80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 1 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = bagging_conv_ensemble(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cead77b",
   "metadata": {},
   "source": [
    "## Evaluate Bagging Ensemble on Test Set Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d07e5409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple :  0 %\n",
      "Accuracy of aquarium_fish :  3 %\n",
      "Accuracy of  baby :  0 %\n",
      "Accuracy of  bear :  9 %\n",
      "Accuracy of beaver :  2 %\n",
      "Accuracy of   bed :  0 %\n",
      "Accuracy of   bee :  0 %\n",
      "Accuracy of beetle :  0 %\n",
      "Accuracy of bicycle :  0 %\n",
      "Accuracy of bottle :  0 %\n",
      "Accuracy of  bowl :  2 %\n",
      "Accuracy of   boy :  1 %\n",
      "Accuracy of bridge :  0 %\n",
      "Accuracy of   bus :  0 %\n",
      "Accuracy of butterfly :  1 %\n",
      "Accuracy of camel :  0 %\n",
      "Accuracy of   can :  0 %\n",
      "Accuracy of castle :  0 %\n",
      "Accuracy of caterpillar :  2 %\n",
      "Accuracy of cattle :  0 %\n",
      "Accuracy of chair :  0 %\n",
      "Accuracy of chimpanzee :  3 %\n",
      "Accuracy of clock :  0 %\n",
      "Accuracy of cloud :  0 %\n",
      "Accuracy of cockroach :  0 %\n",
      "Accuracy of couch :  0 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  0 %\n",
      "Accuracy of   cup :  0 %\n",
      "Accuracy of dinosaur :  0 %\n",
      "Accuracy of dolphin :  1 %\n",
      "Accuracy of elephant :  0 %\n",
      "Accuracy of flatfish :  0 %\n",
      "Accuracy of forest :  0 %\n",
      "Accuracy of   fox :  0 %\n",
      "Accuracy of  girl :  0 %\n",
      "Accuracy of hamster :  0 %\n",
      "Accuracy of house :  2 %\n",
      "Accuracy of kangaroo :  0 %\n",
      "Accuracy of keyboard :  5 %\n",
      "Accuracy of  lamp :  2 %\n",
      "Accuracy of lawn_mower :  4 %\n",
      "Accuracy of leopard :  4 %\n",
      "Accuracy of  lion :  0 %\n",
      "Accuracy of lizard :  0 %\n",
      "Accuracy of lobster :  2 %\n",
      "Accuracy of   man :  0 %\n",
      "Accuracy of maple_tree :  0 %\n",
      "Accuracy of motorcycle :  0 %\n",
      "Accuracy of mountain :  0 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  0 %\n",
      "Accuracy of orange :  0 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  1 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  4 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  0 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  5 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  4 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  1 %\n",
      "Accuracy of rocket :  0 %\n",
      "Accuracy of  rose :  0 %\n",
      "Accuracy of   sea :  1 %\n",
      "Accuracy of  seal :  3 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  4 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  1 %\n",
      "Accuracy of spider :  2 %\n",
      "Accuracy of squirrel :  1 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  2 %\n",
      "Accuracy of  tank :  2 %\n",
      "Accuracy of telephone :  2 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  0 %\n",
      "Accuracy of tractor :  0 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe : 16 %\n",
      "Accuracy of whale :  0 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = bagging_conv_ensemble(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0be26",
   "metadata": {},
   "source": [
    "## Results\n",
    "1. First ensemble gave an accuracy of 50%\n",
    "2. Second ensemble gave an accuracy of 58%\n",
    "3. Third ensemble gave an accuracy of 1%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
