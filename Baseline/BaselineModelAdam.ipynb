{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79d7a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da4c56",
   "metadata": {},
   "source": [
    "## Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6117c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b170b",
   "metadata": {},
   "source": [
    "## Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b574a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336a6dc",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc14492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 100)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move the network parameters to the specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904d075",
   "metadata": {},
   "source": [
    "## Select Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247b7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.Adam(net.parameters()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857ce00",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "211ff432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.690\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.675\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.654\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.654\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.613\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.579\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.522\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.493\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.437\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.383\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.386\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.325\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.285\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.315\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.355\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.329\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.296\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.297\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.250\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.221\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.213\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.136\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.129\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.159\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.001\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 3.946\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.951\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.915\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.903\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.881\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.865\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.947\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.860\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.814\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 4.003\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.819\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.697\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.840\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.684\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.904\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.676\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 4.425\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.853\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.682\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 4.036\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.770\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 4.683\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 4.510\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.957\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.782\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.697\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.850\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.711\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.626\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.636\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.678\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.683\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.649\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.658\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.519\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 3.460\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 3.475\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 3.402\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 3.616\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 3.464\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 3.402\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 4.040\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 3.756\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 3.668\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 3.531\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 3.473\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 3.481\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 3.306\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 3.261\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 3.413\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 3.430\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 3.341\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 3.351\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 3.521\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 3.437\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 3.480\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 3.337\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 3.354\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 4.046\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 3.548\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 3.303\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 3.996\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 3.433\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 3.298\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 3.191\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 3.851\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 3.514\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 3.299\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 20.541\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 4.002\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 3.834\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 3.596\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 3.386\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 3.364\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 3.299\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 3.484\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 6.801\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 4.518\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 4.242\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 4.091\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 4.018\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 3.868\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 3.679\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 3.702\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 3.497\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 3.411\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 3.409\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 875.543\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 4.240\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 4.126\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 4.044\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 3.935\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 3.950\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 3.905\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 3.899\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 3.826\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 3.794\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 3.789\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 3.725\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 3.665\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 3.579\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 3.800\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 4.271\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 4.072\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 4.025\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 3.650\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 3.809\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 3.576\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 3.462\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 3.404\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 3.592\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 3.439\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 3.291\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 3.349\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 3.414\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 5.182\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 3.545\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 3.373\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 3.361\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 3.101\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 3.518\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 4.059\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 3.541\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 3.328\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 3.293\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 3.715\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 3.304\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 3.366\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 3.934\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 3.684\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 3.351\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 3.991\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 3.521\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 3.287\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 3.223\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 3.151\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 3.195\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 3.230\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 3.204\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 3.844\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 3.437\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 3.241\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 3.277\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 3.056\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 3.896\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 4.072\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 3.659\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 3.784\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 3.429\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 3.220\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 3.272\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 3.247\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 3.523\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 3.241\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 3.876\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 3.160\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 3.428\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 3.082\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 3.102\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 3.164\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 4.241\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 4.339\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 3.573\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 3.476\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 3.292\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 3.428\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 3.145\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 4.216\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 3.517\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 3.194\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 3.279\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 3.280\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 3.043\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 3.466\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 3.218\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 3.079\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 3.155\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 3.160\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 3.117\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 4.936\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 3.407\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 3.207\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 3.087\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 3.968\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 4.044\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 3.565\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 3.317\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 3.218\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 3.770\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 4.120\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 3.665\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 3.266\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 3.132\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 3.052\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 3.978\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 3.851\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 3.517\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 3.312\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 3.240\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 3.147\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 3.231\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 3.048\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 3.711\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 3.023\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 3.486\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 3.714\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 3.240\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 3.132\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 5.732\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 4.915\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 7.014\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 4.975\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 4.933\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 4.902\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 4.912\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 4.923\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 4.918\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 4.918\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 4.914\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 4.916\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 4.933\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 4.915\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 4.916\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 4.908\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 4.908\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 4.939\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 4.927\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 4.928\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 4.920\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 4.930\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 4.922\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 4.923\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 4.926\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 4.920\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 4.920\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 4.922\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 4.921\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 4.920\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 4.914\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 4.912\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 4.911\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 4.911\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 4.922\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 4.919\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 4.916\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 4.926\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 4.932\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 4.928\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 4.916\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 4.925\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 4.912\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 4.924\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 4.934\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 4.916\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 4.932\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 4.905\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 4.928\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 4.935\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 4.929\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 4.909\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 4.922\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 4.902\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 4.926\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 4.927\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 4.932\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 4.922\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 4.912\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 4.900\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 4.922\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 4.924\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 4.916\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 4.920\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 4.899\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 4.931\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 4.915\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 4.911\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 4.926\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 4.926\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 4.913\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 4.917\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 4.927\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 4.916\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 4.934\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 4.912\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 4.928\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 4.919\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 4.918\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 4.915\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 4.923\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 4.918\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 4.923\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 4.923\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 4.913\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 4.924\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 4.920\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 4.912\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 4.911\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 4.928\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 4.917\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 4.923\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 4.919\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 4.913\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 4.913\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 4.919\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 4.919\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 4.925\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 4.927\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 4.930\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 4.919\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 4.910\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 4.915\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 4.937\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 4.924\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 4.935\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 4.914\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 4.925\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 4.918\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 4.920\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 4.934\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 4.918\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 4.913\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 4.922\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 4.930\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 4.915\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 4.921\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 4.913\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 4.929\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 4.910\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 4.918\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 4.926\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 4.912\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 4.933\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 4.917\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 4.908\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 4.907\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 4.917\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 4.928\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 4.929\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 4.916\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 4.920\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 4.915\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 4.921\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 4.932\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 4.928\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 4.937\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 4.916\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 4.921\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 4.912\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 4.926\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 4.915\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 4.925\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 4.922\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 4.919\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 4.921\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 4.912\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 4.912\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 4.919\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 4.913\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 4.911\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 4.928\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 4.913\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 4.922\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 4.923\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 4.919\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 4.919\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 4.914\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 4.914\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 4.926\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 4.920\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 4.912\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 4.912\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 4.924\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 4.924\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 4.916\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 4.917\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 4.922\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 4.909\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 4.923\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 4.907\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 4.924\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 4.923\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 4.927\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 4.926\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 4.918\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 4.907\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 4.915\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 4.930\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 4.915\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 4.912\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 4.920\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 4.919\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 4.920\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 4.911\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 4.917\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 4.921\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 35       # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf477eed",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce96227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a2bb",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad224c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 1 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ce54b",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e14f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple :  0 %\n",
      "Accuracy of aquarium_fish :  0 %\n",
      "Accuracy of  baby :  0 %\n",
      "Accuracy of  bear :  0 %\n",
      "Accuracy of beaver :  0 %\n",
      "Accuracy of   bed :  0 %\n",
      "Accuracy of   bee :  0 %\n",
      "Accuracy of beetle :  0 %\n",
      "Accuracy of bicycle :  0 %\n",
      "Accuracy of bottle :  0 %\n",
      "Accuracy of  bowl :  0 %\n",
      "Accuracy of   boy :  0 %\n",
      "Accuracy of bridge :  0 %\n",
      "Accuracy of   bus :  0 %\n",
      "Accuracy of butterfly :  0 %\n",
      "Accuracy of camel :  0 %\n",
      "Accuracy of   can :  0 %\n",
      "Accuracy of castle :  0 %\n",
      "Accuracy of caterpillar :  0 %\n",
      "Accuracy of cattle :  0 %\n",
      "Accuracy of chair :  0 %\n",
      "Accuracy of chimpanzee :  0 %\n",
      "Accuracy of clock :  0 %\n",
      "Accuracy of cloud :  0 %\n",
      "Accuracy of cockroach :  0 %\n",
      "Accuracy of couch :  0 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  0 %\n",
      "Accuracy of   cup :  0 %\n",
      "Accuracy of dinosaur :  0 %\n",
      "Accuracy of dolphin :  0 %\n",
      "Accuracy of elephant :  0 %\n",
      "Accuracy of flatfish :  0 %\n",
      "Accuracy of forest :  0 %\n",
      "Accuracy of   fox :  0 %\n",
      "Accuracy of  girl :  0 %\n",
      "Accuracy of hamster :  0 %\n",
      "Accuracy of house :  0 %\n",
      "Accuracy of kangaroo : 100 %\n",
      "Accuracy of keyboard :  0 %\n",
      "Accuracy of  lamp :  0 %\n",
      "Accuracy of lawn_mower :  0 %\n",
      "Accuracy of leopard :  0 %\n",
      "Accuracy of  lion :  0 %\n",
      "Accuracy of lizard :  0 %\n",
      "Accuracy of lobster :  0 %\n",
      "Accuracy of   man :  0 %\n",
      "Accuracy of maple_tree :  0 %\n",
      "Accuracy of motorcycle :  0 %\n",
      "Accuracy of mountain :  0 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  0 %\n",
      "Accuracy of orange :  0 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  0 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  0 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  0 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  0 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  0 %\n",
      "Accuracy of rocket :  0 %\n",
      "Accuracy of  rose :  0 %\n",
      "Accuracy of   sea :  0 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  0 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  0 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  0 %\n",
      "Accuracy of tractor :  0 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe :  0 %\n",
      "Accuracy of whale :  0 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
