{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8043b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01936673",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a702576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.RandomResizedCrop(224,antialias=True),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8548d7e4",
   "metadata": {},
   "source": [
    "## Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6776c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a828ca0",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "930845fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (fc3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (batchNorm1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchNorm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchNorm3): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchNorm4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11 , stride=4, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5, stride=1, padding=2)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1000)\n",
    "        \n",
    "        self.batchNorm1 = nn.BatchNorm2d(3)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(96)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(384)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(256)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x= self.batchNorm2(x)\n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.batchNorm4(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)    \n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d17361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.989\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.442\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.322\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.245\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.189\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.116\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.095\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.047\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.005\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 3.938\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 3.896\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 3.855\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 3.801\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 3.789\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 3.746\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 3.748\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 3.744\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 3.676\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 3.660\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 3.658\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 3.633\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 3.594\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 3.544\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 3.576\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 3.486\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 3.496\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.450\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.388\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.428\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.410\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.366\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.373\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.389\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.352\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.346\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.335\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.223\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.241\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.232\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.212\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.158\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.191\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.150\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.137\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.154\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.115\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.097\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.110\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 2.976\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.037\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.037\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.053\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 2.983\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 2.975\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.002\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 2.900\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.007\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.000\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 2.959\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 2.950\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.908\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.846\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 2.802\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 2.836\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.784\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 2.870\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 2.789\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.824\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.837\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.801\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.815\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.807\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.699\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.740\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.725\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.642\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.689\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.677\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.698\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.681\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.744\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.723\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.714\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.680\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.563\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.586\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.562\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.560\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.624\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.637\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.610\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.567\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.585\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.603\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.574\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.560\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.503\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.498\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.451\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.560\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.475\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.481\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.466\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.545\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.481\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.457\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.480\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.495\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 2.387\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 2.373\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 2.409\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 2.383\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 2.378\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 2.386\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 2.406\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 2.337\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 2.395\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 2.415\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 2.350\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 2.389\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 2.315\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 2.291\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 2.278\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 2.367\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 2.319\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 2.306\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 2.296\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 2.311\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 2.335\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 2.329\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 2.297\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 2.386\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 2.210\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 2.239\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 2.254\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 2.248\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 2.242\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 2.212\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 2.201\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 2.254\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 2.278\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 2.248\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 2.269\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 2.234\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 2.159\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 2.110\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 2.145\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 2.141\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 2.144\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 2.175\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 2.194\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 2.179\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 2.195\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 2.190\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 2.208\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 2.211\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 2.037\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 2.054\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 2.112\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 2.121\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 2.108\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 2.074\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 2.115\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 2.180\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 2.116\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 2.117\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 2.103\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 2.116\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 1.971\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 2.035\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 2.060\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 1.998\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 2.064\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 2.072\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 2.073\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 2.050\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 2.082\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 2.079\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 2.077\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 2.057\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 1.964\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 2.021\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 1.978\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 1.954\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 1.994\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 2.007\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 2.015\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 2.026\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 2.005\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 2.006\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 2.049\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 1.960\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 1.889\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 1.870\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 1.898\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 1.906\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 1.954\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 1.970\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 1.924\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 1.933\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 1.973\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 1.970\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 1.945\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 1.950\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 1.869\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 1.843\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 1.870\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 1.859\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 1.926\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 1.898\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 1.882\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 1.868\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 1.907\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 1.917\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 1.933\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 1.942\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 1.792\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 1.853\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 1.869\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 1.819\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 1.851\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 1.888\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 1.861\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 1.861\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 1.890\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 1.875\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 1.889\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 1.901\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 1.728\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 1.791\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 1.725\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 1.754\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 1.802\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 1.816\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 1.821\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 1.801\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 1.789\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 1.875\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 1.888\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 1.801\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 1.702\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 1.724\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 1.722\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 1.766\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 1.742\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 1.755\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 1.784\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 1.746\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 1.757\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 1.758\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 1.740\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 1.785\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 1.672\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 1.714\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 1.728\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 1.709\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 1.714\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 1.735\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 1.734\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 1.803\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 1.745\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 1.720\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 1.711\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 1.729\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 1.675\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 1.665\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 1.677\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 1.660\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 1.652\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 1.708\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 1.663\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 1.677\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 1.654\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 1.730\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 1.739\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 1.756\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 1.600\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 1.611\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 1.674\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 1.665\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 1.635\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 1.682\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 1.652\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 1.659\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 1.680\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 1.693\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 1.677\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 1.632\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 1.596\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 1.570\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 1.536\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 1.611\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 1.580\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 1.585\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 1.685\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 1.638\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 1.607\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 1.620\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 1.616\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 1.647\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 1.529\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 1.548\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 1.565\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 1.557\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 1.547\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 1.578\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 1.606\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 1.605\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 1.607\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 1.603\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 1.589\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 1.659\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 1.519\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 1.541\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 1.505\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 1.553\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 1.558\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 1.544\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 1.548\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 1.567\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 1.571\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 1.554\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 1.579\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 1.560\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 1.543\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 1.476\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 1.554\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 1.507\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 1.523\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 1.520\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 1.462\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 1.548\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 1.520\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 1.634\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 1.553\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 1.585\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 1.504\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 1.522\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 1.506\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 1.489\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 1.510\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 1.520\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 1.512\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 1.530\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 1.533\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 1.566\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 1.505\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 1.543\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 1.393\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 1.399\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 1.462\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 1.483\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 1.438\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 1.468\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 1.466\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 1.435\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 1.505\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 1.533\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 1.583\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 1.488\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 1.423\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 1.395\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 1.416\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 1.454\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 1.454\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 1.452\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 1.483\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 1.492\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 1.487\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 1.449\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 1.438\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 1.464\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 1.388\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 1.478\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 1.438\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 1.457\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 1.491\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 1.453\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 1.463\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 1.454\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 1.401\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 1.430\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 1.505\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 1.457\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 1.363\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 1.382\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 1.408\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 1.379\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 1.358\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 1.384\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 1.442\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 1.403\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 1.427\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 1.394\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 1.453\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 1.442\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 1.343\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 1.408\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 1.359\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 1.420\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 1.397\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 1.420\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 1.387\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 1.375\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 1.454\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 1.386\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 1.377\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 1.424\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 1.369\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 1.309\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 1.327\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 1.337\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 1.370\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 1.385\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 1.338\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 1.336\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 1.403\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 1.415\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 1.358\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 1.433\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 1.299\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 1.335\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 1.294\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 1.302\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 1.372\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 1.343\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 1.342\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 1.383\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 1.360\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 1.382\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 1.405\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 1.390\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 1.260\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 1.285\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 1.347\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 1.292\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 1.332\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 1.278\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 1.319\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 1.359\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 1.351\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 1.353\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 1.309\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 1.360\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 1.279\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 1.247\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 1.298\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 1.303\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 1.270\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 1.315\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 1.336\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 1.292\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 1.373\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 1.354\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 1.356\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 1.350\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 1.292\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 1.302\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 1.280\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 1.322\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 1.329\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 1.283\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 1.276\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 1.280\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 1.326\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 1.290\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 1.333\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 1.319\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 1.298\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 1.314\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 1.265\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 1.248\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 1.314\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 1.318\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 1.284\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 1.324\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 1.300\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 1.270\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 1.306\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 1.347\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 1.255\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 1.231\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 1.299\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 1.247\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 1.298\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 1.239\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 1.293\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 1.270\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 1.288\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 1.264\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 1.282\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 1.258\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 1.228\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 1.221\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 1.218\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 1.233\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 1.245\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 1.216\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 1.276\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 1.289\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 1.257\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 1.232\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 1.329\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 1.243\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 1.239\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 1.247\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 1.201\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 1.264\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 1.232\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 1.259\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 1.339\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 1.198\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 1.244\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 1.248\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 1.228\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 1.251\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 1.269\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 1.222\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 1.203\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 1.186\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 1.218\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 1.280\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 1.223\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 1.269\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 1.218\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 1.257\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 1.230\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 1.230\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 1.186\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 1.220\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 1.239\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 1.267\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 1.222\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 1.266\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 1.204\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 1.245\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 1.299\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 1.222\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 1.242\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 1.192\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 1.168\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 1.121\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 1.213\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 1.188\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 1.204\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 1.216\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 1.208\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 1.218\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 1.213\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 1.233\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 1.190\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 1.220\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 1.217\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 1.199\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 1.147\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 1.181\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 1.211\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 1.180\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 1.228\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 1.223\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 1.161\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 1.175\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 1.186\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 1.203\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 1.108\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 1.178\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 1.172\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 1.198\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 1.188\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 1.120\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 1.197\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 1.187\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 1.196\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 1.144\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 1.193\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 1.231\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 1.122\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 1.121\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 1.166\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 1.166\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 1.183\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 1.158\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 1.140\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 1.165\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 1.206\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 1.210\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 1.140\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 1.152\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 1.121\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 1.118\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 1.116\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 1.139\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 1.171\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 1.183\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 1.149\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 1.156\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 1.197\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 1.136\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 1.178\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 1.168\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 1.150\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 1.049\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 1.148\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 1.133\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 1.164\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 1.134\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 1.153\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 1.173\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 1.145\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 1.182\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 1.181\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 1.222\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 1.129\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 1.143\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 1.146\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 1.127\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 1.117\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 1.109\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 1.122\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 1.148\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 1.180\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 1.117\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 1.189\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 1.110\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 1.081\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 1.163\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 1.099\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 1.129\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 1.151\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 1.099\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 1.166\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 1.077\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 1.138\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 1.125\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 1.162\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 1.152\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 1.108\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 1.091\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 1.114\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 1.146\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 1.118\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 1.089\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 1.137\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 1.145\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 1.094\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 1.135\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 1.140\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 1.127\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 1.037\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 1.088\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 1.109\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 1.073\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 1.116\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 1.071\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 1.130\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 1.104\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 1.101\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 1.142\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 1.137\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 1.110\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 1.078\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 1.139\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 1.105\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 1.010\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 1.073\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 1.138\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 1.120\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 1.125\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 1.117\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 1.125\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 1.122\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 1.096\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 1.071\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 1.107\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 1.063\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 1.088\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 1.084\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 1.069\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 1.126\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 1.118\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 1.055\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 1.112\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 1.142\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 1.121\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 1.037\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 1.068\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 1.109\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 1.142\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 1.088\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 1.087\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 1.063\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 1.131\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 1.102\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 1.087\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 1.088\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 1.104\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 1.048\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 1.081\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 1.055\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 1.099\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 1.099\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 1.045\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 1.075\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 1.077\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 1.093\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 1.087\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 1.120\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 1.029\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 1.102\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 1.072\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 1.054\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 1.079\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 1.052\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 1.083\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 1.089\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 1.028\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 1.082\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 1.075\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 1.068\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 1.135\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 1.049\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 1.026\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 1.086\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 1.079\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 1.089\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 1.073\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 1.096\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 1.045\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 1.054\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 1.057\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 1.063\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 1.049\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 1.030\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 1.041\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 1.010\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 1.071\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 1.082\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 1.066\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 1.061\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 1.042\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 1.053\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 1.045\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 1.029\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 1.069\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 1.047\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 0.985\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 1.019\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 1.035\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 1.078\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 1.071\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 1.045\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 1.045\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 1.039\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 1.017\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 1.063\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 1.063\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 1.060\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 1.072\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 1.005\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 1.044\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 1.030\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 0.964\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 1.050\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 1.076\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 1.049\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 1.037\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 1.066\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 1.034\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 1.048\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 1.032\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 1.003\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 0.994\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 1.022\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 1.106\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 1.040\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 1.060\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 1.049\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 1.039\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 1.067\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 1.048\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 1.071\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 1.057\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 0.991\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 1.033\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 0.992\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 1.085\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 1.028\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 1.026\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 1.029\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 1.082\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 0.996\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 1.064\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 1.031\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 1.040\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 1.009\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 1.000\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 1.001\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 1.057\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 1.016\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 1.030\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 1.027\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 1.069\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 1.093\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 1.044\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 1.054\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 1.057\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 1.021\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 1.014\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 0.958\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 0.997\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 0.959\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 0.964\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 1.040\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 1.047\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 1.021\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 0.995\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 0.986\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 1.024\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 1.032\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 1.017\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 0.945\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 1.034\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 0.974\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 1.022\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 1.037\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 0.996\n",
      "[epoch: 68, i: 10999] avg mini-batch loss: 1.025\n",
      "[epoch: 68, i: 11999] avg mini-batch loss: 1.027\n",
      "[epoch: 69, i:   999] avg mini-batch loss: 1.014\n",
      "[epoch: 69, i:  1999] avg mini-batch loss: 1.002\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 0.954\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 1.018\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 1.008\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 0.984\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 0.978\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 0.977\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 0.973\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 0.996\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 1.049\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 0.962\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 0.954\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 0.999\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 1.026\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 0.957\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 0.988\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 1.034\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 0.969\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 0.997\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 1.010\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 1.019\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 1.023\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 0.966\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 0.973\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 0.990\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 0.993\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 0.972\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 1.024\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 0.966\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 1.012\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 0.954\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 0.998\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 0.996\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 1.047\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 0.965\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 1.031\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 0.953\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 0.999\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 0.972\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 0.936\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 1.007\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 0.973\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 1.008\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 1.021\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 1.017\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 1.020\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 1.012\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 0.975\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 0.973\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 1.005\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 0.958\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 0.983\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 0.982\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 1.022\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 0.975\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 0.983\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 0.959\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 0.917\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 1.037\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 0.972\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 0.954\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 0.961\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 0.989\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 0.970\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 0.932\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 0.933\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 0.998\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 0.958\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 0.963\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 0.991\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 1.033\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 75      # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e936ee",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef7844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09a649",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f3a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 46 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792193c",
   "metadata": {},
   "source": [
    "## Evaluate Test Set Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e05eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of {i} : 80 %\n",
      "Accuracy of {i} : 60 %\n",
      "Accuracy of {i} : 37 %\n",
      "Accuracy of {i} : 25 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 35 %\n",
      "Accuracy of {i} : 51 %\n",
      "Accuracy of {i} : 44 %\n",
      "Accuracy of {i} : 57 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 22 %\n",
      "Accuracy of {i} : 33 %\n",
      "Accuracy of {i} : 43 %\n",
      "Accuracy of {i} : 39 %\n",
      "Accuracy of {i} : 40 %\n",
      "Accuracy of {i} : 39 %\n",
      "Accuracy of {i} : 52 %\n",
      "Accuracy of {i} : 42 %\n",
      "Accuracy of {i} : 32 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 58 %\n",
      "Accuracy of {i} : 77 %\n",
      "Accuracy of {i} : 54 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 74 %\n",
      "Accuracy of {i} : 36 %\n",
      "Accuracy of {i} : 36 %\n",
      "Accuracy of {i} : 34 %\n",
      "Accuracy of {i} : 54 %\n",
      "Accuracy of {i} : 41 %\n",
      "Accuracy of {i} : 45 %\n",
      "Accuracy of {i} : 43 %\n",
      "Accuracy of {i} : 33 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 46 %\n",
      "Accuracy of {i} : 18 %\n",
      "Accuracy of {i} : 52 %\n",
      "Accuracy of {i} : 42 %\n",
      "Accuracy of {i} : 28 %\n",
      "Accuracy of {i} : 62 %\n",
      "Accuracy of {i} : 46 %\n",
      "Accuracy of {i} : 63 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 49 %\n",
      "Accuracy of {i} : 15 %\n",
      "Accuracy of {i} : 25 %\n",
      "Accuracy of {i} : 27 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 71 %\n",
      "Accuracy of {i} : 56 %\n",
      "Accuracy of {i} : 25 %\n",
      "Accuracy of {i} : 46 %\n",
      "Accuracy of {i} : 60 %\n",
      "Accuracy of {i} : 74 %\n",
      "Accuracy of {i} : 55 %\n",
      "Accuracy of {i} : 26 %\n",
      "Accuracy of {i} : 66 %\n",
      "Accuracy of {i} : 40 %\n",
      "Accuracy of {i} : 59 %\n",
      "Accuracy of {i} : 29 %\n",
      "Accuracy of {i} : 59 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 36 %\n",
      "Accuracy of {i} : 28 %\n",
      "Accuracy of {i} : 42 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 74 %\n",
      "Accuracy of {i} : 63 %\n",
      "Accuracy of {i} : 57 %\n",
      "Accuracy of {i} : 71 %\n",
      "Accuracy of {i} : 21 %\n",
      "Accuracy of {i} : 30 %\n",
      "Accuracy of {i} : 37 %\n",
      "Accuracy of {i} : 66 %\n",
      "Accuracy of {i} : 65 %\n",
      "Accuracy of {i} : 38 %\n",
      "Accuracy of {i} : 26 %\n",
      "Accuracy of {i} : 51 %\n",
      "Accuracy of {i} : 25 %\n",
      "Accuracy of {i} : 44 %\n",
      "Accuracy of {i} : 79 %\n",
      "Accuracy of {i} : 36 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 42 %\n",
      "Accuracy of {i} : 54 %\n",
      "Accuracy of {i} : 48 %\n",
      "Accuracy of {i} : 41 %\n",
      "Accuracy of {i} : 70 %\n",
      "Accuracy of {i} : 50 %\n",
      "Accuracy of {i} : 30 %\n",
      "Accuracy of {i} : 80 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 25 %\n",
      "Accuracy of {i} : 53 %\n",
      "Accuracy of {i} : 33 %\n",
      "Accuracy of {i} : 55 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of {i} : %2d %%' % (100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
