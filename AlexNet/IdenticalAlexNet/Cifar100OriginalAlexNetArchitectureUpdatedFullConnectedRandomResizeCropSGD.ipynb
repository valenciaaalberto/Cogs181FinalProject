{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee26971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb3fc",
   "metadata": {},
   "source": [
    "## Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a702576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.RandomResizedCrop(224,antialias=True),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a07645",
   "metadata": {},
   "source": [
    "## Selected Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6776c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5759ea",
   "metadata": {},
   "source": [
    "## Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c4c8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  (fc4): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images were resized from (32,32) -> (224,224) to implement original AlexNet model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11 , stride=4, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5, stride=1, padding=2)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 100)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = AlexNet()     # Create the network instance.\n",
    "net.to(device)  # Move t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c2ad",
   "metadata": {},
   "source": [
    "## Select Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d17361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.606\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.602\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.594\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.560\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.520\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.484\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.439\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.423\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.374\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.381\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.376\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.380\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.360\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.337\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 4.338\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 4.333\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 4.292\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 4.304\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 4.279\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 4.262\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 4.262\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 4.239\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 4.245\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 4.217\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 4.191\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 4.141\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 4.162\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 4.143\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 4.131\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 4.105\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 4.117\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 4.101\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 4.069\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 4.038\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 4.049\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 4.040\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.994\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 4.004\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.977\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.943\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.980\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.965\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.952\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.919\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.911\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.900\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.892\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.870\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.844\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 3.857\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 3.826\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 3.816\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 3.786\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 3.790\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 3.785\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 3.817\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 3.778\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 3.767\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 3.761\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 3.717\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 3.768\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 3.719\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 3.739\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 3.678\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 3.649\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 3.678\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 3.697\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 3.690\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 3.652\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 3.610\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 3.665\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 3.599\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 3.639\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 3.577\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 3.598\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 3.558\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 3.554\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 3.598\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 3.559\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 3.548\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 3.563\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 3.516\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 3.522\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 3.531\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 3.516\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 3.447\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 3.475\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 3.428\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 3.474\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 3.458\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 3.453\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 3.424\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 3.422\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 3.420\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 3.386\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 3.416\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 3.392\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 3.345\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 3.341\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 3.316\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 3.297\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 3.325\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 3.280\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 3.290\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 3.296\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 3.316\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 3.251\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 3.313\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 3.279\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 3.221\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 3.176\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 3.244\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 3.182\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 3.219\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 3.203\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 3.162\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 3.237\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 3.218\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 3.188\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 3.138\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 3.149\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 3.094\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 3.143\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 3.073\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 3.069\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 3.090\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 3.108\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 3.095\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 3.108\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 3.113\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 3.056\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 3.076\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 3.118\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 3.031\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 3.038\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 3.025\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 2.993\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 3.043\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 3.042\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 3.073\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 3.029\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 3.046\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 2.986\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 3.032\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 3.061\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 2.991\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 2.972\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 2.937\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 2.981\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 2.952\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 2.925\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 2.985\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 2.890\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 2.977\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 2.935\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 2.948\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 2.962\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 2.902\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 2.954\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 2.920\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 2.873\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 2.869\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 2.951\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 2.897\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 2.895\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 2.888\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 2.944\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 2.872\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 2.917\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 2.852\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 2.869\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 2.862\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 2.886\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 2.825\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 2.859\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 2.868\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 2.829\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 2.849\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 2.850\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 2.792\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 2.838\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 2.757\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 2.813\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 2.783\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 2.834\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 2.856\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 2.810\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 2.810\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 2.821\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 2.823\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 2.844\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 2.824\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 2.807\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 2.745\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 2.829\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 2.765\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 2.771\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 2.765\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 2.815\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 2.797\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 2.770\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 2.751\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 2.790\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 2.805\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 2.813\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 2.695\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 2.770\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 2.756\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 2.744\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 2.700\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 2.760\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 2.800\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 2.756\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 2.741\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 2.764\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 2.762\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 2.770\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 2.675\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 2.706\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 2.677\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 2.730\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 2.758\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 2.707\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 2.756\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 2.735\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 2.717\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 2.808\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 2.700\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 2.722\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 2.662\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 2.639\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 2.723\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 2.701\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 2.661\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 2.728\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 2.749\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 2.678\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 2.663\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 2.734\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 2.713\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 2.718\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 2.575\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 2.676\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 2.671\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 2.673\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 2.658\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 2.673\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 2.695\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 2.717\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 2.718\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 2.686\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 2.696\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 2.662\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 2.644\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 2.653\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 2.641\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 2.674\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 2.658\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 2.717\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 2.688\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 2.693\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 2.689\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 2.692\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 2.666\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 2.656\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 2.662\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 2.626\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 2.631\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 2.665\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 2.633\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 2.649\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 2.657\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 2.659\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 2.681\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 2.695\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 2.686\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 2.637\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 2.605\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 2.650\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 2.610\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 2.604\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 2.611\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 2.658\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 2.624\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 2.607\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 2.649\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 2.660\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 2.679\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 2.655\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 2.571\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 2.660\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 2.616\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 2.607\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 2.648\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 2.677\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 2.655\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 2.654\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 2.629\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 2.663\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 2.632\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 2.623\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 2.570\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 2.618\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 2.609\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 2.603\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 2.642\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 2.657\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 2.662\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 2.615\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 2.705\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 2.598\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 2.668\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 2.666\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 2.544\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 2.609\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 2.648\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 2.652\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 2.678\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 2.628\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 2.664\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 2.691\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 2.691\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 2.641\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 2.659\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 2.631\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 2.553\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 2.616\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 2.646\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 2.693\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 2.562\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 2.626\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 2.674\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 2.672\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 2.683\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 2.658\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 2.643\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 2.693\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 2.634\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 2.580\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 2.627\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 2.589\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 2.600\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 2.646\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 2.651\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 2.671\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 2.648\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 2.649\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 2.734\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 2.647\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 2.576\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 2.632\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 2.641\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 2.582\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 2.614\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 2.669\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 2.617\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 2.663\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 2.636\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 2.696\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 2.639\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 2.621\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 2.633\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 2.593\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 2.632\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 2.622\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 2.645\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 2.651\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 2.731\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 2.675\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 2.699\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 2.652\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 2.644\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 2.767\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 2.613\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 2.646\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 2.683\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 2.669\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 2.642\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 2.633\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 2.702\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 2.714\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 2.654\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 2.693\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 2.760\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 2.754\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 2.660\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 2.657\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 2.664\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 2.618\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 2.700\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 2.696\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 2.640\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 2.668\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 2.713\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 2.714\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 2.720\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 2.669\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 2.674\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 2.614\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 2.673\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 2.734\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 2.692\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 2.688\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 2.717\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 2.754\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 2.724\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 2.704\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 2.727\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 2.773\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 2.644\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 2.678\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 2.713\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 2.671\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 2.736\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 2.720\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 2.717\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 2.729\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 2.713\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 2.663\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 2.756\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 2.788\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 2.658\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 2.632\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 2.733\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 2.716\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 2.656\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 2.734\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 2.774\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 2.731\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 2.691\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 2.787\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 2.760\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 2.760\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 2.667\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 2.669\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 2.708\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 2.776\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 2.743\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 2.715\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 2.767\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 2.800\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 2.702\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 2.814\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 2.772\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 2.824\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 2.688\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 2.692\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 2.689\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 2.758\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 2.720\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 2.694\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 2.758\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 2.754\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 2.743\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 2.789\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 2.846\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 2.801\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 2.728\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 2.731\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 2.765\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 2.797\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 2.797\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 2.755\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 2.747\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 2.805\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 2.785\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 2.782\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 2.774\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 2.756\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 2.772\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 2.699\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 2.738\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 2.827\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 2.784\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 2.855\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 2.825\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 2.883\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 2.882\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 2.828\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 2.840\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 2.838\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 2.757\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 2.764\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 2.858\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 2.780\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 2.825\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 2.814\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 2.873\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 2.857\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 2.855\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 2.860\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 2.791\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 2.917\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 2.784\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 2.854\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 2.817\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 2.816\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 2.811\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 2.858\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 2.834\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 2.823\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 2.835\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 2.858\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 2.839\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 2.864\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 2.874\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 2.903\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 2.833\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 2.871\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 2.830\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 2.875\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 2.927\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 2.835\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 2.938\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 2.883\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 2.906\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 2.918\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 2.873\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 2.850\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 2.874\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 2.873\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 2.887\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 2.894\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 2.950\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 2.904\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 2.887\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 2.992\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 2.945\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 2.927\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 2.864\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 2.902\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 2.872\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 2.895\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 2.915\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 2.882\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 2.866\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 2.963\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 2.944\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 2.861\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 2.933\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 2.891\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 2.881\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 2.929\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 2.881\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 2.989\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 2.860\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 2.932\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 2.964\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 2.985\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 2.988\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 2.979\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 2.949\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 3.038\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 3.035\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 2.987\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 2.950\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 2.980\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 3.031\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 3.042\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 3.013\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 3.026\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 3.035\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 3.058\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 3.013\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 3.026\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 2.988\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 3.005\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 3.055\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 3.036\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 3.045\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 3.040\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 3.087\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 3.051\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 3.000\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 3.085\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 3.034\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 3.064\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 3.031\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 3.076\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 3.046\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 3.128\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 3.014\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 3.035\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 3.163\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 3.145\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 3.061\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 3.141\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 3.134\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 3.072\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 3.093\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 3.061\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 3.122\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 3.073\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 3.121\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 3.114\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 3.165\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 3.114\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 3.134\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 3.083\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 3.086\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 3.130\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 3.073\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 3.150\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 3.060\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 3.109\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 3.186\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 3.160\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 3.081\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 3.219\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 3.220\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 3.166\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 3.161\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 3.235\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 3.209\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 3.182\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 3.183\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 3.232\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 3.169\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 3.160\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 3.201\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 3.143\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 3.189\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 3.233\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 3.199\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 3.185\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 3.244\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 3.098\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 3.186\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 3.276\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 3.263\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 3.270\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 3.230\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 3.257\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 3.224\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 3.265\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 3.243\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 3.252\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 3.271\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 3.293\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 3.210\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 3.289\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 3.279\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 3.258\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 3.294\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 3.282\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 3.273\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 3.311\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 3.310\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 3.277\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 3.305\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 3.306\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 3.300\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 3.276\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 3.283\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 3.339\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 3.388\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 3.327\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 3.320\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 3.297\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 3.336\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 3.388\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 3.361\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 3.274\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 3.342\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 3.439\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 3.345\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 3.361\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 3.397\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 3.422\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 3.409\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 3.374\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 3.353\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 3.382\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 3.430\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 3.466\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 3.409\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 3.461\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 3.426\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 3.449\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 3.461\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 3.405\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 3.405\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 3.412\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 3.418\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 3.405\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 3.493\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 3.375\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 3.496\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 3.453\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 3.547\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 3.413\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 3.467\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 3.431\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 3.512\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 3.530\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 3.477\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 3.405\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 3.446\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 3.553\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 3.551\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 3.485\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 3.499\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 3.465\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 3.503\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 3.568\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 3.554\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 3.461\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 3.529\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 3.418\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 3.494\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 3.441\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 3.461\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 3.504\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 3.480\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 3.482\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 3.529\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 3.531\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 3.616\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 3.552\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 3.487\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 3.539\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 3.561\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 3.551\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 3.493\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 3.608\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 3.600\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 3.542\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 3.532\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 3.544\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 3.604\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 3.681\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 3.640\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 3.589\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 3.578\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 3.593\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 3.571\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 3.658\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 3.682\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 3.664\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 3.660\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 3.677\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 3.631\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 3.648\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 3.563\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 3.607\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 3.571\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 3.570\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 3.675\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 3.646\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 3.677\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 3.644\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 3.697\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 3.779\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 3.725\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 3.747\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 3.715\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 3.720\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 3.710\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 3.694\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 3.661\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 3.689\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 3.675\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 3.686\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 3.689\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 3.713\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 3.683\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 3.755\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 3.766\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 3.696\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 3.676\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 3.688\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 3.762\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 3.768\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 3.721\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 3.736\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 3.729\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 3.802\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 3.813\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 3.716\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 3.754\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 3.751\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 3.779\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 3.722\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 3.736\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 3.718\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 3.721\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 3.696\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 3.715\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 3.793\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 3.737\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 3.739\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 3.763\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 3.899\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 3.752\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 3.874\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 3.815\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 3.818\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 3.782\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 3.778\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 3.818\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 3.851\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 3.808\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 3.765\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 3.849\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 3.737\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 3.784\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 3.796\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 3.811\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 3.772\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 3.831\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 3.816\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 3.785\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 3.865\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 3.810\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 3.824\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 3.851\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 3.799\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 3.848\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 3.897\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 3.823\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 3.819\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 3.829\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 3.824\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 3.834\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 3.990\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 3.865\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 3.958\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 3.929\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 3.862\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 3.915\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 3.806\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 3.955\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 3.890\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 3.936\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 3.917\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 3.911\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 3.949\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 3.897\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 3.848\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 3.807\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 3.867\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 3.945\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 3.900\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 3.909\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 3.860\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 3.822\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 3.901\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 3.952\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 3.972\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 3.911\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 3.985\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 3.971\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 4.000\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 3.955\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 3.946\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 3.994\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 3.953\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 3.936\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 3.939\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 3.995\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 3.933\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 3.894\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 4.003\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 4.040\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 3.970\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 4.004\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 3.997\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 4.002\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 3.996\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 4.034\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 3.972\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 4.028\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 4.039\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 3.965\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 3.989\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 4.021\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 4.069\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 3.989\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 4.023\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 3.981\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 3.962\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 3.938\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 4.000\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 4.009\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 4.004\n",
      "[epoch: 75, i:   999] avg mini-batch loss: 3.959\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: 3.962\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: 3.981\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: 3.963\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: 4.057\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: 3.974\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: 3.988\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: 4.098\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: 4.039\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: 4.027\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: 4.051\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: 4.155\n",
      "[epoch: 76, i:   999] avg mini-batch loss: 4.015\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: 4.121\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: 4.091\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: 3.988\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: 4.102\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: 4.026\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: 4.046\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: 4.038\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: 4.033\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: 4.045\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: 4.076\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: 4.062\n",
      "[epoch: 77, i:   999] avg mini-batch loss: 4.084\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: 4.146\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: 3.970\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: 4.028\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: 4.043\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: 4.048\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: 3.977\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: 4.032\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: 3.997\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: 3.996\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: 4.089\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: 4.038\n",
      "[epoch: 78, i:   999] avg mini-batch loss: 4.107\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: 4.080\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: 4.100\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: 4.015\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: 3.996\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: 3.996\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: 4.105\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: 4.020\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: 4.064\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: 4.114\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: 4.060\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: 4.089\n",
      "[epoch: 79, i:   999] avg mini-batch loss: 3.979\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: 4.035\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: 4.045\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: 4.138\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: 4.097\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: 4.052\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: 4.042\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: 4.037\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: 4.036\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: 4.049\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: 4.151\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: 4.112\n",
      "[epoch: 80, i:   999] avg mini-batch loss: 4.092\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: 4.019\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: 4.116\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: 4.076\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: 4.189\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: 4.088\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: 4.023\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: 4.013\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: 4.092\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: 4.091\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: 4.038\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: 4.067\n",
      "[epoch: 81, i:   999] avg mini-batch loss: 4.060\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: 4.070\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: 4.094\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: 4.056\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: 4.038\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: 4.107\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: 4.057\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: 4.071\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: 4.212\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: 4.177\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: 4.139\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: 4.085\n",
      "[epoch: 82, i:   999] avg mini-batch loss: 4.154\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: 4.105\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: 4.060\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: 4.135\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: 4.095\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: 4.078\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: 4.053\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: 4.036\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: 4.156\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: 4.075\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: 4.080\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: 4.146\n",
      "[epoch: 83, i:   999] avg mini-batch loss: 4.069\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: 4.075\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: 4.068\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: 4.144\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: 4.105\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: 4.137\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: 4.107\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: 4.120\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: 4.143\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: 4.146\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: 4.174\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: 4.104\n",
      "[epoch: 84, i:   999] avg mini-batch loss: 4.112\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: 4.147\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: 4.125\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: 4.076\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: 4.115\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: 4.158\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: 4.171\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: 4.155\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: 4.141\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: 4.146\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: 4.109\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: 4.176\n",
      "[epoch: 85, i:   999] avg mini-batch loss: 4.134\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: 4.186\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: 4.120\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: 4.163\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: 4.128\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: 4.161\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: 4.123\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: 4.160\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: 4.171\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: 4.157\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: 4.182\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: 4.193\n",
      "[epoch: 86, i:   999] avg mini-batch loss: 4.137\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: 4.219\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: 4.110\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: 4.144\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: 4.166\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: 4.163\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: 4.170\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: 4.141\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: 4.187\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: 4.227\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: 4.225\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: 4.237\n",
      "[epoch: 87, i:   999] avg mini-batch loss: 4.138\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: 4.153\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: 4.161\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: 4.191\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: 4.160\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: 4.154\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: 4.261\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: 4.209\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: 4.203\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: 4.279\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: 4.286\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: 4.200\n",
      "[epoch: 88, i:   999] avg mini-batch loss: 4.134\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: 4.135\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: 4.378\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: 4.314\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: 4.181\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: 4.157\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: 4.163\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: 4.185\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: 4.188\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: 4.199\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: 4.176\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: 4.192\n",
      "[epoch: 89, i:   999] avg mini-batch loss: 4.186\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: 4.150\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: 4.152\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: 4.169\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: 4.086\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: 4.156\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: 4.246\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: 4.176\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: 4.179\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: 4.283\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: 4.276\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: 4.200\n",
      "[epoch: 90, i:   999] avg mini-batch loss: 4.167\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: 4.186\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: 4.192\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: 4.185\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: 4.155\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: 4.149\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: 4.155\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: 4.177\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: 4.198\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: 4.246\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: 4.233\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: 4.195\n",
      "[epoch: 91, i:   999] avg mini-batch loss: 4.277\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: 4.174\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: 4.188\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: 4.203\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: 4.120\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: 4.169\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: 4.199\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: 4.186\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: 4.121\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: 4.129\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: 4.102\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: 4.184\n",
      "[epoch: 92, i:   999] avg mini-batch loss: 4.161\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: 4.182\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: 4.166\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: 4.218\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: 4.147\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: 4.211\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: 4.202\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: 4.170\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: 4.140\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: 4.190\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: 4.346\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: 4.276\n",
      "[epoch: 93, i:   999] avg mini-batch loss: 4.206\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: 4.191\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: 4.173\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: 4.235\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: 4.179\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: 4.156\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: 4.120\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: 4.156\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: 4.380\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: 4.310\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: 4.220\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: 4.173\n",
      "[epoch: 94, i:   999] avg mini-batch loss: 4.230\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: 4.187\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: 4.157\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: 4.123\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: 4.213\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: 4.172\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: 4.226\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: 4.241\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: 4.149\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: 4.196\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: 4.168\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: 4.178\n",
      "[epoch: 95, i:   999] avg mini-batch loss: 4.124\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: 4.149\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: 4.184\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: 4.177\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: 4.242\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: 4.163\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: 4.194\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: 4.137\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: 4.204\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: 4.178\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: 4.136\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: 4.161\n",
      "[epoch: 96, i:   999] avg mini-batch loss: 4.190\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: 4.201\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: 4.160\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: 4.125\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: 4.204\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: 4.248\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: 4.184\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: 4.179\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: 4.175\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: 4.190\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: 4.243\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: 4.375\n",
      "[epoch: 97, i:   999] avg mini-batch loss: 4.255\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: 4.244\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: 4.180\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: 4.180\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: 4.161\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: 4.338\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: 4.171\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: 4.231\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: 4.259\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: 4.152\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: 4.171\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: 4.213\n",
      "[epoch: 98, i:   999] avg mini-batch loss: 4.213\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: 4.174\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: 4.205\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: 4.163\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: 4.223\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: 4.181\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: 4.215\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: 4.196\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: 4.226\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: 4.199\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: 4.174\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: 4.096\n",
      "[epoch: 99, i:   999] avg mini-batch loss: 4.188\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: 4.148\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: 4.190\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: 4.202\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: 4.163\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: 4.306\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: 4.207\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: 4.184\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: 4.214\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: 4.218\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: 4.162\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: 4.188\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 100       # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e936ee",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef7844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09a649",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f3a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 5 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21ad7c",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fdce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 13 %\n",
      "Accuracy of aquarium_fish :  2 %\n",
      "Accuracy of  baby :  4 %\n",
      "Accuracy of  bear :  2 %\n",
      "Accuracy of beaver :  0 %\n",
      "Accuracy of   bed :  2 %\n",
      "Accuracy of   bee :  0 %\n",
      "Accuracy of beetle :  2 %\n",
      "Accuracy of bicycle :  0 %\n",
      "Accuracy of bottle :  4 %\n",
      "Accuracy of  bowl :  0 %\n",
      "Accuracy of   boy :  0 %\n",
      "Accuracy of bridge :  7 %\n",
      "Accuracy of   bus :  4 %\n",
      "Accuracy of butterfly :  1 %\n",
      "Accuracy of camel :  2 %\n",
      "Accuracy of   can :  4 %\n",
      "Accuracy of castle : 15 %\n",
      "Accuracy of caterpillar : 18 %\n",
      "Accuracy of cattle :  1 %\n",
      "Accuracy of chair :  6 %\n",
      "Accuracy of chimpanzee :  4 %\n",
      "Accuracy of clock :  0 %\n",
      "Accuracy of cloud : 14 %\n",
      "Accuracy of cockroach :  7 %\n",
      "Accuracy of couch :  2 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  4 %\n",
      "Accuracy of   cup :  2 %\n",
      "Accuracy of dinosaur :  1 %\n",
      "Accuracy of dolphin : 18 %\n",
      "Accuracy of elephant :  1 %\n",
      "Accuracy of flatfish :  0 %\n",
      "Accuracy of forest :  3 %\n",
      "Accuracy of   fox :  0 %\n",
      "Accuracy of  girl :  4 %\n",
      "Accuracy of hamster :  3 %\n",
      "Accuracy of house :  4 %\n",
      "Accuracy of kangaroo :  5 %\n",
      "Accuracy of keyboard :  2 %\n",
      "Accuracy of  lamp :  0 %\n",
      "Accuracy of lawn_mower : 10 %\n",
      "Accuracy of leopard :  5 %\n",
      "Accuracy of  lion : 11 %\n",
      "Accuracy of lizard :  6 %\n",
      "Accuracy of lobster :  0 %\n",
      "Accuracy of   man :  1 %\n",
      "Accuracy of maple_tree :  5 %\n",
      "Accuracy of motorcycle :  7 %\n",
      "Accuracy of mountain : 10 %\n",
      "Accuracy of mouse :  1 %\n",
      "Accuracy of mushroom :  1 %\n",
      "Accuracy of oak_tree : 22 %\n",
      "Accuracy of orange : 46 %\n",
      "Accuracy of orchid :  7 %\n",
      "Accuracy of otter :  1 %\n",
      "Accuracy of palm_tree :  3 %\n",
      "Accuracy of  pear :  3 %\n",
      "Accuracy of pickup_truck :  6 %\n",
      "Accuracy of pine_tree :  4 %\n",
      "Accuracy of plain : 19 %\n",
      "Accuracy of plate :  5 %\n",
      "Accuracy of poppy : 16 %\n",
      "Accuracy of porcupine :  3 %\n",
      "Accuracy of possum :  7 %\n",
      "Accuracy of rabbit :  1 %\n",
      "Accuracy of raccoon :  3 %\n",
      "Accuracy of   ray :  2 %\n",
      "Accuracy of  road : 21 %\n",
      "Accuracy of rocket :  2 %\n",
      "Accuracy of  rose : 16 %\n",
      "Accuracy of   sea : 12 %\n",
      "Accuracy of  seal :  1 %\n",
      "Accuracy of shark :  9 %\n",
      "Accuracy of shrew :  1 %\n",
      "Accuracy of skunk : 10 %\n",
      "Accuracy of skyscraper :  9 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  4 %\n",
      "Accuracy of spider :  1 %\n",
      "Accuracy of squirrel :  1 %\n",
      "Accuracy of streetcar :  8 %\n",
      "Accuracy of sunflower : 18 %\n",
      "Accuracy of sweet_pepper :  6 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank : 11 %\n",
      "Accuracy of telephone :  1 %\n",
      "Accuracy of television : 13 %\n",
      "Accuracy of tiger :  8 %\n",
      "Accuracy of tractor :  2 %\n",
      "Accuracy of train :  3 %\n",
      "Accuracy of trout :  2 %\n",
      "Accuracy of tulip : 11 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe : 17 %\n",
      "Accuracy of whale : 11 %\n",
      "Accuracy of willow_tree :  8 %\n",
      "Accuracy of  wolf :  6 %\n",
      "Accuracy of woman :  2 %\n",
      "Accuracy of  worm :  1 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
