{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "092e5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb3fc",
   "metadata": {},
   "source": [
    "## Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a702576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.Resize(224,antialias=True),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a07645",
   "metadata": {},
   "source": [
    "## Selected Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6776c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5759ea",
   "metadata": {},
   "source": [
    "## Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c4c8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  (fc4): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images were resized from (32,32) -> (224,224) to implement original AlexNet model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11 , stride=4, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5, stride=1, padding=2)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 100)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) \n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = AlexNet()     # Create the network instance.\n",
    "net.to(device)  # Move t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c2ad",
   "metadata": {},
   "source": [
    "## Select Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d17361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.605\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.602\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.584\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.542\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.394\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.347\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.314\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.285\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.254\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.228\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.169\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.174\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.154\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.112\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.079\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.065\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 3.976\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 3.992\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 3.980\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 3.936\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.936\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.880\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.863\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.837\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.829\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.776\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.715\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.725\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.638\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.591\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.576\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.539\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.527\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.469\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.482\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.460\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.436\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.409\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.329\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.278\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.191\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.133\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.148\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.146\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.063\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.125\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.029\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.061\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.041\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.028\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 2.965\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 2.952\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.800\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.809\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 2.776\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 2.777\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.767\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 2.747\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 2.713\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.705\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.691\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.619\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.739\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.634\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.478\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.448\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.524\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.474\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.464\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.454\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.477\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.438\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.466\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.443\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.408\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.425\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.182\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.163\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.218\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.230\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.223\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.208\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.275\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.222\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.267\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.231\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.277\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.208\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 1.941\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 1.980\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.021\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 1.965\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.055\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.036\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.063\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.073\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.055\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.121\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.089\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.044\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 1.760\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 1.819\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 1.769\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 1.862\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 1.850\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 1.876\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 1.892\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 1.916\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 1.948\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 1.906\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 1.895\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 1.975\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 1.585\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 1.624\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 1.689\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 1.716\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 1.731\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 1.729\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 1.747\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 1.791\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 1.852\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 1.837\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 1.817\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 1.789\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 1.491\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 1.529\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 1.551\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 1.582\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 1.648\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 1.666\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 1.684\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 1.704\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 1.674\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 1.697\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 1.690\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 1.730\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 1.367\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 1.421\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 1.505\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 1.569\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 1.528\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 1.553\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 1.563\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 1.608\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 1.592\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 1.635\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 1.682\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 1.658\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 1.285\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 1.340\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 1.462\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 1.427\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 1.464\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 1.463\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 1.505\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 1.574\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 1.563\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 1.646\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 1.644\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 1.710\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 1.304\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 1.357\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 1.452\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 1.410\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 1.459\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 1.453\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 1.515\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 1.484\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 1.509\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 1.600\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 1.570\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 1.625\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 1.331\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 1.375\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 1.353\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 1.357\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 1.425\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 1.491\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 1.500\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 1.528\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 1.558\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 1.582\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 1.638\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 1.651\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 1.274\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 1.297\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 1.312\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 1.403\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 1.434\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 1.439\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 1.520\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 1.571\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 1.540\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 1.623\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 1.630\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 1.668\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 1.329\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 1.411\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 1.467\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 1.411\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 1.456\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 1.477\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 1.544\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 1.519\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 1.682\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 1.609\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 1.634\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 1.696\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 1.420\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 1.498\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 1.440\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 1.553\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 1.518\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 1.562\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 1.611\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 1.685\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 1.650\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 1.730\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 1.614\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 1.752\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 1.461\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 1.512\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 1.596\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 1.637\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 1.655\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 1.744\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 1.694\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 1.711\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 1.711\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 1.795\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 1.893\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 1.843\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 1.596\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 1.600\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 1.656\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 1.764\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 1.755\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 1.781\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 1.942\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 1.844\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 1.979\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 2.001\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 2.030\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 2.060\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 1.800\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 1.798\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 1.895\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 1.812\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 1.896\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 1.929\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 2.180\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 2.014\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 2.081\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 2.093\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 2.123\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 2.122\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 1.912\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 1.912\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 2.093\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 2.051\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 2.182\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 2.190\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 2.110\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 2.185\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 2.180\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 2.241\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 2.299\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 2.337\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 2.120\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 2.138\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 2.181\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 2.346\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 2.279\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 2.292\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 2.376\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 2.455\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 2.428\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 2.451\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 2.442\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 2.538\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 2.369\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 2.399\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 2.423\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 2.477\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 2.520\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 2.640\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 2.399\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 2.569\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 2.568\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 2.706\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 2.802\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 2.766\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 2.579\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 2.559\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 2.646\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 2.713\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 2.729\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 2.838\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 2.835\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 2.885\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 2.830\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 2.988\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 2.965\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 3.055\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 2.853\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 2.872\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 2.904\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 2.945\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 3.005\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 3.131\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 2.993\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 3.082\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 3.128\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 3.055\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 3.055\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 3.116\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 3.031\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 3.002\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 3.098\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 3.192\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 3.128\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 3.277\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 3.235\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 3.285\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 3.293\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 3.389\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 3.215\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 3.304\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 3.200\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 3.138\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 3.315\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 3.325\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 3.427\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 3.380\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 3.289\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 3.413\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 3.405\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 3.611\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 3.463\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 3.467\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 3.440\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 3.466\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 3.347\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 3.384\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 3.609\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 3.598\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 3.539\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 3.476\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 3.551\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 3.707\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 3.542\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 3.699\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 3.572\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 3.507\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 3.778\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 3.649\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 3.763\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 3.729\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 3.819\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 3.657\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 3.694\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 3.719\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 3.813\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 3.634\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 3.558\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 3.759\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 3.701\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 3.719\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 3.726\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 3.946\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 3.796\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 3.781\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 3.718\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 3.788\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 3.728\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 3.840\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 3.814\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 3.717\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 3.778\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 3.756\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 3.822\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 3.810\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 3.830\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 3.804\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 3.858\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 3.855\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 3.875\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 3.919\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 3.714\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 3.718\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 3.839\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 3.823\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 3.873\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 3.921\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 3.942\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 3.910\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 4.014\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 3.942\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 3.992\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 3.968\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 3.854\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 3.969\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 3.865\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 3.806\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 3.875\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 3.823\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 4.040\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 4.038\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 4.013\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 3.909\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 3.924\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 4.000\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 3.959\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 4.008\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 3.999\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 3.930\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 3.993\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 3.979\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 4.029\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 3.948\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 3.879\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 4.003\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 3.999\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 3.838\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 3.890\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 3.886\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 3.893\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 4.001\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 3.934\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 3.957\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 3.988\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 3.953\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 3.981\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 3.898\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 4.013\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 4.027\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 4.115\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 3.935\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 3.913\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 4.113\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 3.991\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 4.062\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 4.021\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 4.029\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 4.010\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 4.196\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 4.146\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 4.180\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 4.117\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 4.092\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 4.026\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 4.099\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 3.997\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 4.011\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 4.085\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 4.066\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 4.018\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 4.132\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 4.105\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 4.141\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 4.032\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 4.154\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 4.092\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 4.156\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 4.165\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 4.082\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 4.210\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 4.196\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 4.187\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 4.262\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 4.168\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 4.179\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 4.180\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 4.043\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 4.128\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 4.092\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 4.143\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 4.189\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 4.138\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 4.112\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 4.095\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 4.196\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 4.123\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 4.154\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 4.164\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 4.232\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 4.113\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 4.127\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 4.086\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 4.343\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 4.079\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 4.179\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 4.184\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 4.208\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 4.272\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 4.160\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 4.091\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 4.047\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 4.130\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 4.133\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 4.199\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 4.094\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 4.116\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 4.178\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 4.136\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 4.104\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 4.142\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 4.138\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 4.165\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 4.182\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 4.110\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 4.236\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 4.073\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 4.128\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 4.217\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 4.261\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 4.184\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 4.160\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 4.249\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 4.152\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 4.213\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 4.381\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 4.169\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 4.243\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 4.249\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 4.229\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 4.194\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 4.296\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 4.226\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 4.250\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 4.290\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 4.298\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 4.127\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 4.096\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 4.289\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 4.164\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 4.164\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 4.158\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 4.192\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 4.119\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 4.173\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 4.380\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 4.243\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 4.244\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 4.099\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 4.082\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 4.186\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 4.164\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 4.164\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 4.627\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 4.541\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 4.513\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 4.547\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 4.558\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 4.510\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 4.384\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 4.386\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 4.331\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 4.235\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 4.328\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 4.261\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 4.313\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 4.223\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 4.171\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 4.190\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 4.271\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 4.201\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 4.264\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 4.239\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 4.134\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 4.155\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 4.213\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 4.179\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 4.190\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 4.280\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 4.223\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 4.106\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 4.192\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 4.267\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 4.206\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 4.187\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 4.248\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 4.210\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 4.282\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 4.182\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 4.174\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 4.134\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 4.205\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 4.181\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 4.237\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 4.235\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 4.241\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 4.237\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 4.172\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 4.305\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 4.177\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 4.230\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 4.165\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 4.259\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 4.260\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 4.295\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 4.196\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 4.622\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 4.439\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 4.346\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 4.325\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 4.238\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 4.344\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 4.257\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 4.258\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 4.355\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 4.388\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 4.357\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 4.308\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 4.185\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 4.263\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 4.238\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 4.278\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 4.223\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 4.214\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 4.149\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 4.374\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 4.260\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 4.270\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 4.237\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 4.228\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 4.179\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 4.449\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 4.630\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 4.623\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 4.620\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 4.622\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 4.612\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 4.618\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 4.611\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 4.614\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 4.614\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 4.612\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 9120.124\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:   999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: nan\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: nan\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: nan\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: nan\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 100       # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e936ee",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef7844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09a649",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f3a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 1 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21ad7c",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fdce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 100 %\n",
      "Accuracy of aquarium_fish :  0 %\n",
      "Accuracy of  baby :  0 %\n",
      "Accuracy of  bear :  0 %\n",
      "Accuracy of beaver :  0 %\n",
      "Accuracy of   bed :  0 %\n",
      "Accuracy of   bee :  0 %\n",
      "Accuracy of beetle :  0 %\n",
      "Accuracy of bicycle :  0 %\n",
      "Accuracy of bottle :  0 %\n",
      "Accuracy of  bowl :  0 %\n",
      "Accuracy of   boy :  0 %\n",
      "Accuracy of bridge :  0 %\n",
      "Accuracy of   bus :  0 %\n",
      "Accuracy of butterfly :  0 %\n",
      "Accuracy of camel :  0 %\n",
      "Accuracy of   can :  0 %\n",
      "Accuracy of castle :  0 %\n",
      "Accuracy of caterpillar :  0 %\n",
      "Accuracy of cattle :  0 %\n",
      "Accuracy of chair :  0 %\n",
      "Accuracy of chimpanzee :  0 %\n",
      "Accuracy of clock :  0 %\n",
      "Accuracy of cloud :  0 %\n",
      "Accuracy of cockroach :  0 %\n",
      "Accuracy of couch :  0 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  0 %\n",
      "Accuracy of   cup :  0 %\n",
      "Accuracy of dinosaur :  0 %\n",
      "Accuracy of dolphin :  0 %\n",
      "Accuracy of elephant :  0 %\n",
      "Accuracy of flatfish :  0 %\n",
      "Accuracy of forest :  0 %\n",
      "Accuracy of   fox :  0 %\n",
      "Accuracy of  girl :  0 %\n",
      "Accuracy of hamster :  0 %\n",
      "Accuracy of house :  0 %\n",
      "Accuracy of kangaroo :  0 %\n",
      "Accuracy of keyboard :  0 %\n",
      "Accuracy of  lamp :  0 %\n",
      "Accuracy of lawn_mower :  0 %\n",
      "Accuracy of leopard :  0 %\n",
      "Accuracy of  lion :  0 %\n",
      "Accuracy of lizard :  0 %\n",
      "Accuracy of lobster :  0 %\n",
      "Accuracy of   man :  0 %\n",
      "Accuracy of maple_tree :  0 %\n",
      "Accuracy of motorcycle :  0 %\n",
      "Accuracy of mountain :  0 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  0 %\n",
      "Accuracy of orange :  0 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  0 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  0 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  0 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  0 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  0 %\n",
      "Accuracy of rocket :  0 %\n",
      "Accuracy of  rose :  0 %\n",
      "Accuracy of   sea :  0 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  0 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  0 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  0 %\n",
      "Accuracy of tractor :  0 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe :  0 %\n",
      "Accuracy of whale :  0 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
