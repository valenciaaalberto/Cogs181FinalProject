{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8043b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3eb3fc",
   "metadata": {},
   "source": [
    "## Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a702576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),transforms.RandomResizedCrop(224,antialias=True),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a07645",
   "metadata": {},
   "source": [
    "## Selected Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6776c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5759ea",
   "metadata": {},
   "source": [
    "## Created Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c4c8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (conv_layer_one): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv_layer_three): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=6400, out_features=4096, bias=True)\n",
       "  (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "  (fc3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Images were resized from (32,32) -> (224,224) to implement original AlexNet model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=96,kernel_size=11 , stride=4, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5, stride=1, padding=2)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=384,out_channels=384,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=384,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(6400, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.fc3 = nn.Linear(2048, 1000)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.5)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,(3,3),stride=2)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)    \n",
    "        return x\n",
    "\n",
    "\n",
    "net = AlexNet()     # Create the network instance.\n",
    "net.to(device)  # Move t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911c2ad",
   "metadata": {},
   "source": [
    "## Select Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12d8556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use cross-entropy as loss function.\n",
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d17361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 6.366\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.910\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.807\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.766\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.750\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.724\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.737\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.717\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.716\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.706\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.703\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.698\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.560\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.511\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.445\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.429\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.403\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.381\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.349\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.298\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.284\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.277\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.253\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.211\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.145\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 4.142\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 4.093\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 4.067\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 4.055\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 4.060\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 4.012\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 4.030\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 4.023\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.963\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.968\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.949\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.920\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.917\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.889\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.873\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.847\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.832\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.807\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.789\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.754\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.747\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.745\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.719\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.677\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.682\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.670\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.667\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.608\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.581\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.627\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.613\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.604\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.567\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 3.603\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 3.551\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 3.522\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 3.457\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 3.505\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 3.459\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 3.480\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 3.430\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 3.430\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 3.449\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 3.395\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 3.371\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 3.371\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 3.401\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 3.297\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 3.341\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 3.314\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 3.295\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 3.283\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 3.302\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 3.302\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 3.254\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 3.217\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 3.263\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 3.217\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 3.231\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 3.211\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 3.144\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 3.202\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 3.133\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 3.199\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 3.181\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 3.151\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 3.122\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 3.154\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 3.107\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 3.091\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 3.099\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 3.054\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 3.088\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 3.060\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 3.035\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 3.024\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.994\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.980\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 3.056\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.996\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 3.023\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.990\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 3.031\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 2.955\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 2.956\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 2.941\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 2.968\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 2.934\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 2.878\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 2.971\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 2.945\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 2.966\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 2.893\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 2.937\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 2.944\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 2.849\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 2.834\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 2.859\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 2.871\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 2.909\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 2.932\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 2.943\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 2.847\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 2.836\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 2.786\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 2.850\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 2.848\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 2.778\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 2.798\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 2.791\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 2.782\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 2.835\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 2.753\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 2.770\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 2.819\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 2.803\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 2.757\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 2.807\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 2.773\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 2.774\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 2.745\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 2.748\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 2.745\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 2.740\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 2.724\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 2.754\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 2.748\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 2.774\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 2.690\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 2.721\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 2.734\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 2.692\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 2.712\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 2.704\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 2.736\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 2.642\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 2.668\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 2.712\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 2.699\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 2.648\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 2.682\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 2.711\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 2.716\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 2.578\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 2.679\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 2.603\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 2.641\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 2.640\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 2.615\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 2.683\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 2.711\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 2.692\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 2.678\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 2.628\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 2.686\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 2.561\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 2.541\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 2.645\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 2.640\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 2.569\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 2.658\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 2.605\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 2.579\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 2.607\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 2.684\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 2.655\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 2.626\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 2.590\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 2.530\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 2.588\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 2.635\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 2.597\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 2.584\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 2.608\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 2.691\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 2.613\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 2.663\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 2.608\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 2.598\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 2.497\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 2.594\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 2.579\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 2.548\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 2.589\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 2.630\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 2.623\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 2.603\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 2.557\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 2.635\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 2.570\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 2.561\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 2.506\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 2.504\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 2.533\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 2.553\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 2.531\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 2.539\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 2.586\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 2.575\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 2.574\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 2.552\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 2.518\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 2.568\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 2.456\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 2.510\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 2.570\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 2.540\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 2.566\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 2.581\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 2.535\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 2.537\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 2.544\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 2.609\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 2.561\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 2.573\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 2.434\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 2.553\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 2.558\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 2.513\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 2.611\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 2.568\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 2.526\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 2.540\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 2.600\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 2.621\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 2.583\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 2.487\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 2.442\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 2.532\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 2.529\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 2.522\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 2.567\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 2.500\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 2.533\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 2.570\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 2.544\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 2.545\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 2.563\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 2.495\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 2.508\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 2.475\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 2.527\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 2.494\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 2.460\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 2.496\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 2.572\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 2.538\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 2.524\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 2.576\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 2.614\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 2.552\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 2.501\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 2.512\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 2.473\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 2.507\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 2.505\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 2.505\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 2.475\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 2.504\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 2.593\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 2.531\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 2.523\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 2.590\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 2.513\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 2.494\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 2.478\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 2.517\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 2.594\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 2.554\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 2.519\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 2.542\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 2.526\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 2.561\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 2.529\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 2.553\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 2.463\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 2.512\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 2.530\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 2.516\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 2.507\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 2.534\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 2.554\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 2.563\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 2.559\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 2.574\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 2.576\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 2.591\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 2.472\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 2.457\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 2.536\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 2.467\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 2.540\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 2.549\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 2.524\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 2.557\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 2.555\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 2.572\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 2.580\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 2.529\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 2.518\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 2.508\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 2.509\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 2.515\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 2.536\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 2.549\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 2.575\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 2.552\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 2.557\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 2.588\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 2.592\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 2.605\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 2.465\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 2.483\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 2.564\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 2.574\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 2.535\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 2.538\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 2.559\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 2.565\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 2.549\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 2.623\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 2.565\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 2.587\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 2.531\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 2.574\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 2.504\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 2.568\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 2.538\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 2.588\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 2.601\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 2.560\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 2.623\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 2.558\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 2.577\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 2.665\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 2.536\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 2.565\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 2.550\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 2.504\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 2.638\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 2.531\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 2.545\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 2.582\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 2.597\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 2.610\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 2.606\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 2.618\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 2.566\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 2.530\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 2.570\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 2.519\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 2.611\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 2.592\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 2.553\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 2.613\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 2.623\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 2.604\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 2.631\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 2.655\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 2.553\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 2.607\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 2.603\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 2.594\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 2.583\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 2.571\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 2.619\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 2.594\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 2.611\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 2.651\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 2.646\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 2.599\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 2.550\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 2.638\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 2.550\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 2.612\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 2.586\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 2.587\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 2.618\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 2.573\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 2.670\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 2.647\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 2.637\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 2.674\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 2.591\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 2.668\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 2.635\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 2.642\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 2.654\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 2.707\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 2.633\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 2.707\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 2.639\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 2.675\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 2.708\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 2.627\n",
      "[epoch: 35, i:   999] avg mini-batch loss: 2.645\n",
      "[epoch: 35, i:  1999] avg mini-batch loss: 2.634\n",
      "[epoch: 35, i:  2999] avg mini-batch loss: 2.659\n",
      "[epoch: 35, i:  3999] avg mini-batch loss: 2.662\n",
      "[epoch: 35, i:  4999] avg mini-batch loss: 2.654\n",
      "[epoch: 35, i:  5999] avg mini-batch loss: 2.663\n",
      "[epoch: 35, i:  6999] avg mini-batch loss: 2.600\n",
      "[epoch: 35, i:  7999] avg mini-batch loss: 2.675\n",
      "[epoch: 35, i:  8999] avg mini-batch loss: 2.671\n",
      "[epoch: 35, i:  9999] avg mini-batch loss: 2.676\n",
      "[epoch: 35, i: 10999] avg mini-batch loss: 2.710\n",
      "[epoch: 35, i: 11999] avg mini-batch loss: 2.761\n",
      "[epoch: 36, i:   999] avg mini-batch loss: 2.682\n",
      "[epoch: 36, i:  1999] avg mini-batch loss: 2.700\n",
      "[epoch: 36, i:  2999] avg mini-batch loss: 2.637\n",
      "[epoch: 36, i:  3999] avg mini-batch loss: 2.668\n",
      "[epoch: 36, i:  4999] avg mini-batch loss: 2.638\n",
      "[epoch: 36, i:  5999] avg mini-batch loss: 2.700\n",
      "[epoch: 36, i:  6999] avg mini-batch loss: 2.709\n",
      "[epoch: 36, i:  7999] avg mini-batch loss: 2.659\n",
      "[epoch: 36, i:  8999] avg mini-batch loss: 2.708\n",
      "[epoch: 36, i:  9999] avg mini-batch loss: 2.736\n",
      "[epoch: 36, i: 10999] avg mini-batch loss: 2.689\n",
      "[epoch: 36, i: 11999] avg mini-batch loss: 2.644\n",
      "[epoch: 37, i:   999] avg mini-batch loss: 2.624\n",
      "[epoch: 37, i:  1999] avg mini-batch loss: 2.719\n",
      "[epoch: 37, i:  2999] avg mini-batch loss: 2.644\n",
      "[epoch: 37, i:  3999] avg mini-batch loss: 2.654\n",
      "[epoch: 37, i:  4999] avg mini-batch loss: 2.723\n",
      "[epoch: 37, i:  5999] avg mini-batch loss: 2.708\n",
      "[epoch: 37, i:  6999] avg mini-batch loss: 2.727\n",
      "[epoch: 37, i:  7999] avg mini-batch loss: 2.727\n",
      "[epoch: 37, i:  8999] avg mini-batch loss: 2.705\n",
      "[epoch: 37, i:  9999] avg mini-batch loss: 2.687\n",
      "[epoch: 37, i: 10999] avg mini-batch loss: 2.722\n",
      "[epoch: 37, i: 11999] avg mini-batch loss: 2.780\n",
      "[epoch: 38, i:   999] avg mini-batch loss: 2.605\n",
      "[epoch: 38, i:  1999] avg mini-batch loss: 2.691\n",
      "[epoch: 38, i:  2999] avg mini-batch loss: 2.650\n",
      "[epoch: 38, i:  3999] avg mini-batch loss: 2.686\n",
      "[epoch: 38, i:  4999] avg mini-batch loss: 2.679\n",
      "[epoch: 38, i:  5999] avg mini-batch loss: 2.712\n",
      "[epoch: 38, i:  6999] avg mini-batch loss: 2.760\n",
      "[epoch: 38, i:  7999] avg mini-batch loss: 2.731\n",
      "[epoch: 38, i:  8999] avg mini-batch loss: 2.750\n",
      "[epoch: 38, i:  9999] avg mini-batch loss: 2.725\n",
      "[epoch: 38, i: 10999] avg mini-batch loss: 2.715\n",
      "[epoch: 38, i: 11999] avg mini-batch loss: 2.759\n",
      "[epoch: 39, i:   999] avg mini-batch loss: 2.686\n",
      "[epoch: 39, i:  1999] avg mini-batch loss: 2.716\n",
      "[epoch: 39, i:  2999] avg mini-batch loss: 2.727\n",
      "[epoch: 39, i:  3999] avg mini-batch loss: 2.736\n",
      "[epoch: 39, i:  4999] avg mini-batch loss: 2.682\n",
      "[epoch: 39, i:  5999] avg mini-batch loss: 2.725\n",
      "[epoch: 39, i:  6999] avg mini-batch loss: 2.795\n",
      "[epoch: 39, i:  7999] avg mini-batch loss: 2.716\n",
      "[epoch: 39, i:  8999] avg mini-batch loss: 2.747\n",
      "[epoch: 39, i:  9999] avg mini-batch loss: 2.731\n",
      "[epoch: 39, i: 10999] avg mini-batch loss: 2.776\n",
      "[epoch: 39, i: 11999] avg mini-batch loss: 2.753\n",
      "[epoch: 40, i:   999] avg mini-batch loss: 2.715\n",
      "[epoch: 40, i:  1999] avg mini-batch loss: 2.733\n",
      "[epoch: 40, i:  2999] avg mini-batch loss: 2.718\n",
      "[epoch: 40, i:  3999] avg mini-batch loss: 2.760\n",
      "[epoch: 40, i:  4999] avg mini-batch loss: 2.819\n",
      "[epoch: 40, i:  5999] avg mini-batch loss: 2.846\n",
      "[epoch: 40, i:  6999] avg mini-batch loss: 2.750\n",
      "[epoch: 40, i:  7999] avg mini-batch loss: 2.798\n",
      "[epoch: 40, i:  8999] avg mini-batch loss: 2.827\n",
      "[epoch: 40, i:  9999] avg mini-batch loss: 2.779\n",
      "[epoch: 40, i: 10999] avg mini-batch loss: 2.852\n",
      "[epoch: 40, i: 11999] avg mini-batch loss: 2.806\n",
      "[epoch: 41, i:   999] avg mini-batch loss: 2.751\n",
      "[epoch: 41, i:  1999] avg mini-batch loss: 2.742\n",
      "[epoch: 41, i:  2999] avg mini-batch loss: 2.791\n",
      "[epoch: 41, i:  3999] avg mini-batch loss: 2.794\n",
      "[epoch: 41, i:  4999] avg mini-batch loss: 2.780\n",
      "[epoch: 41, i:  5999] avg mini-batch loss: 2.863\n",
      "[epoch: 41, i:  6999] avg mini-batch loss: 2.758\n",
      "[epoch: 41, i:  7999] avg mini-batch loss: 2.814\n",
      "[epoch: 41, i:  8999] avg mini-batch loss: 2.778\n",
      "[epoch: 41, i:  9999] avg mini-batch loss: 2.731\n",
      "[epoch: 41, i: 10999] avg mini-batch loss: 2.793\n",
      "[epoch: 41, i: 11999] avg mini-batch loss: 2.833\n",
      "[epoch: 42, i:   999] avg mini-batch loss: 2.793\n",
      "[epoch: 42, i:  1999] avg mini-batch loss: 2.788\n",
      "[epoch: 42, i:  2999] avg mini-batch loss: 2.758\n",
      "[epoch: 42, i:  3999] avg mini-batch loss: 2.817\n",
      "[epoch: 42, i:  4999] avg mini-batch loss: 2.818\n",
      "[epoch: 42, i:  5999] avg mini-batch loss: 2.856\n",
      "[epoch: 42, i:  6999] avg mini-batch loss: 2.820\n",
      "[epoch: 42, i:  7999] avg mini-batch loss: 2.848\n",
      "[epoch: 42, i:  8999] avg mini-batch loss: 2.884\n",
      "[epoch: 42, i:  9999] avg mini-batch loss: 2.827\n",
      "[epoch: 42, i: 10999] avg mini-batch loss: 2.810\n",
      "[epoch: 42, i: 11999] avg mini-batch loss: 2.853\n",
      "[epoch: 43, i:   999] avg mini-batch loss: 2.822\n",
      "[epoch: 43, i:  1999] avg mini-batch loss: 2.788\n",
      "[epoch: 43, i:  2999] avg mini-batch loss: 2.800\n",
      "[epoch: 43, i:  3999] avg mini-batch loss: 2.801\n",
      "[epoch: 43, i:  4999] avg mini-batch loss: 2.878\n",
      "[epoch: 43, i:  5999] avg mini-batch loss: 2.845\n",
      "[epoch: 43, i:  6999] avg mini-batch loss: 2.850\n",
      "[epoch: 43, i:  7999] avg mini-batch loss: 2.802\n",
      "[epoch: 43, i:  8999] avg mini-batch loss: 2.890\n",
      "[epoch: 43, i:  9999] avg mini-batch loss: 2.894\n",
      "[epoch: 43, i: 10999] avg mini-batch loss: 2.897\n",
      "[epoch: 43, i: 11999] avg mini-batch loss: 2.888\n",
      "[epoch: 44, i:   999] avg mini-batch loss: 2.820\n",
      "[epoch: 44, i:  1999] avg mini-batch loss: 2.884\n",
      "[epoch: 44, i:  2999] avg mini-batch loss: 2.819\n",
      "[epoch: 44, i:  3999] avg mini-batch loss: 2.861\n",
      "[epoch: 44, i:  4999] avg mini-batch loss: 2.863\n",
      "[epoch: 44, i:  5999] avg mini-batch loss: 2.868\n",
      "[epoch: 44, i:  6999] avg mini-batch loss: 2.878\n",
      "[epoch: 44, i:  7999] avg mini-batch loss: 2.837\n",
      "[epoch: 44, i:  8999] avg mini-batch loss: 2.866\n",
      "[epoch: 44, i:  9999] avg mini-batch loss: 2.891\n",
      "[epoch: 44, i: 10999] avg mini-batch loss: 2.903\n",
      "[epoch: 44, i: 11999] avg mini-batch loss: 2.899\n",
      "[epoch: 45, i:   999] avg mini-batch loss: 2.890\n",
      "[epoch: 45, i:  1999] avg mini-batch loss: 2.892\n",
      "[epoch: 45, i:  2999] avg mini-batch loss: 2.886\n",
      "[epoch: 45, i:  3999] avg mini-batch loss: 2.842\n",
      "[epoch: 45, i:  4999] avg mini-batch loss: 2.918\n",
      "[epoch: 45, i:  5999] avg mini-batch loss: 2.938\n",
      "[epoch: 45, i:  6999] avg mini-batch loss: 2.925\n",
      "[epoch: 45, i:  7999] avg mini-batch loss: 2.925\n",
      "[epoch: 45, i:  8999] avg mini-batch loss: 2.885\n",
      "[epoch: 45, i:  9999] avg mini-batch loss: 2.952\n",
      "[epoch: 45, i: 10999] avg mini-batch loss: 2.890\n",
      "[epoch: 45, i: 11999] avg mini-batch loss: 2.937\n",
      "[epoch: 46, i:   999] avg mini-batch loss: 2.917\n",
      "[epoch: 46, i:  1999] avg mini-batch loss: 2.856\n",
      "[epoch: 46, i:  2999] avg mini-batch loss: 2.884\n",
      "[epoch: 46, i:  3999] avg mini-batch loss: 2.871\n",
      "[epoch: 46, i:  4999] avg mini-batch loss: 2.967\n",
      "[epoch: 46, i:  5999] avg mini-batch loss: 2.965\n",
      "[epoch: 46, i:  6999] avg mini-batch loss: 2.965\n",
      "[epoch: 46, i:  7999] avg mini-batch loss: 2.981\n",
      "[epoch: 46, i:  8999] avg mini-batch loss: 2.896\n",
      "[epoch: 46, i:  9999] avg mini-batch loss: 2.937\n",
      "[epoch: 46, i: 10999] avg mini-batch loss: 2.896\n",
      "[epoch: 46, i: 11999] avg mini-batch loss: 2.971\n",
      "[epoch: 47, i:   999] avg mini-batch loss: 2.982\n",
      "[epoch: 47, i:  1999] avg mini-batch loss: 2.894\n",
      "[epoch: 47, i:  2999] avg mini-batch loss: 2.926\n",
      "[epoch: 47, i:  3999] avg mini-batch loss: 2.909\n",
      "[epoch: 47, i:  4999] avg mini-batch loss: 2.991\n",
      "[epoch: 47, i:  5999] avg mini-batch loss: 2.932\n",
      "[epoch: 47, i:  6999] avg mini-batch loss: 3.020\n",
      "[epoch: 47, i:  7999] avg mini-batch loss: 2.960\n",
      "[epoch: 47, i:  8999] avg mini-batch loss: 2.972\n",
      "[epoch: 47, i:  9999] avg mini-batch loss: 2.982\n",
      "[epoch: 47, i: 10999] avg mini-batch loss: 3.046\n",
      "[epoch: 47, i: 11999] avg mini-batch loss: 3.037\n",
      "[epoch: 48, i:   999] avg mini-batch loss: 2.984\n",
      "[epoch: 48, i:  1999] avg mini-batch loss: 2.942\n",
      "[epoch: 48, i:  2999] avg mini-batch loss: 2.979\n",
      "[epoch: 48, i:  3999] avg mini-batch loss: 2.998\n",
      "[epoch: 48, i:  4999] avg mini-batch loss: 3.002\n",
      "[epoch: 48, i:  5999] avg mini-batch loss: 2.951\n",
      "[epoch: 48, i:  6999] avg mini-batch loss: 2.972\n",
      "[epoch: 48, i:  7999] avg mini-batch loss: 3.038\n",
      "[epoch: 48, i:  8999] avg mini-batch loss: 3.050\n",
      "[epoch: 48, i:  9999] avg mini-batch loss: 3.049\n",
      "[epoch: 48, i: 10999] avg mini-batch loss: 3.039\n",
      "[epoch: 48, i: 11999] avg mini-batch loss: 3.052\n",
      "[epoch: 49, i:   999] avg mini-batch loss: 3.016\n",
      "[epoch: 49, i:  1999] avg mini-batch loss: 2.909\n",
      "[epoch: 49, i:  2999] avg mini-batch loss: 3.008\n",
      "[epoch: 49, i:  3999] avg mini-batch loss: 3.076\n",
      "[epoch: 49, i:  4999] avg mini-batch loss: 2.990\n",
      "[epoch: 49, i:  5999] avg mini-batch loss: 2.967\n",
      "[epoch: 49, i:  6999] avg mini-batch loss: 3.033\n",
      "[epoch: 49, i:  7999] avg mini-batch loss: 3.014\n",
      "[epoch: 49, i:  8999] avg mini-batch loss: 3.043\n",
      "[epoch: 49, i:  9999] avg mini-batch loss: 3.023\n",
      "[epoch: 49, i: 10999] avg mini-batch loss: 3.051\n",
      "[epoch: 49, i: 11999] avg mini-batch loss: 3.074\n",
      "[epoch: 50, i:   999] avg mini-batch loss: 3.020\n",
      "[epoch: 50, i:  1999] avg mini-batch loss: 3.043\n",
      "[epoch: 50, i:  2999] avg mini-batch loss: 3.010\n",
      "[epoch: 50, i:  3999] avg mini-batch loss: 3.033\n",
      "[epoch: 50, i:  4999] avg mini-batch loss: 3.109\n",
      "[epoch: 50, i:  5999] avg mini-batch loss: 3.053\n",
      "[epoch: 50, i:  6999] avg mini-batch loss: 3.099\n",
      "[epoch: 50, i:  7999] avg mini-batch loss: 3.055\n",
      "[epoch: 50, i:  8999] avg mini-batch loss: 3.096\n",
      "[epoch: 50, i:  9999] avg mini-batch loss: 3.083\n",
      "[epoch: 50, i: 10999] avg mini-batch loss: 3.085\n",
      "[epoch: 50, i: 11999] avg mini-batch loss: 3.090\n",
      "[epoch: 51, i:   999] avg mini-batch loss: 2.984\n",
      "[epoch: 51, i:  1999] avg mini-batch loss: 3.027\n",
      "[epoch: 51, i:  2999] avg mini-batch loss: 3.111\n",
      "[epoch: 51, i:  3999] avg mini-batch loss: 3.122\n",
      "[epoch: 51, i:  4999] avg mini-batch loss: 3.057\n",
      "[epoch: 51, i:  5999] avg mini-batch loss: 3.092\n",
      "[epoch: 51, i:  6999] avg mini-batch loss: 3.075\n",
      "[epoch: 51, i:  7999] avg mini-batch loss: 3.078\n",
      "[epoch: 51, i:  8999] avg mini-batch loss: 3.166\n",
      "[epoch: 51, i:  9999] avg mini-batch loss: 3.087\n",
      "[epoch: 51, i: 10999] avg mini-batch loss: 3.085\n",
      "[epoch: 51, i: 11999] avg mini-batch loss: 3.142\n",
      "[epoch: 52, i:   999] avg mini-batch loss: 3.036\n",
      "[epoch: 52, i:  1999] avg mini-batch loss: 3.068\n",
      "[epoch: 52, i:  2999] avg mini-batch loss: 3.137\n",
      "[epoch: 52, i:  3999] avg mini-batch loss: 3.159\n",
      "[epoch: 52, i:  4999] avg mini-batch loss: 3.096\n",
      "[epoch: 52, i:  5999] avg mini-batch loss: 3.130\n",
      "[epoch: 52, i:  6999] avg mini-batch loss: 3.076\n",
      "[epoch: 52, i:  7999] avg mini-batch loss: 3.128\n",
      "[epoch: 52, i:  8999] avg mini-batch loss: 3.167\n",
      "[epoch: 52, i:  9999] avg mini-batch loss: 3.133\n",
      "[epoch: 52, i: 10999] avg mini-batch loss: 3.149\n",
      "[epoch: 52, i: 11999] avg mini-batch loss: 3.150\n",
      "[epoch: 53, i:   999] avg mini-batch loss: 3.128\n",
      "[epoch: 53, i:  1999] avg mini-batch loss: 3.066\n",
      "[epoch: 53, i:  2999] avg mini-batch loss: 3.096\n",
      "[epoch: 53, i:  3999] avg mini-batch loss: 3.146\n",
      "[epoch: 53, i:  4999] avg mini-batch loss: 3.151\n",
      "[epoch: 53, i:  5999] avg mini-batch loss: 3.124\n",
      "[epoch: 53, i:  6999] avg mini-batch loss: 3.171\n",
      "[epoch: 53, i:  7999] avg mini-batch loss: 3.230\n",
      "[epoch: 53, i:  8999] avg mini-batch loss: 3.129\n",
      "[epoch: 53, i:  9999] avg mini-batch loss: 3.121\n",
      "[epoch: 53, i: 10999] avg mini-batch loss: 3.137\n",
      "[epoch: 53, i: 11999] avg mini-batch loss: 3.177\n",
      "[epoch: 54, i:   999] avg mini-batch loss: 3.144\n",
      "[epoch: 54, i:  1999] avg mini-batch loss: 3.190\n",
      "[epoch: 54, i:  2999] avg mini-batch loss: 3.203\n",
      "[epoch: 54, i:  3999] avg mini-batch loss: 3.165\n",
      "[epoch: 54, i:  4999] avg mini-batch loss: 3.133\n",
      "[epoch: 54, i:  5999] avg mini-batch loss: 3.169\n",
      "[epoch: 54, i:  6999] avg mini-batch loss: 3.111\n",
      "[epoch: 54, i:  7999] avg mini-batch loss: 3.179\n",
      "[epoch: 54, i:  8999] avg mini-batch loss: 3.247\n",
      "[epoch: 54, i:  9999] avg mini-batch loss: 3.155\n",
      "[epoch: 54, i: 10999] avg mini-batch loss: 3.225\n",
      "[epoch: 54, i: 11999] avg mini-batch loss: 3.151\n",
      "[epoch: 55, i:   999] avg mini-batch loss: 3.108\n",
      "[epoch: 55, i:  1999] avg mini-batch loss: 3.182\n",
      "[epoch: 55, i:  2999] avg mini-batch loss: 3.158\n",
      "[epoch: 55, i:  3999] avg mini-batch loss: 3.202\n",
      "[epoch: 55, i:  4999] avg mini-batch loss: 3.140\n",
      "[epoch: 55, i:  5999] avg mini-batch loss: 3.230\n",
      "[epoch: 55, i:  6999] avg mini-batch loss: 3.242\n",
      "[epoch: 55, i:  7999] avg mini-batch loss: 3.242\n",
      "[epoch: 55, i:  8999] avg mini-batch loss: 3.284\n",
      "[epoch: 55, i:  9999] avg mini-batch loss: 3.216\n",
      "[epoch: 55, i: 10999] avg mini-batch loss: 3.274\n",
      "[epoch: 55, i: 11999] avg mini-batch loss: 3.275\n",
      "[epoch: 56, i:   999] avg mini-batch loss: 3.219\n",
      "[epoch: 56, i:  1999] avg mini-batch loss: 3.274\n",
      "[epoch: 56, i:  2999] avg mini-batch loss: 3.146\n",
      "[epoch: 56, i:  3999] avg mini-batch loss: 3.196\n",
      "[epoch: 56, i:  4999] avg mini-batch loss: 3.281\n",
      "[epoch: 56, i:  5999] avg mini-batch loss: 3.284\n",
      "[epoch: 56, i:  6999] avg mini-batch loss: 3.201\n",
      "[epoch: 56, i:  7999] avg mini-batch loss: 3.251\n",
      "[epoch: 56, i:  8999] avg mini-batch loss: 3.310\n",
      "[epoch: 56, i:  9999] avg mini-batch loss: 3.305\n",
      "[epoch: 56, i: 10999] avg mini-batch loss: 3.293\n",
      "[epoch: 56, i: 11999] avg mini-batch loss: 3.215\n",
      "[epoch: 57, i:   999] avg mini-batch loss: 3.282\n",
      "[epoch: 57, i:  1999] avg mini-batch loss: 3.221\n",
      "[epoch: 57, i:  2999] avg mini-batch loss: 3.286\n",
      "[epoch: 57, i:  3999] avg mini-batch loss: 3.394\n",
      "[epoch: 57, i:  4999] avg mini-batch loss: 3.249\n",
      "[epoch: 57, i:  5999] avg mini-batch loss: 3.318\n",
      "[epoch: 57, i:  6999] avg mini-batch loss: 3.278\n",
      "[epoch: 57, i:  7999] avg mini-batch loss: 3.288\n",
      "[epoch: 57, i:  8999] avg mini-batch loss: 3.297\n",
      "[epoch: 57, i:  9999] avg mini-batch loss: 3.289\n",
      "[epoch: 57, i: 10999] avg mini-batch loss: 3.299\n",
      "[epoch: 57, i: 11999] avg mini-batch loss: 3.308\n",
      "[epoch: 58, i:   999] avg mini-batch loss: 3.257\n",
      "[epoch: 58, i:  1999] avg mini-batch loss: 3.306\n",
      "[epoch: 58, i:  2999] avg mini-batch loss: 3.227\n",
      "[epoch: 58, i:  3999] avg mini-batch loss: 3.362\n",
      "[epoch: 58, i:  4999] avg mini-batch loss: 3.433\n",
      "[epoch: 58, i:  5999] avg mini-batch loss: 3.293\n",
      "[epoch: 58, i:  6999] avg mini-batch loss: 3.295\n",
      "[epoch: 58, i:  7999] avg mini-batch loss: 3.229\n",
      "[epoch: 58, i:  8999] avg mini-batch loss: 3.265\n",
      "[epoch: 58, i:  9999] avg mini-batch loss: 3.314\n",
      "[epoch: 58, i: 10999] avg mini-batch loss: 3.303\n",
      "[epoch: 58, i: 11999] avg mini-batch loss: 3.344\n",
      "[epoch: 59, i:   999] avg mini-batch loss: 3.316\n",
      "[epoch: 59, i:  1999] avg mini-batch loss: 3.327\n",
      "[epoch: 59, i:  2999] avg mini-batch loss: 3.381\n",
      "[epoch: 59, i:  3999] avg mini-batch loss: 3.368\n",
      "[epoch: 59, i:  4999] avg mini-batch loss: 3.348\n",
      "[epoch: 59, i:  5999] avg mini-batch loss: 3.415\n",
      "[epoch: 59, i:  6999] avg mini-batch loss: 3.305\n",
      "[epoch: 59, i:  7999] avg mini-batch loss: 3.290\n",
      "[epoch: 59, i:  8999] avg mini-batch loss: 3.353\n",
      "[epoch: 59, i:  9999] avg mini-batch loss: 3.314\n",
      "[epoch: 59, i: 10999] avg mini-batch loss: 3.403\n",
      "[epoch: 59, i: 11999] avg mini-batch loss: 3.375\n",
      "[epoch: 60, i:   999] avg mini-batch loss: 3.385\n",
      "[epoch: 60, i:  1999] avg mini-batch loss: 3.332\n",
      "[epoch: 60, i:  2999] avg mini-batch loss: 3.398\n",
      "[epoch: 60, i:  3999] avg mini-batch loss: 3.367\n",
      "[epoch: 60, i:  4999] avg mini-batch loss: 3.365\n",
      "[epoch: 60, i:  5999] avg mini-batch loss: 3.263\n",
      "[epoch: 60, i:  6999] avg mini-batch loss: 3.348\n",
      "[epoch: 60, i:  7999] avg mini-batch loss: 3.359\n",
      "[epoch: 60, i:  8999] avg mini-batch loss: 3.366\n",
      "[epoch: 60, i:  9999] avg mini-batch loss: 3.408\n",
      "[epoch: 60, i: 10999] avg mini-batch loss: 3.478\n",
      "[epoch: 60, i: 11999] avg mini-batch loss: 3.374\n",
      "[epoch: 61, i:   999] avg mini-batch loss: 3.412\n",
      "[epoch: 61, i:  1999] avg mini-batch loss: 3.349\n",
      "[epoch: 61, i:  2999] avg mini-batch loss: 3.409\n",
      "[epoch: 61, i:  3999] avg mini-batch loss: 3.363\n",
      "[epoch: 61, i:  4999] avg mini-batch loss: 3.427\n",
      "[epoch: 61, i:  5999] avg mini-batch loss: 3.356\n",
      "[epoch: 61, i:  6999] avg mini-batch loss: 3.456\n",
      "[epoch: 61, i:  7999] avg mini-batch loss: 3.310\n",
      "[epoch: 61, i:  8999] avg mini-batch loss: 3.459\n",
      "[epoch: 61, i:  9999] avg mini-batch loss: 3.465\n",
      "[epoch: 61, i: 10999] avg mini-batch loss: 3.420\n",
      "[epoch: 61, i: 11999] avg mini-batch loss: 3.490\n",
      "[epoch: 62, i:   999] avg mini-batch loss: 3.400\n",
      "[epoch: 62, i:  1999] avg mini-batch loss: 3.381\n",
      "[epoch: 62, i:  2999] avg mini-batch loss: 3.417\n",
      "[epoch: 62, i:  3999] avg mini-batch loss: 3.374\n",
      "[epoch: 62, i:  4999] avg mini-batch loss: 3.457\n",
      "[epoch: 62, i:  5999] avg mini-batch loss: 3.427\n",
      "[epoch: 62, i:  6999] avg mini-batch loss: 3.419\n",
      "[epoch: 62, i:  7999] avg mini-batch loss: 3.441\n",
      "[epoch: 62, i:  8999] avg mini-batch loss: 3.452\n",
      "[epoch: 62, i:  9999] avg mini-batch loss: 3.410\n",
      "[epoch: 62, i: 10999] avg mini-batch loss: 3.469\n",
      "[epoch: 62, i: 11999] avg mini-batch loss: 3.453\n",
      "[epoch: 63, i:   999] avg mini-batch loss: 3.457\n",
      "[epoch: 63, i:  1999] avg mini-batch loss: 3.434\n",
      "[epoch: 63, i:  2999] avg mini-batch loss: 3.400\n",
      "[epoch: 63, i:  3999] avg mini-batch loss: 3.465\n",
      "[epoch: 63, i:  4999] avg mini-batch loss: 3.448\n",
      "[epoch: 63, i:  5999] avg mini-batch loss: 3.448\n",
      "[epoch: 63, i:  6999] avg mini-batch loss: 3.405\n",
      "[epoch: 63, i:  7999] avg mini-batch loss: 3.470\n",
      "[epoch: 63, i:  8999] avg mini-batch loss: 3.444\n",
      "[epoch: 63, i:  9999] avg mini-batch loss: 3.527\n",
      "[epoch: 63, i: 10999] avg mini-batch loss: 3.458\n",
      "[epoch: 63, i: 11999] avg mini-batch loss: 3.490\n",
      "[epoch: 64, i:   999] avg mini-batch loss: 3.431\n",
      "[epoch: 64, i:  1999] avg mini-batch loss: 3.445\n",
      "[epoch: 64, i:  2999] avg mini-batch loss: 3.447\n",
      "[epoch: 64, i:  3999] avg mini-batch loss: 3.510\n",
      "[epoch: 64, i:  4999] avg mini-batch loss: 3.415\n",
      "[epoch: 64, i:  5999] avg mini-batch loss: 3.394\n",
      "[epoch: 64, i:  6999] avg mini-batch loss: 3.455\n",
      "[epoch: 64, i:  7999] avg mini-batch loss: 3.496\n",
      "[epoch: 64, i:  8999] avg mini-batch loss: 3.506\n",
      "[epoch: 64, i:  9999] avg mini-batch loss: 3.510\n",
      "[epoch: 64, i: 10999] avg mini-batch loss: 3.483\n",
      "[epoch: 64, i: 11999] avg mini-batch loss: 3.523\n",
      "[epoch: 65, i:   999] avg mini-batch loss: 3.366\n",
      "[epoch: 65, i:  1999] avg mini-batch loss: 3.440\n",
      "[epoch: 65, i:  2999] avg mini-batch loss: 3.464\n",
      "[epoch: 65, i:  3999] avg mini-batch loss: 3.447\n",
      "[epoch: 65, i:  4999] avg mini-batch loss: 3.501\n",
      "[epoch: 65, i:  5999] avg mini-batch loss: 3.417\n",
      "[epoch: 65, i:  6999] avg mini-batch loss: 3.478\n",
      "[epoch: 65, i:  7999] avg mini-batch loss: 3.464\n",
      "[epoch: 65, i:  8999] avg mini-batch loss: 3.434\n",
      "[epoch: 65, i:  9999] avg mini-batch loss: 3.510\n",
      "[epoch: 65, i: 10999] avg mini-batch loss: 3.470\n",
      "[epoch: 65, i: 11999] avg mini-batch loss: 3.536\n",
      "[epoch: 66, i:   999] avg mini-batch loss: 3.463\n",
      "[epoch: 66, i:  1999] avg mini-batch loss: 3.503\n",
      "[epoch: 66, i:  2999] avg mini-batch loss: 3.578\n",
      "[epoch: 66, i:  3999] avg mini-batch loss: 3.499\n",
      "[epoch: 66, i:  4999] avg mini-batch loss: 3.560\n",
      "[epoch: 66, i:  5999] avg mini-batch loss: 3.560\n",
      "[epoch: 66, i:  6999] avg mini-batch loss: 3.579\n",
      "[epoch: 66, i:  7999] avg mini-batch loss: 3.589\n",
      "[epoch: 66, i:  8999] avg mini-batch loss: 3.471\n",
      "[epoch: 66, i:  9999] avg mini-batch loss: 3.515\n",
      "[epoch: 66, i: 10999] avg mini-batch loss: 3.489\n",
      "[epoch: 66, i: 11999] avg mini-batch loss: 3.592\n",
      "[epoch: 67, i:   999] avg mini-batch loss: 3.507\n",
      "[epoch: 67, i:  1999] avg mini-batch loss: 3.601\n",
      "[epoch: 67, i:  2999] avg mini-batch loss: 3.564\n",
      "[epoch: 67, i:  3999] avg mini-batch loss: 3.556\n",
      "[epoch: 67, i:  4999] avg mini-batch loss: 3.524\n",
      "[epoch: 67, i:  5999] avg mini-batch loss: 3.528\n",
      "[epoch: 67, i:  6999] avg mini-batch loss: 3.510\n",
      "[epoch: 67, i:  7999] avg mini-batch loss: 3.563\n",
      "[epoch: 67, i:  8999] avg mini-batch loss: 3.610\n",
      "[epoch: 67, i:  9999] avg mini-batch loss: 3.505\n",
      "[epoch: 67, i: 10999] avg mini-batch loss: 3.584\n",
      "[epoch: 67, i: 11999] avg mini-batch loss: 3.530\n",
      "[epoch: 68, i:   999] avg mini-batch loss: 3.511\n",
      "[epoch: 68, i:  1999] avg mini-batch loss: 3.531\n",
      "[epoch: 68, i:  2999] avg mini-batch loss: 3.625\n",
      "[epoch: 68, i:  3999] avg mini-batch loss: 3.545\n",
      "[epoch: 68, i:  4999] avg mini-batch loss: 3.643\n",
      "[epoch: 68, i:  5999] avg mini-batch loss: 3.652\n",
      "[epoch: 68, i:  6999] avg mini-batch loss: 3.613\n",
      "[epoch: 68, i:  7999] avg mini-batch loss: 3.527\n",
      "[epoch: 68, i:  8999] avg mini-batch loss: 3.658\n",
      "[epoch: 68, i:  9999] avg mini-batch loss: 3.702\n",
      "[epoch: 68, i: 10999] avg mini-batch loss: 3.622\n",
      "[epoch: 68, i: 11999] avg mini-batch loss: 3.635\n",
      "[epoch: 69, i:   999] avg mini-batch loss: 3.604\n",
      "[epoch: 69, i:  1999] avg mini-batch loss: 3.604\n",
      "[epoch: 69, i:  2999] avg mini-batch loss: 3.697\n",
      "[epoch: 69, i:  3999] avg mini-batch loss: 3.718\n",
      "[epoch: 69, i:  4999] avg mini-batch loss: 3.643\n",
      "[epoch: 69, i:  5999] avg mini-batch loss: 3.650\n",
      "[epoch: 69, i:  6999] avg mini-batch loss: 3.663\n",
      "[epoch: 69, i:  7999] avg mini-batch loss: 3.617\n",
      "[epoch: 69, i:  8999] avg mini-batch loss: 3.636\n",
      "[epoch: 69, i:  9999] avg mini-batch loss: 3.583\n",
      "[epoch: 69, i: 10999] avg mini-batch loss: 3.625\n",
      "[epoch: 69, i: 11999] avg mini-batch loss: 3.625\n",
      "[epoch: 70, i:   999] avg mini-batch loss: 3.593\n",
      "[epoch: 70, i:  1999] avg mini-batch loss: 3.624\n",
      "[epoch: 70, i:  2999] avg mini-batch loss: 3.680\n",
      "[epoch: 70, i:  3999] avg mini-batch loss: 3.651\n",
      "[epoch: 70, i:  4999] avg mini-batch loss: 3.594\n",
      "[epoch: 70, i:  5999] avg mini-batch loss: 3.643\n",
      "[epoch: 70, i:  6999] avg mini-batch loss: 3.696\n",
      "[epoch: 70, i:  7999] avg mini-batch loss: 3.612\n",
      "[epoch: 70, i:  8999] avg mini-batch loss: 3.645\n",
      "[epoch: 70, i:  9999] avg mini-batch loss: 3.607\n",
      "[epoch: 70, i: 10999] avg mini-batch loss: 3.678\n",
      "[epoch: 70, i: 11999] avg mini-batch loss: 3.685\n",
      "[epoch: 71, i:   999] avg mini-batch loss: 3.577\n",
      "[epoch: 71, i:  1999] avg mini-batch loss: 3.572\n",
      "[epoch: 71, i:  2999] avg mini-batch loss: 3.610\n",
      "[epoch: 71, i:  3999] avg mini-batch loss: 3.709\n",
      "[epoch: 71, i:  4999] avg mini-batch loss: 3.664\n",
      "[epoch: 71, i:  5999] avg mini-batch loss: 3.628\n",
      "[epoch: 71, i:  6999] avg mini-batch loss: 3.637\n",
      "[epoch: 71, i:  7999] avg mini-batch loss: 3.664\n",
      "[epoch: 71, i:  8999] avg mini-batch loss: 3.675\n",
      "[epoch: 71, i:  9999] avg mini-batch loss: 3.665\n",
      "[epoch: 71, i: 10999] avg mini-batch loss: 3.576\n",
      "[epoch: 71, i: 11999] avg mini-batch loss: 3.574\n",
      "[epoch: 72, i:   999] avg mini-batch loss: 3.609\n",
      "[epoch: 72, i:  1999] avg mini-batch loss: 3.601\n",
      "[epoch: 72, i:  2999] avg mini-batch loss: 3.644\n",
      "[epoch: 72, i:  3999] avg mini-batch loss: 3.675\n",
      "[epoch: 72, i:  4999] avg mini-batch loss: 3.576\n",
      "[epoch: 72, i:  5999] avg mini-batch loss: 3.658\n",
      "[epoch: 72, i:  6999] avg mini-batch loss: 3.744\n",
      "[epoch: 72, i:  7999] avg mini-batch loss: 3.556\n",
      "[epoch: 72, i:  8999] avg mini-batch loss: 3.671\n",
      "[epoch: 72, i:  9999] avg mini-batch loss: 3.643\n",
      "[epoch: 72, i: 10999] avg mini-batch loss: 3.626\n",
      "[epoch: 72, i: 11999] avg mini-batch loss: 3.755\n",
      "[epoch: 73, i:   999] avg mini-batch loss: 3.717\n",
      "[epoch: 73, i:  1999] avg mini-batch loss: 3.687\n",
      "[epoch: 73, i:  2999] avg mini-batch loss: 3.646\n",
      "[epoch: 73, i:  3999] avg mini-batch loss: 3.689\n",
      "[epoch: 73, i:  4999] avg mini-batch loss: 3.610\n",
      "[epoch: 73, i:  5999] avg mini-batch loss: 3.590\n",
      "[epoch: 73, i:  6999] avg mini-batch loss: 3.675\n",
      "[epoch: 73, i:  7999] avg mini-batch loss: 3.670\n",
      "[epoch: 73, i:  8999] avg mini-batch loss: 3.618\n",
      "[epoch: 73, i:  9999] avg mini-batch loss: 3.698\n",
      "[epoch: 73, i: 10999] avg mini-batch loss: 3.644\n",
      "[epoch: 73, i: 11999] avg mini-batch loss: 3.723\n",
      "[epoch: 74, i:   999] avg mini-batch loss: 3.702\n",
      "[epoch: 74, i:  1999] avg mini-batch loss: 3.746\n",
      "[epoch: 74, i:  2999] avg mini-batch loss: 3.715\n",
      "[epoch: 74, i:  3999] avg mini-batch loss: 3.717\n",
      "[epoch: 74, i:  4999] avg mini-batch loss: 3.717\n",
      "[epoch: 74, i:  5999] avg mini-batch loss: 3.642\n",
      "[epoch: 74, i:  6999] avg mini-batch loss: 3.778\n",
      "[epoch: 74, i:  7999] avg mini-batch loss: 3.687\n",
      "[epoch: 74, i:  8999] avg mini-batch loss: 3.673\n",
      "[epoch: 74, i:  9999] avg mini-batch loss: 3.566\n",
      "[epoch: 74, i: 10999] avg mini-batch loss: 3.717\n",
      "[epoch: 74, i: 11999] avg mini-batch loss: 3.678\n",
      "[epoch: 75, i:   999] avg mini-batch loss: 3.625\n",
      "[epoch: 75, i:  1999] avg mini-batch loss: 3.700\n",
      "[epoch: 75, i:  2999] avg mini-batch loss: 3.717\n",
      "[epoch: 75, i:  3999] avg mini-batch loss: 3.681\n",
      "[epoch: 75, i:  4999] avg mini-batch loss: 3.668\n",
      "[epoch: 75, i:  5999] avg mini-batch loss: 3.710\n",
      "[epoch: 75, i:  6999] avg mini-batch loss: 3.690\n",
      "[epoch: 75, i:  7999] avg mini-batch loss: 3.676\n",
      "[epoch: 75, i:  8999] avg mini-batch loss: 3.719\n",
      "[epoch: 75, i:  9999] avg mini-batch loss: 3.668\n",
      "[epoch: 75, i: 10999] avg mini-batch loss: 3.711\n",
      "[epoch: 75, i: 11999] avg mini-batch loss: 3.654\n",
      "[epoch: 76, i:   999] avg mini-batch loss: 3.669\n",
      "[epoch: 76, i:  1999] avg mini-batch loss: 3.757\n",
      "[epoch: 76, i:  2999] avg mini-batch loss: 3.758\n",
      "[epoch: 76, i:  3999] avg mini-batch loss: 3.694\n",
      "[epoch: 76, i:  4999] avg mini-batch loss: 3.665\n",
      "[epoch: 76, i:  5999] avg mini-batch loss: 3.679\n",
      "[epoch: 76, i:  6999] avg mini-batch loss: 3.669\n",
      "[epoch: 76, i:  7999] avg mini-batch loss: 3.694\n",
      "[epoch: 76, i:  8999] avg mini-batch loss: 3.754\n",
      "[epoch: 76, i:  9999] avg mini-batch loss: 3.802\n",
      "[epoch: 76, i: 10999] avg mini-batch loss: 3.653\n",
      "[epoch: 76, i: 11999] avg mini-batch loss: 3.669\n",
      "[epoch: 77, i:   999] avg mini-batch loss: 3.619\n",
      "[epoch: 77, i:  1999] avg mini-batch loss: 3.821\n",
      "[epoch: 77, i:  2999] avg mini-batch loss: 3.719\n",
      "[epoch: 77, i:  3999] avg mini-batch loss: 3.687\n",
      "[epoch: 77, i:  4999] avg mini-batch loss: 3.694\n",
      "[epoch: 77, i:  5999] avg mini-batch loss: 3.757\n",
      "[epoch: 77, i:  6999] avg mini-batch loss: 3.780\n",
      "[epoch: 77, i:  7999] avg mini-batch loss: 3.745\n",
      "[epoch: 77, i:  8999] avg mini-batch loss: 3.751\n",
      "[epoch: 77, i:  9999] avg mini-batch loss: 3.681\n",
      "[epoch: 77, i: 10999] avg mini-batch loss: 3.715\n",
      "[epoch: 77, i: 11999] avg mini-batch loss: 3.734\n",
      "[epoch: 78, i:   999] avg mini-batch loss: 3.738\n",
      "[epoch: 78, i:  1999] avg mini-batch loss: 3.674\n",
      "[epoch: 78, i:  2999] avg mini-batch loss: 3.782\n",
      "[epoch: 78, i:  3999] avg mini-batch loss: 3.750\n",
      "[epoch: 78, i:  4999] avg mini-batch loss: 3.765\n",
      "[epoch: 78, i:  5999] avg mini-batch loss: 3.744\n",
      "[epoch: 78, i:  6999] avg mini-batch loss: 3.704\n",
      "[epoch: 78, i:  7999] avg mini-batch loss: 3.804\n",
      "[epoch: 78, i:  8999] avg mini-batch loss: 3.906\n",
      "[epoch: 78, i:  9999] avg mini-batch loss: 3.755\n",
      "[epoch: 78, i: 10999] avg mini-batch loss: 3.792\n",
      "[epoch: 78, i: 11999] avg mini-batch loss: 3.813\n",
      "[epoch: 79, i:   999] avg mini-batch loss: 3.683\n",
      "[epoch: 79, i:  1999] avg mini-batch loss: 3.716\n",
      "[epoch: 79, i:  2999] avg mini-batch loss: 3.689\n",
      "[epoch: 79, i:  3999] avg mini-batch loss: 3.810\n",
      "[epoch: 79, i:  4999] avg mini-batch loss: 3.700\n",
      "[epoch: 79, i:  5999] avg mini-batch loss: 3.768\n",
      "[epoch: 79, i:  6999] avg mini-batch loss: 3.792\n",
      "[epoch: 79, i:  7999] avg mini-batch loss: 3.736\n",
      "[epoch: 79, i:  8999] avg mini-batch loss: 3.785\n",
      "[epoch: 79, i:  9999] avg mini-batch loss: 3.787\n",
      "[epoch: 79, i: 10999] avg mini-batch loss: 3.881\n",
      "[epoch: 79, i: 11999] avg mini-batch loss: 3.825\n",
      "[epoch: 80, i:   999] avg mini-batch loss: 3.666\n",
      "[epoch: 80, i:  1999] avg mini-batch loss: 3.735\n",
      "[epoch: 80, i:  2999] avg mini-batch loss: 3.825\n",
      "[epoch: 80, i:  3999] avg mini-batch loss: 3.796\n",
      "[epoch: 80, i:  4999] avg mini-batch loss: 3.809\n",
      "[epoch: 80, i:  5999] avg mini-batch loss: 3.751\n",
      "[epoch: 80, i:  6999] avg mini-batch loss: 3.810\n",
      "[epoch: 80, i:  7999] avg mini-batch loss: 3.886\n",
      "[epoch: 80, i:  8999] avg mini-batch loss: 3.741\n",
      "[epoch: 80, i:  9999] avg mini-batch loss: 3.700\n",
      "[epoch: 80, i: 10999] avg mini-batch loss: 3.721\n",
      "[epoch: 80, i: 11999] avg mini-batch loss: 3.767\n",
      "[epoch: 81, i:   999] avg mini-batch loss: 3.749\n",
      "[epoch: 81, i:  1999] avg mini-batch loss: 3.752\n",
      "[epoch: 81, i:  2999] avg mini-batch loss: 3.859\n",
      "[epoch: 81, i:  3999] avg mini-batch loss: 3.874\n",
      "[epoch: 81, i:  4999] avg mini-batch loss: 3.787\n",
      "[epoch: 81, i:  5999] avg mini-batch loss: 3.775\n",
      "[epoch: 81, i:  6999] avg mini-batch loss: 3.889\n",
      "[epoch: 81, i:  7999] avg mini-batch loss: 3.780\n",
      "[epoch: 81, i:  8999] avg mini-batch loss: 3.727\n",
      "[epoch: 81, i:  9999] avg mini-batch loss: 3.769\n",
      "[epoch: 81, i: 10999] avg mini-batch loss: 3.794\n",
      "[epoch: 81, i: 11999] avg mini-batch loss: 3.784\n",
      "[epoch: 82, i:   999] avg mini-batch loss: 3.702\n",
      "[epoch: 82, i:  1999] avg mini-batch loss: 3.883\n",
      "[epoch: 82, i:  2999] avg mini-batch loss: 3.853\n",
      "[epoch: 82, i:  3999] avg mini-batch loss: 3.864\n",
      "[epoch: 82, i:  4999] avg mini-batch loss: 3.729\n",
      "[epoch: 82, i:  5999] avg mini-batch loss: 3.782\n",
      "[epoch: 82, i:  6999] avg mini-batch loss: 3.849\n",
      "[epoch: 82, i:  7999] avg mini-batch loss: 3.795\n",
      "[epoch: 82, i:  8999] avg mini-batch loss: 3.823\n",
      "[epoch: 82, i:  9999] avg mini-batch loss: 3.840\n",
      "[epoch: 82, i: 10999] avg mini-batch loss: 3.887\n",
      "[epoch: 82, i: 11999] avg mini-batch loss: 3.736\n",
      "[epoch: 83, i:   999] avg mini-batch loss: 3.804\n",
      "[epoch: 83, i:  1999] avg mini-batch loss: 3.693\n",
      "[epoch: 83, i:  2999] avg mini-batch loss: 3.856\n",
      "[epoch: 83, i:  3999] avg mini-batch loss: 3.776\n",
      "[epoch: 83, i:  4999] avg mini-batch loss: 3.821\n",
      "[epoch: 83, i:  5999] avg mini-batch loss: 3.769\n",
      "[epoch: 83, i:  6999] avg mini-batch loss: 3.782\n",
      "[epoch: 83, i:  7999] avg mini-batch loss: 3.814\n",
      "[epoch: 83, i:  8999] avg mini-batch loss: 3.836\n",
      "[epoch: 83, i:  9999] avg mini-batch loss: 3.793\n",
      "[epoch: 83, i: 10999] avg mini-batch loss: 3.866\n",
      "[epoch: 83, i: 11999] avg mini-batch loss: 3.818\n",
      "[epoch: 84, i:   999] avg mini-batch loss: 3.788\n",
      "[epoch: 84, i:  1999] avg mini-batch loss: 3.875\n",
      "[epoch: 84, i:  2999] avg mini-batch loss: 3.820\n",
      "[epoch: 84, i:  3999] avg mini-batch loss: 3.741\n",
      "[epoch: 84, i:  4999] avg mini-batch loss: 3.706\n",
      "[epoch: 84, i:  5999] avg mini-batch loss: 3.768\n",
      "[epoch: 84, i:  6999] avg mini-batch loss: 3.835\n",
      "[epoch: 84, i:  7999] avg mini-batch loss: 3.820\n",
      "[epoch: 84, i:  8999] avg mini-batch loss: 3.807\n",
      "[epoch: 84, i:  9999] avg mini-batch loss: 3.753\n",
      "[epoch: 84, i: 10999] avg mini-batch loss: 3.755\n",
      "[epoch: 84, i: 11999] avg mini-batch loss: 3.914\n",
      "[epoch: 85, i:   999] avg mini-batch loss: 3.768\n",
      "[epoch: 85, i:  1999] avg mini-batch loss: 3.772\n",
      "[epoch: 85, i:  2999] avg mini-batch loss: 3.801\n",
      "[epoch: 85, i:  3999] avg mini-batch loss: 3.819\n",
      "[epoch: 85, i:  4999] avg mini-batch loss: 3.864\n",
      "[epoch: 85, i:  5999] avg mini-batch loss: 3.736\n",
      "[epoch: 85, i:  6999] avg mini-batch loss: 3.783\n",
      "[epoch: 85, i:  7999] avg mini-batch loss: 3.822\n",
      "[epoch: 85, i:  8999] avg mini-batch loss: 3.795\n",
      "[epoch: 85, i:  9999] avg mini-batch loss: 3.763\n",
      "[epoch: 85, i: 10999] avg mini-batch loss: 3.827\n",
      "[epoch: 85, i: 11999] avg mini-batch loss: 3.828\n",
      "[epoch: 86, i:   999] avg mini-batch loss: 3.779\n",
      "[epoch: 86, i:  1999] avg mini-batch loss: 3.890\n",
      "[epoch: 86, i:  2999] avg mini-batch loss: 3.833\n",
      "[epoch: 86, i:  3999] avg mini-batch loss: 3.795\n",
      "[epoch: 86, i:  4999] avg mini-batch loss: 3.855\n",
      "[epoch: 86, i:  5999] avg mini-batch loss: 3.902\n",
      "[epoch: 86, i:  6999] avg mini-batch loss: 3.873\n",
      "[epoch: 86, i:  7999] avg mini-batch loss: 3.846\n",
      "[epoch: 86, i:  8999] avg mini-batch loss: 3.887\n",
      "[epoch: 86, i:  9999] avg mini-batch loss: 3.864\n",
      "[epoch: 86, i: 10999] avg mini-batch loss: 3.754\n",
      "[epoch: 86, i: 11999] avg mini-batch loss: 3.872\n",
      "[epoch: 87, i:   999] avg mini-batch loss: 3.853\n",
      "[epoch: 87, i:  1999] avg mini-batch loss: 3.855\n",
      "[epoch: 87, i:  2999] avg mini-batch loss: 3.792\n",
      "[epoch: 87, i:  3999] avg mini-batch loss: 3.854\n",
      "[epoch: 87, i:  4999] avg mini-batch loss: 3.893\n",
      "[epoch: 87, i:  5999] avg mini-batch loss: 3.805\n",
      "[epoch: 87, i:  6999] avg mini-batch loss: 3.840\n",
      "[epoch: 87, i:  7999] avg mini-batch loss: 4.032\n",
      "[epoch: 87, i:  8999] avg mini-batch loss: 3.878\n",
      "[epoch: 87, i:  9999] avg mini-batch loss: 3.809\n",
      "[epoch: 87, i: 10999] avg mini-batch loss: 3.836\n",
      "[epoch: 87, i: 11999] avg mini-batch loss: 3.819\n",
      "[epoch: 88, i:   999] avg mini-batch loss: 3.892\n",
      "[epoch: 88, i:  1999] avg mini-batch loss: 3.844\n",
      "[epoch: 88, i:  2999] avg mini-batch loss: 3.859\n",
      "[epoch: 88, i:  3999] avg mini-batch loss: 3.893\n",
      "[epoch: 88, i:  4999] avg mini-batch loss: 3.822\n",
      "[epoch: 88, i:  5999] avg mini-batch loss: 3.826\n",
      "[epoch: 88, i:  6999] avg mini-batch loss: 3.842\n",
      "[epoch: 88, i:  7999] avg mini-batch loss: 3.835\n",
      "[epoch: 88, i:  8999] avg mini-batch loss: 3.819\n",
      "[epoch: 88, i:  9999] avg mini-batch loss: 3.783\n",
      "[epoch: 88, i: 10999] avg mini-batch loss: 3.925\n",
      "[epoch: 88, i: 11999] avg mini-batch loss: 3.852\n",
      "[epoch: 89, i:   999] avg mini-batch loss: 3.894\n",
      "[epoch: 89, i:  1999] avg mini-batch loss: 3.905\n",
      "[epoch: 89, i:  2999] avg mini-batch loss: 3.879\n",
      "[epoch: 89, i:  3999] avg mini-batch loss: 3.875\n",
      "[epoch: 89, i:  4999] avg mini-batch loss: 3.852\n",
      "[epoch: 89, i:  5999] avg mini-batch loss: 3.854\n",
      "[epoch: 89, i:  6999] avg mini-batch loss: 3.859\n",
      "[epoch: 89, i:  7999] avg mini-batch loss: 3.970\n",
      "[epoch: 89, i:  8999] avg mini-batch loss: 3.882\n",
      "[epoch: 89, i:  9999] avg mini-batch loss: 3.916\n",
      "[epoch: 89, i: 10999] avg mini-batch loss: 3.938\n",
      "[epoch: 89, i: 11999] avg mini-batch loss: 3.909\n",
      "[epoch: 90, i:   999] avg mini-batch loss: 3.912\n",
      "[epoch: 90, i:  1999] avg mini-batch loss: 3.985\n",
      "[epoch: 90, i:  2999] avg mini-batch loss: 3.855\n",
      "[epoch: 90, i:  3999] avg mini-batch loss: 3.907\n",
      "[epoch: 90, i:  4999] avg mini-batch loss: 3.890\n",
      "[epoch: 90, i:  5999] avg mini-batch loss: 3.839\n",
      "[epoch: 90, i:  6999] avg mini-batch loss: 3.952\n",
      "[epoch: 90, i:  7999] avg mini-batch loss: 3.836\n",
      "[epoch: 90, i:  8999] avg mini-batch loss: 3.868\n",
      "[epoch: 90, i:  9999] avg mini-batch loss: 3.918\n",
      "[epoch: 90, i: 10999] avg mini-batch loss: 3.936\n",
      "[epoch: 90, i: 11999] avg mini-batch loss: 3.961\n",
      "[epoch: 91, i:   999] avg mini-batch loss: 3.857\n",
      "[epoch: 91, i:  1999] avg mini-batch loss: 3.934\n",
      "[epoch: 91, i:  2999] avg mini-batch loss: 3.899\n",
      "[epoch: 91, i:  3999] avg mini-batch loss: 3.884\n",
      "[epoch: 91, i:  4999] avg mini-batch loss: 3.826\n",
      "[epoch: 91, i:  5999] avg mini-batch loss: 3.810\n",
      "[epoch: 91, i:  6999] avg mini-batch loss: 3.954\n",
      "[epoch: 91, i:  7999] avg mini-batch loss: 4.008\n",
      "[epoch: 91, i:  8999] avg mini-batch loss: 3.980\n",
      "[epoch: 91, i:  9999] avg mini-batch loss: 3.977\n",
      "[epoch: 91, i: 10999] avg mini-batch loss: 3.926\n",
      "[epoch: 91, i: 11999] avg mini-batch loss: 3.877\n",
      "[epoch: 92, i:   999] avg mini-batch loss: 3.866\n",
      "[epoch: 92, i:  1999] avg mini-batch loss: 3.847\n",
      "[epoch: 92, i:  2999] avg mini-batch loss: 3.836\n",
      "[epoch: 92, i:  3999] avg mini-batch loss: 3.884\n",
      "[epoch: 92, i:  4999] avg mini-batch loss: 3.777\n",
      "[epoch: 92, i:  5999] avg mini-batch loss: 3.903\n",
      "[epoch: 92, i:  6999] avg mini-batch loss: 3.896\n",
      "[epoch: 92, i:  7999] avg mini-batch loss: 3.902\n",
      "[epoch: 92, i:  8999] avg mini-batch loss: 3.937\n",
      "[epoch: 92, i:  9999] avg mini-batch loss: 3.911\n",
      "[epoch: 92, i: 10999] avg mini-batch loss: 3.877\n",
      "[epoch: 92, i: 11999] avg mini-batch loss: 3.862\n",
      "[epoch: 93, i:   999] avg mini-batch loss: 3.859\n",
      "[epoch: 93, i:  1999] avg mini-batch loss: 3.848\n",
      "[epoch: 93, i:  2999] avg mini-batch loss: 3.819\n",
      "[epoch: 93, i:  3999] avg mini-batch loss: 3.798\n",
      "[epoch: 93, i:  4999] avg mini-batch loss: 3.882\n",
      "[epoch: 93, i:  5999] avg mini-batch loss: 3.867\n",
      "[epoch: 93, i:  6999] avg mini-batch loss: 3.830\n",
      "[epoch: 93, i:  7999] avg mini-batch loss: 3.771\n",
      "[epoch: 93, i:  8999] avg mini-batch loss: 3.879\n",
      "[epoch: 93, i:  9999] avg mini-batch loss: 3.835\n",
      "[epoch: 93, i: 10999] avg mini-batch loss: 3.860\n",
      "[epoch: 93, i: 11999] avg mini-batch loss: 3.889\n",
      "[epoch: 94, i:   999] avg mini-batch loss: 3.831\n",
      "[epoch: 94, i:  1999] avg mini-batch loss: 3.869\n",
      "[epoch: 94, i:  2999] avg mini-batch loss: 3.851\n",
      "[epoch: 94, i:  3999] avg mini-batch loss: 3.879\n",
      "[epoch: 94, i:  4999] avg mini-batch loss: 3.944\n",
      "[epoch: 94, i:  5999] avg mini-batch loss: 3.918\n",
      "[epoch: 94, i:  6999] avg mini-batch loss: 3.903\n",
      "[epoch: 94, i:  7999] avg mini-batch loss: 3.909\n",
      "[epoch: 94, i:  8999] avg mini-batch loss: 4.015\n",
      "[epoch: 94, i:  9999] avg mini-batch loss: 3.951\n",
      "[epoch: 94, i: 10999] avg mini-batch loss: 4.074\n",
      "[epoch: 94, i: 11999] avg mini-batch loss: 3.956\n",
      "[epoch: 95, i:   999] avg mini-batch loss: 3.817\n",
      "[epoch: 95, i:  1999] avg mini-batch loss: 3.842\n",
      "[epoch: 95, i:  2999] avg mini-batch loss: 3.914\n",
      "[epoch: 95, i:  3999] avg mini-batch loss: 3.889\n",
      "[epoch: 95, i:  4999] avg mini-batch loss: 3.901\n",
      "[epoch: 95, i:  5999] avg mini-batch loss: 3.922\n",
      "[epoch: 95, i:  6999] avg mini-batch loss: 3.842\n",
      "[epoch: 95, i:  7999] avg mini-batch loss: 3.910\n",
      "[epoch: 95, i:  8999] avg mini-batch loss: 3.933\n",
      "[epoch: 95, i:  9999] avg mini-batch loss: 3.913\n",
      "[epoch: 95, i: 10999] avg mini-batch loss: 3.897\n",
      "[epoch: 95, i: 11999] avg mini-batch loss: 3.921\n",
      "[epoch: 96, i:   999] avg mini-batch loss: 3.990\n",
      "[epoch: 96, i:  1999] avg mini-batch loss: 3.865\n",
      "[epoch: 96, i:  2999] avg mini-batch loss: 3.911\n",
      "[epoch: 96, i:  3999] avg mini-batch loss: 3.860\n",
      "[epoch: 96, i:  4999] avg mini-batch loss: 3.912\n",
      "[epoch: 96, i:  5999] avg mini-batch loss: 3.949\n",
      "[epoch: 96, i:  6999] avg mini-batch loss: 3.919\n",
      "[epoch: 96, i:  7999] avg mini-batch loss: 3.897\n",
      "[epoch: 96, i:  8999] avg mini-batch loss: 3.929\n",
      "[epoch: 96, i:  9999] avg mini-batch loss: 4.037\n",
      "[epoch: 96, i: 10999] avg mini-batch loss: 3.935\n",
      "[epoch: 96, i: 11999] avg mini-batch loss: 3.866\n",
      "[epoch: 97, i:   999] avg mini-batch loss: 3.867\n",
      "[epoch: 97, i:  1999] avg mini-batch loss: 3.901\n",
      "[epoch: 97, i:  2999] avg mini-batch loss: 3.898\n",
      "[epoch: 97, i:  3999] avg mini-batch loss: 3.826\n",
      "[epoch: 97, i:  4999] avg mini-batch loss: 3.865\n",
      "[epoch: 97, i:  5999] avg mini-batch loss: 3.923\n",
      "[epoch: 97, i:  6999] avg mini-batch loss: 3.890\n",
      "[epoch: 97, i:  7999] avg mini-batch loss: 3.939\n",
      "[epoch: 97, i:  8999] avg mini-batch loss: 3.919\n",
      "[epoch: 97, i:  9999] avg mini-batch loss: 3.860\n",
      "[epoch: 97, i: 10999] avg mini-batch loss: 3.902\n",
      "[epoch: 97, i: 11999] avg mini-batch loss: 3.975\n",
      "[epoch: 98, i:   999] avg mini-batch loss: 3.872\n",
      "[epoch: 98, i:  1999] avg mini-batch loss: 3.835\n",
      "[epoch: 98, i:  2999] avg mini-batch loss: 3.957\n",
      "[epoch: 98, i:  3999] avg mini-batch loss: 3.873\n",
      "[epoch: 98, i:  4999] avg mini-batch loss: 3.881\n",
      "[epoch: 98, i:  5999] avg mini-batch loss: 3.902\n",
      "[epoch: 98, i:  6999] avg mini-batch loss: 3.909\n",
      "[epoch: 98, i:  7999] avg mini-batch loss: 3.940\n",
      "[epoch: 98, i:  8999] avg mini-batch loss: 3.909\n",
      "[epoch: 98, i:  9999] avg mini-batch loss: 3.920\n",
      "[epoch: 98, i: 10999] avg mini-batch loss: 3.896\n",
      "[epoch: 98, i: 11999] avg mini-batch loss: 3.938\n",
      "[epoch: 99, i:   999] avg mini-batch loss: 3.790\n",
      "[epoch: 99, i:  1999] avg mini-batch loss: 3.806\n",
      "[epoch: 99, i:  2999] avg mini-batch loss: 3.910\n",
      "[epoch: 99, i:  3999] avg mini-batch loss: 3.819\n",
      "[epoch: 99, i:  4999] avg mini-batch loss: 3.994\n",
      "[epoch: 99, i:  5999] avg mini-batch loss: 3.953\n",
      "[epoch: 99, i:  6999] avg mini-batch loss: 3.847\n",
      "[epoch: 99, i:  7999] avg mini-batch loss: 3.935\n",
      "[epoch: 99, i:  8999] avg mini-batch loss: 3.909\n",
      "[epoch: 99, i:  9999] avg mini-batch loss: 3.979\n",
      "[epoch: 99, i: 10999] avg mini-batch loss: 3.973\n",
      "[epoch: 99, i: 11999] avg mini-batch loss: 3.883\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 100       # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e936ee",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ef7844e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed09a649",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1f3a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21ad7c",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0fdce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 20 %\n",
      "Accuracy of aquarium_fish : 16 %\n",
      "Accuracy of  baby :  4 %\n",
      "Accuracy of  bear :  9 %\n",
      "Accuracy of beaver :  5 %\n",
      "Accuracy of   bed :  8 %\n",
      "Accuracy of   bee :  9 %\n",
      "Accuracy of beetle : 10 %\n",
      "Accuracy of bicycle :  8 %\n",
      "Accuracy of bottle : 10 %\n",
      "Accuracy of  bowl :  1 %\n",
      "Accuracy of   boy :  4 %\n",
      "Accuracy of bridge :  2 %\n",
      "Accuracy of   bus :  7 %\n",
      "Accuracy of butterfly :  2 %\n",
      "Accuracy of camel :  1 %\n",
      "Accuracy of   can :  6 %\n",
      "Accuracy of castle : 20 %\n",
      "Accuracy of caterpillar :  9 %\n",
      "Accuracy of cattle :  2 %\n",
      "Accuracy of chair : 22 %\n",
      "Accuracy of chimpanzee :  9 %\n",
      "Accuracy of clock :  2 %\n",
      "Accuracy of cloud : 12 %\n",
      "Accuracy of cockroach : 12 %\n",
      "Accuracy of couch :  1 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  5 %\n",
      "Accuracy of   cup :  4 %\n",
      "Accuracy of dinosaur :  5 %\n",
      "Accuracy of dolphin : 13 %\n",
      "Accuracy of elephant :  8 %\n",
      "Accuracy of flatfish :  2 %\n",
      "Accuracy of forest : 15 %\n",
      "Accuracy of   fox :  3 %\n",
      "Accuracy of  girl :  5 %\n",
      "Accuracy of hamster :  4 %\n",
      "Accuracy of house :  8 %\n",
      "Accuracy of kangaroo :  5 %\n",
      "Accuracy of keyboard : 11 %\n",
      "Accuracy of  lamp :  3 %\n",
      "Accuracy of lawn_mower : 13 %\n",
      "Accuracy of leopard :  8 %\n",
      "Accuracy of  lion : 28 %\n",
      "Accuracy of lizard :  2 %\n",
      "Accuracy of lobster :  2 %\n",
      "Accuracy of   man :  1 %\n",
      "Accuracy of maple_tree :  9 %\n",
      "Accuracy of motorcycle : 13 %\n",
      "Accuracy of mountain : 18 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  1 %\n",
      "Accuracy of oak_tree : 33 %\n",
      "Accuracy of orange : 26 %\n",
      "Accuracy of orchid : 16 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  6 %\n",
      "Accuracy of  pear :  5 %\n",
      "Accuracy of pickup_truck : 12 %\n",
      "Accuracy of pine_tree : 16 %\n",
      "Accuracy of plain : 31 %\n",
      "Accuracy of plate : 11 %\n",
      "Accuracy of poppy : 20 %\n",
      "Accuracy of porcupine : 11 %\n",
      "Accuracy of possum :  5 %\n",
      "Accuracy of rabbit :  2 %\n",
      "Accuracy of raccoon : 17 %\n",
      "Accuracy of   ray : 19 %\n",
      "Accuracy of  road : 26 %\n",
      "Accuracy of rocket : 13 %\n",
      "Accuracy of  rose : 29 %\n",
      "Accuracy of   sea : 16 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark : 17 %\n",
      "Accuracy of shrew :  4 %\n",
      "Accuracy of skunk : 21 %\n",
      "Accuracy of skyscraper : 21 %\n",
      "Accuracy of snail :  3 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  4 %\n",
      "Accuracy of squirrel :  6 %\n",
      "Accuracy of streetcar :  4 %\n",
      "Accuracy of sunflower : 47 %\n",
      "Accuracy of sweet_pepper :  9 %\n",
      "Accuracy of table :  1 %\n",
      "Accuracy of  tank :  7 %\n",
      "Accuracy of telephone :  8 %\n",
      "Accuracy of television : 12 %\n",
      "Accuracy of tiger :  5 %\n",
      "Accuracy of tractor :  5 %\n",
      "Accuracy of train :  4 %\n",
      "Accuracy of trout :  5 %\n",
      "Accuracy of tulip :  8 %\n",
      "Accuracy of turtle :  1 %\n",
      "Accuracy of wardrobe : 30 %\n",
      "Accuracy of whale : 30 %\n",
      "Accuracy of willow_tree :  7 %\n",
      "Accuracy of  wolf : 13 %\n",
      "Accuracy of woman :  2 %\n",
      "Accuracy of  worm :  2 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
