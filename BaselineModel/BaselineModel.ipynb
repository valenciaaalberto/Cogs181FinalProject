{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79d7a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da4c56",
   "metadata": {},
   "source": [
    "## Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6117c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = [\n",
    "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n",
    "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n",
    "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n",
    "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n",
    "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n",
    "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
    "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
    "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
    "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', \n",
    "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', \n",
    "    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', \n",
    "    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', \n",
    "    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', \n",
    "    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', \n",
    "    'worm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b170b",
   "metadata": {},
   "source": [
    "## Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b574a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336a6dc",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc14492e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv_layer_one): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_two): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_three): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_four): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_five): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv_layer_six): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (batchNorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv_layer_one = nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3 , stride=1, padding=1)\n",
    "        self.conv_layer_two = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_three = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_four = nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_five = nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_layer_six = nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "        self.batchNorm = nn.BatchNorm2d(3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 100)\n",
    "  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.conv_layer_one(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_two(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_three(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_four(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_five(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.avg_pool2d(x,(2,2))\n",
    "        x = self.conv_layer_six(x)\n",
    "        x = F.elu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.elu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()     # Create the network instance.\n",
    "net.to(device)  # Move the network parameters to the specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904d075",
   "metadata": {},
   "source": [
    "## Select Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247b7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()  \n",
    "opt = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b857ce00",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "211ff432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0, i:   999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  1999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  2999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  3999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  4999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  5999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  6999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  7999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i:  8999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i:  9999] avg mini-batch loss: 4.607\n",
      "[epoch: 0, i: 10999] avg mini-batch loss: 4.606\n",
      "[epoch: 0, i: 11999] avg mini-batch loss: 4.605\n",
      "[epoch: 1, i:   999] avg mini-batch loss: 4.601\n",
      "[epoch: 1, i:  1999] avg mini-batch loss: 4.557\n",
      "[epoch: 1, i:  2999] avg mini-batch loss: 4.464\n",
      "[epoch: 1, i:  3999] avg mini-batch loss: 4.445\n",
      "[epoch: 1, i:  4999] avg mini-batch loss: 4.385\n",
      "[epoch: 1, i:  5999] avg mini-batch loss: 4.341\n",
      "[epoch: 1, i:  6999] avg mini-batch loss: 4.314\n",
      "[epoch: 1, i:  7999] avg mini-batch loss: 4.288\n",
      "[epoch: 1, i:  8999] avg mini-batch loss: 4.271\n",
      "[epoch: 1, i:  9999] avg mini-batch loss: 4.243\n",
      "[epoch: 1, i: 10999] avg mini-batch loss: 4.222\n",
      "[epoch: 1, i: 11999] avg mini-batch loss: 4.152\n",
      "[epoch: 2, i:   999] avg mini-batch loss: 4.098\n",
      "[epoch: 2, i:  1999] avg mini-batch loss: 4.088\n",
      "[epoch: 2, i:  2999] avg mini-batch loss: 4.056\n",
      "[epoch: 2, i:  3999] avg mini-batch loss: 4.020\n",
      "[epoch: 2, i:  4999] avg mini-batch loss: 3.987\n",
      "[epoch: 2, i:  5999] avg mini-batch loss: 3.938\n",
      "[epoch: 2, i:  6999] avg mini-batch loss: 3.949\n",
      "[epoch: 2, i:  7999] avg mini-batch loss: 3.947\n",
      "[epoch: 2, i:  8999] avg mini-batch loss: 3.864\n",
      "[epoch: 2, i:  9999] avg mini-batch loss: 3.878\n",
      "[epoch: 2, i: 10999] avg mini-batch loss: 3.798\n",
      "[epoch: 2, i: 11999] avg mini-batch loss: 3.784\n",
      "[epoch: 3, i:   999] avg mini-batch loss: 3.693\n",
      "[epoch: 3, i:  1999] avg mini-batch loss: 3.700\n",
      "[epoch: 3, i:  2999] avg mini-batch loss: 3.630\n",
      "[epoch: 3, i:  3999] avg mini-batch loss: 3.640\n",
      "[epoch: 3, i:  4999] avg mini-batch loss: 3.606\n",
      "[epoch: 3, i:  5999] avg mini-batch loss: 3.549\n",
      "[epoch: 3, i:  6999] avg mini-batch loss: 3.520\n",
      "[epoch: 3, i:  7999] avg mini-batch loss: 3.484\n",
      "[epoch: 3, i:  8999] avg mini-batch loss: 3.401\n",
      "[epoch: 3, i:  9999] avg mini-batch loss: 3.398\n",
      "[epoch: 3, i: 10999] avg mini-batch loss: 3.420\n",
      "[epoch: 3, i: 11999] avg mini-batch loss: 3.297\n",
      "[epoch: 4, i:   999] avg mini-batch loss: 3.264\n",
      "[epoch: 4, i:  1999] avg mini-batch loss: 3.228\n",
      "[epoch: 4, i:  2999] avg mini-batch loss: 3.192\n",
      "[epoch: 4, i:  3999] avg mini-batch loss: 3.151\n",
      "[epoch: 4, i:  4999] avg mini-batch loss: 3.169\n",
      "[epoch: 4, i:  5999] avg mini-batch loss: 3.120\n",
      "[epoch: 4, i:  6999] avg mini-batch loss: 3.143\n",
      "[epoch: 4, i:  7999] avg mini-batch loss: 3.056\n",
      "[epoch: 4, i:  8999] avg mini-batch loss: 3.008\n",
      "[epoch: 4, i:  9999] avg mini-batch loss: 3.032\n",
      "[epoch: 4, i: 10999] avg mini-batch loss: 2.963\n",
      "[epoch: 4, i: 11999] avg mini-batch loss: 2.915\n",
      "[epoch: 5, i:   999] avg mini-batch loss: 2.848\n",
      "[epoch: 5, i:  1999] avg mini-batch loss: 2.887\n",
      "[epoch: 5, i:  2999] avg mini-batch loss: 2.876\n",
      "[epoch: 5, i:  3999] avg mini-batch loss: 2.789\n",
      "[epoch: 5, i:  4999] avg mini-batch loss: 2.770\n",
      "[epoch: 5, i:  5999] avg mini-batch loss: 2.796\n",
      "[epoch: 5, i:  6999] avg mini-batch loss: 2.741\n",
      "[epoch: 5, i:  7999] avg mini-batch loss: 2.743\n",
      "[epoch: 5, i:  8999] avg mini-batch loss: 2.748\n",
      "[epoch: 5, i:  9999] avg mini-batch loss: 2.689\n",
      "[epoch: 5, i: 10999] avg mini-batch loss: 2.691\n",
      "[epoch: 5, i: 11999] avg mini-batch loss: 2.625\n",
      "[epoch: 6, i:   999] avg mini-batch loss: 2.570\n",
      "[epoch: 6, i:  1999] avg mini-batch loss: 2.583\n",
      "[epoch: 6, i:  2999] avg mini-batch loss: 2.537\n",
      "[epoch: 6, i:  3999] avg mini-batch loss: 2.559\n",
      "[epoch: 6, i:  4999] avg mini-batch loss: 2.556\n",
      "[epoch: 6, i:  5999] avg mini-batch loss: 2.479\n",
      "[epoch: 6, i:  6999] avg mini-batch loss: 2.522\n",
      "[epoch: 6, i:  7999] avg mini-batch loss: 2.528\n",
      "[epoch: 6, i:  8999] avg mini-batch loss: 2.484\n",
      "[epoch: 6, i:  9999] avg mini-batch loss: 2.495\n",
      "[epoch: 6, i: 10999] avg mini-batch loss: 2.425\n",
      "[epoch: 6, i: 11999] avg mini-batch loss: 2.439\n",
      "[epoch: 7, i:   999] avg mini-batch loss: 2.300\n",
      "[epoch: 7, i:  1999] avg mini-batch loss: 2.312\n",
      "[epoch: 7, i:  2999] avg mini-batch loss: 2.334\n",
      "[epoch: 7, i:  3999] avg mini-batch loss: 2.341\n",
      "[epoch: 7, i:  4999] avg mini-batch loss: 2.351\n",
      "[epoch: 7, i:  5999] avg mini-batch loss: 2.290\n",
      "[epoch: 7, i:  6999] avg mini-batch loss: 2.313\n",
      "[epoch: 7, i:  7999] avg mini-batch loss: 2.286\n",
      "[epoch: 7, i:  8999] avg mini-batch loss: 2.335\n",
      "[epoch: 7, i:  9999] avg mini-batch loss: 2.297\n",
      "[epoch: 7, i: 10999] avg mini-batch loss: 2.315\n",
      "[epoch: 7, i: 11999] avg mini-batch loss: 2.200\n",
      "[epoch: 8, i:   999] avg mini-batch loss: 2.115\n",
      "[epoch: 8, i:  1999] avg mini-batch loss: 2.132\n",
      "[epoch: 8, i:  2999] avg mini-batch loss: 2.082\n",
      "[epoch: 8, i:  3999] avg mini-batch loss: 2.112\n",
      "[epoch: 8, i:  4999] avg mini-batch loss: 2.172\n",
      "[epoch: 8, i:  5999] avg mini-batch loss: 2.124\n",
      "[epoch: 8, i:  6999] avg mini-batch loss: 2.097\n",
      "[epoch: 8, i:  7999] avg mini-batch loss: 2.146\n",
      "[epoch: 8, i:  8999] avg mini-batch loss: 2.130\n",
      "[epoch: 8, i:  9999] avg mini-batch loss: 2.124\n",
      "[epoch: 8, i: 10999] avg mini-batch loss: 2.106\n",
      "[epoch: 8, i: 11999] avg mini-batch loss: 2.093\n",
      "[epoch: 9, i:   999] avg mini-batch loss: 1.971\n",
      "[epoch: 9, i:  1999] avg mini-batch loss: 1.928\n",
      "[epoch: 9, i:  2999] avg mini-batch loss: 1.985\n",
      "[epoch: 9, i:  3999] avg mini-batch loss: 1.932\n",
      "[epoch: 9, i:  4999] avg mini-batch loss: 1.976\n",
      "[epoch: 9, i:  5999] avg mini-batch loss: 1.937\n",
      "[epoch: 9, i:  6999] avg mini-batch loss: 1.928\n",
      "[epoch: 9, i:  7999] avg mini-batch loss: 1.963\n",
      "[epoch: 9, i:  8999] avg mini-batch loss: 1.972\n",
      "[epoch: 9, i:  9999] avg mini-batch loss: 1.967\n",
      "[epoch: 9, i: 10999] avg mini-batch loss: 1.956\n",
      "[epoch: 9, i: 11999] avg mini-batch loss: 1.936\n",
      "[epoch: 10, i:   999] avg mini-batch loss: 1.812\n",
      "[epoch: 10, i:  1999] avg mini-batch loss: 1.733\n",
      "[epoch: 10, i:  2999] avg mini-batch loss: 1.844\n",
      "[epoch: 10, i:  3999] avg mini-batch loss: 1.802\n",
      "[epoch: 10, i:  4999] avg mini-batch loss: 1.784\n",
      "[epoch: 10, i:  5999] avg mini-batch loss: 1.781\n",
      "[epoch: 10, i:  6999] avg mini-batch loss: 1.809\n",
      "[epoch: 10, i:  7999] avg mini-batch loss: 1.820\n",
      "[epoch: 10, i:  8999] avg mini-batch loss: 1.794\n",
      "[epoch: 10, i:  9999] avg mini-batch loss: 1.801\n",
      "[epoch: 10, i: 10999] avg mini-batch loss: 1.841\n",
      "[epoch: 10, i: 11999] avg mini-batch loss: 1.816\n",
      "[epoch: 11, i:   999] avg mini-batch loss: 1.553\n",
      "[epoch: 11, i:  1999] avg mini-batch loss: 1.610\n",
      "[epoch: 11, i:  2999] avg mini-batch loss: 1.635\n",
      "[epoch: 11, i:  3999] avg mini-batch loss: 1.661\n",
      "[epoch: 11, i:  4999] avg mini-batch loss: 1.680\n",
      "[epoch: 11, i:  5999] avg mini-batch loss: 1.615\n",
      "[epoch: 11, i:  6999] avg mini-batch loss: 1.669\n",
      "[epoch: 11, i:  7999] avg mini-batch loss: 1.693\n",
      "[epoch: 11, i:  8999] avg mini-batch loss: 1.657\n",
      "[epoch: 11, i:  9999] avg mini-batch loss: 1.640\n",
      "[epoch: 11, i: 10999] avg mini-batch loss: 1.639\n",
      "[epoch: 11, i: 11999] avg mini-batch loss: 1.658\n",
      "[epoch: 12, i:   999] avg mini-batch loss: 1.381\n",
      "[epoch: 12, i:  1999] avg mini-batch loss: 1.469\n",
      "[epoch: 12, i:  2999] avg mini-batch loss: 1.484\n",
      "[epoch: 12, i:  3999] avg mini-batch loss: 1.467\n",
      "[epoch: 12, i:  4999] avg mini-batch loss: 1.469\n",
      "[epoch: 12, i:  5999] avg mini-batch loss: 1.507\n",
      "[epoch: 12, i:  6999] avg mini-batch loss: 1.529\n",
      "[epoch: 12, i:  7999] avg mini-batch loss: 1.538\n",
      "[epoch: 12, i:  8999] avg mini-batch loss: 1.494\n",
      "[epoch: 12, i:  9999] avg mini-batch loss: 1.523\n",
      "[epoch: 12, i: 10999] avg mini-batch loss: 1.584\n",
      "[epoch: 12, i: 11999] avg mini-batch loss: 1.556\n",
      "[epoch: 13, i:   999] avg mini-batch loss: 1.252\n",
      "[epoch: 13, i:  1999] avg mini-batch loss: 1.300\n",
      "[epoch: 13, i:  2999] avg mini-batch loss: 1.334\n",
      "[epoch: 13, i:  3999] avg mini-batch loss: 1.299\n",
      "[epoch: 13, i:  4999] avg mini-batch loss: 1.348\n",
      "[epoch: 13, i:  5999] avg mini-batch loss: 1.360\n",
      "[epoch: 13, i:  6999] avg mini-batch loss: 1.375\n",
      "[epoch: 13, i:  7999] avg mini-batch loss: 1.432\n",
      "[epoch: 13, i:  8999] avg mini-batch loss: 1.376\n",
      "[epoch: 13, i:  9999] avg mini-batch loss: 1.416\n",
      "[epoch: 13, i: 10999] avg mini-batch loss: 1.405\n",
      "[epoch: 13, i: 11999] avg mini-batch loss: 1.384\n",
      "[epoch: 14, i:   999] avg mini-batch loss: 1.077\n",
      "[epoch: 14, i:  1999] avg mini-batch loss: 1.136\n",
      "[epoch: 14, i:  2999] avg mini-batch loss: 1.159\n",
      "[epoch: 14, i:  3999] avg mini-batch loss: 1.183\n",
      "[epoch: 14, i:  4999] avg mini-batch loss: 1.186\n",
      "[epoch: 14, i:  5999] avg mini-batch loss: 1.220\n",
      "[epoch: 14, i:  6999] avg mini-batch loss: 1.264\n",
      "[epoch: 14, i:  7999] avg mini-batch loss: 1.258\n",
      "[epoch: 14, i:  8999] avg mini-batch loss: 1.277\n",
      "[epoch: 14, i:  9999] avg mini-batch loss: 1.286\n",
      "[epoch: 14, i: 10999] avg mini-batch loss: 1.292\n",
      "[epoch: 14, i: 11999] avg mini-batch loss: 1.269\n",
      "[epoch: 15, i:   999] avg mini-batch loss: 0.945\n",
      "[epoch: 15, i:  1999] avg mini-batch loss: 0.927\n",
      "[epoch: 15, i:  2999] avg mini-batch loss: 1.042\n",
      "[epoch: 15, i:  3999] avg mini-batch loss: 1.044\n",
      "[epoch: 15, i:  4999] avg mini-batch loss: 1.074\n",
      "[epoch: 15, i:  5999] avg mini-batch loss: 1.096\n",
      "[epoch: 15, i:  6999] avg mini-batch loss: 1.115\n",
      "[epoch: 15, i:  7999] avg mini-batch loss: 1.147\n",
      "[epoch: 15, i:  8999] avg mini-batch loss: 1.126\n",
      "[epoch: 15, i:  9999] avg mini-batch loss: 1.121\n",
      "[epoch: 15, i: 10999] avg mini-batch loss: 1.124\n",
      "[epoch: 15, i: 11999] avg mini-batch loss: 1.152\n",
      "[epoch: 16, i:   999] avg mini-batch loss: 0.791\n",
      "[epoch: 16, i:  1999] avg mini-batch loss: 0.848\n",
      "[epoch: 16, i:  2999] avg mini-batch loss: 0.932\n",
      "[epoch: 16, i:  3999] avg mini-batch loss: 0.951\n",
      "[epoch: 16, i:  4999] avg mini-batch loss: 0.959\n",
      "[epoch: 16, i:  5999] avg mini-batch loss: 0.949\n",
      "[epoch: 16, i:  6999] avg mini-batch loss: 0.994\n",
      "[epoch: 16, i:  7999] avg mini-batch loss: 0.998\n",
      "[epoch: 16, i:  8999] avg mini-batch loss: 1.057\n",
      "[epoch: 16, i:  9999] avg mini-batch loss: 1.031\n",
      "[epoch: 16, i: 10999] avg mini-batch loss: 1.006\n",
      "[epoch: 16, i: 11999] avg mini-batch loss: 1.024\n",
      "[epoch: 17, i:   999] avg mini-batch loss: 0.683\n",
      "[epoch: 17, i:  1999] avg mini-batch loss: 0.752\n",
      "[epoch: 17, i:  2999] avg mini-batch loss: 0.725\n",
      "[epoch: 17, i:  3999] avg mini-batch loss: 0.814\n",
      "[epoch: 17, i:  4999] avg mini-batch loss: 0.802\n",
      "[epoch: 17, i:  5999] avg mini-batch loss: 0.842\n",
      "[epoch: 17, i:  6999] avg mini-batch loss: 0.859\n",
      "[epoch: 17, i:  7999] avg mini-batch loss: 0.869\n",
      "[epoch: 17, i:  8999] avg mini-batch loss: 0.925\n",
      "[epoch: 17, i:  9999] avg mini-batch loss: 0.877\n",
      "[epoch: 17, i: 10999] avg mini-batch loss: 0.958\n",
      "[epoch: 17, i: 11999] avg mini-batch loss: 0.910\n",
      "[epoch: 18, i:   999] avg mini-batch loss: 0.587\n",
      "[epoch: 18, i:  1999] avg mini-batch loss: 0.638\n",
      "[epoch: 18, i:  2999] avg mini-batch loss: 0.644\n",
      "[epoch: 18, i:  3999] avg mini-batch loss: 0.683\n",
      "[epoch: 18, i:  4999] avg mini-batch loss: 0.743\n",
      "[epoch: 18, i:  5999] avg mini-batch loss: 0.741\n",
      "[epoch: 18, i:  6999] avg mini-batch loss: 0.784\n",
      "[epoch: 18, i:  7999] avg mini-batch loss: 0.726\n",
      "[epoch: 18, i:  8999] avg mini-batch loss: 0.815\n",
      "[epoch: 18, i:  9999] avg mini-batch loss: 0.765\n",
      "[epoch: 18, i: 10999] avg mini-batch loss: 0.836\n",
      "[epoch: 18, i: 11999] avg mini-batch loss: 0.835\n",
      "[epoch: 19, i:   999] avg mini-batch loss: 0.519\n",
      "[epoch: 19, i:  1999] avg mini-batch loss: 0.525\n",
      "[epoch: 19, i:  2999] avg mini-batch loss: 0.583\n",
      "[epoch: 19, i:  3999] avg mini-batch loss: 0.621\n",
      "[epoch: 19, i:  4999] avg mini-batch loss: 0.652\n",
      "[epoch: 19, i:  5999] avg mini-batch loss: 0.652\n",
      "[epoch: 19, i:  6999] avg mini-batch loss: 0.700\n",
      "[epoch: 19, i:  7999] avg mini-batch loss: 0.658\n",
      "[epoch: 19, i:  8999] avg mini-batch loss: 0.708\n",
      "[epoch: 19, i:  9999] avg mini-batch loss: 0.702\n",
      "[epoch: 19, i: 10999] avg mini-batch loss: 0.755\n",
      "[epoch: 19, i: 11999] avg mini-batch loss: 0.729\n",
      "[epoch: 20, i:   999] avg mini-batch loss: 0.444\n",
      "[epoch: 20, i:  1999] avg mini-batch loss: 0.463\n",
      "[epoch: 20, i:  2999] avg mini-batch loss: 0.521\n",
      "[epoch: 20, i:  3999] avg mini-batch loss: 0.529\n",
      "[epoch: 20, i:  4999] avg mini-batch loss: 0.545\n",
      "[epoch: 20, i:  5999] avg mini-batch loss: 0.566\n",
      "[epoch: 20, i:  6999] avg mini-batch loss: 0.559\n",
      "[epoch: 20, i:  7999] avg mini-batch loss: 0.526\n",
      "[epoch: 20, i:  8999] avg mini-batch loss: 0.668\n",
      "[epoch: 20, i:  9999] avg mini-batch loss: 0.647\n",
      "[epoch: 20, i: 10999] avg mini-batch loss: 0.623\n",
      "[epoch: 20, i: 11999] avg mini-batch loss: 0.684\n",
      "[epoch: 21, i:   999] avg mini-batch loss: 0.406\n",
      "[epoch: 21, i:  1999] avg mini-batch loss: 0.395\n",
      "[epoch: 21, i:  2999] avg mini-batch loss: 0.471\n",
      "[epoch: 21, i:  3999] avg mini-batch loss: 0.487\n",
      "[epoch: 21, i:  4999] avg mini-batch loss: 0.477\n",
      "[epoch: 21, i:  5999] avg mini-batch loss: 0.506\n",
      "[epoch: 21, i:  6999] avg mini-batch loss: 0.555\n",
      "[epoch: 21, i:  7999] avg mini-batch loss: 0.555\n",
      "[epoch: 21, i:  8999] avg mini-batch loss: 0.554\n",
      "[epoch: 21, i:  9999] avg mini-batch loss: 0.540\n",
      "[epoch: 21, i: 10999] avg mini-batch loss: 0.587\n",
      "[epoch: 21, i: 11999] avg mini-batch loss: 0.561\n",
      "[epoch: 22, i:   999] avg mini-batch loss: 0.342\n",
      "[epoch: 22, i:  1999] avg mini-batch loss: 0.385\n",
      "[epoch: 22, i:  2999] avg mini-batch loss: 0.398\n",
      "[epoch: 22, i:  3999] avg mini-batch loss: 0.398\n",
      "[epoch: 22, i:  4999] avg mini-batch loss: 0.449\n",
      "[epoch: 22, i:  5999] avg mini-batch loss: 0.470\n",
      "[epoch: 22, i:  6999] avg mini-batch loss: 0.460\n",
      "[epoch: 22, i:  7999] avg mini-batch loss: 0.510\n",
      "[epoch: 22, i:  8999] avg mini-batch loss: 0.480\n",
      "[epoch: 22, i:  9999] avg mini-batch loss: 0.502\n",
      "[epoch: 22, i: 10999] avg mini-batch loss: 0.539\n",
      "[epoch: 22, i: 11999] avg mini-batch loss: 0.517\n",
      "[epoch: 23, i:   999] avg mini-batch loss: 0.315\n",
      "[epoch: 23, i:  1999] avg mini-batch loss: 0.341\n",
      "[epoch: 23, i:  2999] avg mini-batch loss: 0.347\n",
      "[epoch: 23, i:  3999] avg mini-batch loss: 0.407\n",
      "[epoch: 23, i:  4999] avg mini-batch loss: 0.392\n",
      "[epoch: 23, i:  5999] avg mini-batch loss: 0.420\n",
      "[epoch: 23, i:  6999] avg mini-batch loss: 0.387\n",
      "[epoch: 23, i:  7999] avg mini-batch loss: 0.408\n",
      "[epoch: 23, i:  8999] avg mini-batch loss: 0.496\n",
      "[epoch: 23, i:  9999] avg mini-batch loss: 0.416\n",
      "[epoch: 23, i: 10999] avg mini-batch loss: 0.451\n",
      "[epoch: 23, i: 11999] avg mini-batch loss: 0.471\n",
      "[epoch: 24, i:   999] avg mini-batch loss: 0.297\n",
      "[epoch: 24, i:  1999] avg mini-batch loss: 0.323\n",
      "[epoch: 24, i:  2999] avg mini-batch loss: 0.320\n",
      "[epoch: 24, i:  3999] avg mini-batch loss: 0.343\n",
      "[epoch: 24, i:  4999] avg mini-batch loss: 0.342\n",
      "[epoch: 24, i:  5999] avg mini-batch loss: 0.381\n",
      "[epoch: 24, i:  6999] avg mini-batch loss: 0.371\n",
      "[epoch: 24, i:  7999] avg mini-batch loss: 0.413\n",
      "[epoch: 24, i:  8999] avg mini-batch loss: 0.409\n",
      "[epoch: 24, i:  9999] avg mini-batch loss: 0.447\n",
      "[epoch: 24, i: 10999] avg mini-batch loss: 0.406\n",
      "[epoch: 24, i: 11999] avg mini-batch loss: 0.438\n",
      "[epoch: 25, i:   999] avg mini-batch loss: 0.260\n",
      "[epoch: 25, i:  1999] avg mini-batch loss: 0.278\n",
      "[epoch: 25, i:  2999] avg mini-batch loss: 0.253\n",
      "[epoch: 25, i:  3999] avg mini-batch loss: 0.319\n",
      "[epoch: 25, i:  4999] avg mini-batch loss: 0.332\n",
      "[epoch: 25, i:  5999] avg mini-batch loss: 0.311\n",
      "[epoch: 25, i:  6999] avg mini-batch loss: 0.355\n",
      "[epoch: 25, i:  7999] avg mini-batch loss: 0.416\n",
      "[epoch: 25, i:  8999] avg mini-batch loss: 0.373\n",
      "[epoch: 25, i:  9999] avg mini-batch loss: 0.387\n",
      "[epoch: 25, i: 10999] avg mini-batch loss: 0.417\n",
      "[epoch: 25, i: 11999] avg mini-batch loss: 0.435\n",
      "[epoch: 26, i:   999] avg mini-batch loss: 0.241\n",
      "[epoch: 26, i:  1999] avg mini-batch loss: 0.229\n",
      "[epoch: 26, i:  2999] avg mini-batch loss: 0.315\n",
      "[epoch: 26, i:  3999] avg mini-batch loss: 0.297\n",
      "[epoch: 26, i:  4999] avg mini-batch loss: 0.345\n",
      "[epoch: 26, i:  5999] avg mini-batch loss: 0.307\n",
      "[epoch: 26, i:  6999] avg mini-batch loss: 0.287\n",
      "[epoch: 26, i:  7999] avg mini-batch loss: 0.325\n",
      "[epoch: 26, i:  8999] avg mini-batch loss: 0.299\n",
      "[epoch: 26, i:  9999] avg mini-batch loss: 0.344\n",
      "[epoch: 26, i: 10999] avg mini-batch loss: 0.353\n",
      "[epoch: 26, i: 11999] avg mini-batch loss: 0.348\n",
      "[epoch: 27, i:   999] avg mini-batch loss: 0.200\n",
      "[epoch: 27, i:  1999] avg mini-batch loss: 0.222\n",
      "[epoch: 27, i:  2999] avg mini-batch loss: 0.251\n",
      "[epoch: 27, i:  3999] avg mini-batch loss: 0.257\n",
      "[epoch: 27, i:  4999] avg mini-batch loss: 0.236\n",
      "[epoch: 27, i:  5999] avg mini-batch loss: 0.292\n",
      "[epoch: 27, i:  6999] avg mini-batch loss: 0.325\n",
      "[epoch: 27, i:  7999] avg mini-batch loss: 0.306\n",
      "[epoch: 27, i:  8999] avg mini-batch loss: 0.351\n",
      "[epoch: 27, i:  9999] avg mini-batch loss: 0.318\n",
      "[epoch: 27, i: 10999] avg mini-batch loss: 0.321\n",
      "[epoch: 27, i: 11999] avg mini-batch loss: 0.345\n",
      "[epoch: 28, i:   999] avg mini-batch loss: 0.206\n",
      "[epoch: 28, i:  1999] avg mini-batch loss: 0.194\n",
      "[epoch: 28, i:  2999] avg mini-batch loss: 0.234\n",
      "[epoch: 28, i:  3999] avg mini-batch loss: 0.223\n",
      "[epoch: 28, i:  4999] avg mini-batch loss: 0.296\n",
      "[epoch: 28, i:  5999] avg mini-batch loss: 0.321\n",
      "[epoch: 28, i:  6999] avg mini-batch loss: 0.254\n",
      "[epoch: 28, i:  7999] avg mini-batch loss: 0.261\n",
      "[epoch: 28, i:  8999] avg mini-batch loss: 0.276\n",
      "[epoch: 28, i:  9999] avg mini-batch loss: 0.325\n",
      "[epoch: 28, i: 10999] avg mini-batch loss: 0.320\n",
      "[epoch: 28, i: 11999] avg mini-batch loss: 0.339\n",
      "[epoch: 29, i:   999] avg mini-batch loss: 0.216\n",
      "[epoch: 29, i:  1999] avg mini-batch loss: 0.219\n",
      "[epoch: 29, i:  2999] avg mini-batch loss: 0.238\n",
      "[epoch: 29, i:  3999] avg mini-batch loss: 0.243\n",
      "[epoch: 29, i:  4999] avg mini-batch loss: 0.269\n",
      "[epoch: 29, i:  5999] avg mini-batch loss: 0.273\n",
      "[epoch: 29, i:  6999] avg mini-batch loss: 0.252\n",
      "[epoch: 29, i:  7999] avg mini-batch loss: 0.299\n",
      "[epoch: 29, i:  8999] avg mini-batch loss: 0.267\n",
      "[epoch: 29, i:  9999] avg mini-batch loss: 0.248\n",
      "[epoch: 29, i: 10999] avg mini-batch loss: 0.277\n",
      "[epoch: 29, i: 11999] avg mini-batch loss: 0.276\n",
      "[epoch: 30, i:   999] avg mini-batch loss: 0.170\n",
      "[epoch: 30, i:  1999] avg mini-batch loss: 0.188\n",
      "[epoch: 30, i:  2999] avg mini-batch loss: 0.197\n",
      "[epoch: 30, i:  3999] avg mini-batch loss: 0.206\n",
      "[epoch: 30, i:  4999] avg mini-batch loss: 0.205\n",
      "[epoch: 30, i:  5999] avg mini-batch loss: 0.222\n",
      "[epoch: 30, i:  6999] avg mini-batch loss: 0.228\n",
      "[epoch: 30, i:  7999] avg mini-batch loss: 0.286\n",
      "[epoch: 30, i:  8999] avg mini-batch loss: 0.247\n",
      "[epoch: 30, i:  9999] avg mini-batch loss: 0.254\n",
      "[epoch: 30, i: 10999] avg mini-batch loss: 0.281\n",
      "[epoch: 30, i: 11999] avg mini-batch loss: 0.291\n",
      "[epoch: 31, i:   999] avg mini-batch loss: 0.170\n",
      "[epoch: 31, i:  1999] avg mini-batch loss: 0.177\n",
      "[epoch: 31, i:  2999] avg mini-batch loss: 0.201\n",
      "[epoch: 31, i:  3999] avg mini-batch loss: 0.208\n",
      "[epoch: 31, i:  4999] avg mini-batch loss: 0.221\n",
      "[epoch: 31, i:  5999] avg mini-batch loss: 0.184\n",
      "[epoch: 31, i:  6999] avg mini-batch loss: 0.214\n",
      "[epoch: 31, i:  7999] avg mini-batch loss: 0.223\n",
      "[epoch: 31, i:  8999] avg mini-batch loss: 0.297\n",
      "[epoch: 31, i:  9999] avg mini-batch loss: 0.277\n",
      "[epoch: 31, i: 10999] avg mini-batch loss: 0.262\n",
      "[epoch: 31, i: 11999] avg mini-batch loss: 0.251\n",
      "[epoch: 32, i:   999] avg mini-batch loss: 0.170\n",
      "[epoch: 32, i:  1999] avg mini-batch loss: 0.196\n",
      "[epoch: 32, i:  2999] avg mini-batch loss: 0.201\n",
      "[epoch: 32, i:  3999] avg mini-batch loss: 0.182\n",
      "[epoch: 32, i:  4999] avg mini-batch loss: 0.203\n",
      "[epoch: 32, i:  5999] avg mini-batch loss: 0.249\n",
      "[epoch: 32, i:  6999] avg mini-batch loss: 0.195\n",
      "[epoch: 32, i:  7999] avg mini-batch loss: 0.252\n",
      "[epoch: 32, i:  8999] avg mini-batch loss: 0.239\n",
      "[epoch: 32, i:  9999] avg mini-batch loss: 0.218\n",
      "[epoch: 32, i: 10999] avg mini-batch loss: 0.293\n",
      "[epoch: 32, i: 11999] avg mini-batch loss: 0.252\n",
      "[epoch: 33, i:   999] avg mini-batch loss: 0.158\n",
      "[epoch: 33, i:  1999] avg mini-batch loss: 0.161\n",
      "[epoch: 33, i:  2999] avg mini-batch loss: 0.169\n",
      "[epoch: 33, i:  3999] avg mini-batch loss: 0.208\n",
      "[epoch: 33, i:  4999] avg mini-batch loss: 0.220\n",
      "[epoch: 33, i:  5999] avg mini-batch loss: 0.206\n",
      "[epoch: 33, i:  6999] avg mini-batch loss: 0.214\n",
      "[epoch: 33, i:  7999] avg mini-batch loss: 0.229\n",
      "[epoch: 33, i:  8999] avg mini-batch loss: 0.217\n",
      "[epoch: 33, i:  9999] avg mini-batch loss: 0.264\n",
      "[epoch: 33, i: 10999] avg mini-batch loss: 0.262\n",
      "[epoch: 33, i: 11999] avg mini-batch loss: 0.289\n",
      "[epoch: 34, i:   999] avg mini-batch loss: 0.131\n",
      "[epoch: 34, i:  1999] avg mini-batch loss: 0.155\n",
      "[epoch: 34, i:  2999] avg mini-batch loss: 0.172\n",
      "[epoch: 34, i:  3999] avg mini-batch loss: 0.178\n",
      "[epoch: 34, i:  4999] avg mini-batch loss: 0.194\n",
      "[epoch: 34, i:  5999] avg mini-batch loss: 0.228\n",
      "[epoch: 34, i:  6999] avg mini-batch loss: 0.279\n",
      "[epoch: 34, i:  7999] avg mini-batch loss: 0.213\n",
      "[epoch: 34, i:  8999] avg mini-batch loss: 0.193\n",
      "[epoch: 34, i:  9999] avg mini-batch loss: 0.205\n",
      "[epoch: 34, i: 10999] avg mini-batch loss: 0.219\n",
      "[epoch: 34, i: 11999] avg mini-batch loss: 0.232\n",
      "Finished Training.\n"
     ]
    }
   ],
   "source": [
    "avg_losses = []   # Avg. losses.\n",
    "epochs = 35       # Total epochs.\n",
    "print_freq = 1000  # Print frequency.\n",
    "\n",
    "for epoch in range(epochs):  # Loop over the dataset multiple times.\n",
    "    running_loss = 0.0       # Initialize running loss.\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs.\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Move the inputs to the specified device.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients.\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Forward step.\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        \n",
    "        # Backward step.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimization step (update the parameters).\n",
    "        opt.step()\n",
    "\n",
    "        # Print statistics.\n",
    "        running_loss += loss.item()\n",
    "        if i % print_freq == print_freq - 1: # Print every several mini-batches.\n",
    "            avg_loss = running_loss / print_freq\n",
    "            print('[epoch: {}, i: {:5d}] avg mini-batch loss: {:.3f}'.format(\n",
    "                epoch, i, avg_loss))\n",
    "            avg_losses.append(avg_loss)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf477eed",
   "metadata": {},
   "source": [
    "## Plotting Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ce96227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(avg_losses)\n",
    "plt.xlabel('mini-batch index / {}'.format(print_freq))\n",
    "plt.ylabel('avg. mini-batch loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1a2bb",
   "metadata": {},
   "source": [
    "## Evaluate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad224c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 45 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330ce54b",
   "metadata": {},
   "source": [
    "## Evaluate Test Data Across Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5e14f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple : 67 %\n",
      "Accuracy of aquarium_fish : 51 %\n",
      "Accuracy of  baby : 36 %\n",
      "Accuracy of  bear : 22 %\n",
      "Accuracy of beaver : 31 %\n",
      "Accuracy of   bed : 41 %\n",
      "Accuracy of   bee : 39 %\n",
      "Accuracy of beetle : 38 %\n",
      "Accuracy of bicycle : 67 %\n",
      "Accuracy of bottle : 63 %\n",
      "Accuracy of  bowl : 28 %\n",
      "Accuracy of   boy : 37 %\n",
      "Accuracy of bridge : 49 %\n",
      "Accuracy of   bus : 21 %\n",
      "Accuracy of butterfly : 40 %\n",
      "Accuracy of camel : 36 %\n",
      "Accuracy of   can : 48 %\n",
      "Accuracy of castle : 55 %\n",
      "Accuracy of caterpillar : 23 %\n",
      "Accuracy of cattle : 25 %\n",
      "Accuracy of chair : 82 %\n",
      "Accuracy of chimpanzee : 40 %\n",
      "Accuracy of clock : 51 %\n",
      "Accuracy of cloud : 58 %\n",
      "Accuracy of cockroach : 72 %\n",
      "Accuracy of couch : 38 %\n",
      "Accuracy of  crab : 34 %\n",
      "Accuracy of crocodile : 20 %\n",
      "Accuracy of   cup : 56 %\n",
      "Accuracy of dinosaur : 55 %\n",
      "Accuracy of dolphin : 50 %\n",
      "Accuracy of elephant : 33 %\n",
      "Accuracy of flatfish : 42 %\n",
      "Accuracy of forest : 44 %\n",
      "Accuracy of   fox : 44 %\n",
      "Accuracy of  girl : 12 %\n",
      "Accuracy of hamster : 48 %\n",
      "Accuracy of house : 47 %\n",
      "Accuracy of kangaroo : 40 %\n",
      "Accuracy of keyboard : 72 %\n",
      "Accuracy of  lamp : 42 %\n",
      "Accuracy of lawn_mower : 63 %\n",
      "Accuracy of leopard : 34 %\n",
      "Accuracy of  lion : 39 %\n",
      "Accuracy of lizard : 23 %\n",
      "Accuracy of lobster : 38 %\n",
      "Accuracy of   man : 37 %\n",
      "Accuracy of maple_tree : 53 %\n",
      "Accuracy of motorcycle : 73 %\n",
      "Accuracy of mountain : 62 %\n",
      "Accuracy of mouse : 16 %\n",
      "Accuracy of mushroom : 41 %\n",
      "Accuracy of oak_tree : 57 %\n",
      "Accuracy of orange : 71 %\n",
      "Accuracy of orchid : 55 %\n",
      "Accuracy of otter : 11 %\n",
      "Accuracy of palm_tree : 71 %\n",
      "Accuracy of  pear : 48 %\n",
      "Accuracy of pickup_truck : 61 %\n",
      "Accuracy of pine_tree : 53 %\n",
      "Accuracy of plain : 68 %\n",
      "Accuracy of plate : 59 %\n",
      "Accuracy of poppy : 44 %\n",
      "Accuracy of porcupine : 50 %\n",
      "Accuracy of possum : 18 %\n",
      "Accuracy of rabbit : 27 %\n",
      "Accuracy of raccoon : 46 %\n",
      "Accuracy of   ray : 32 %\n",
      "Accuracy of  road : 66 %\n",
      "Accuracy of rocket : 77 %\n",
      "Accuracy of  rose : 48 %\n",
      "Accuracy of   sea : 59 %\n",
      "Accuracy of  seal : 24 %\n",
      "Accuracy of shark : 31 %\n",
      "Accuracy of shrew : 26 %\n",
      "Accuracy of skunk : 74 %\n",
      "Accuracy of skyscraper : 69 %\n",
      "Accuracy of snail : 27 %\n",
      "Accuracy of snake : 28 %\n",
      "Accuracy of spider : 42 %\n",
      "Accuracy of squirrel : 17 %\n",
      "Accuracy of streetcar : 48 %\n",
      "Accuracy of sunflower : 77 %\n",
      "Accuracy of sweet_pepper : 41 %\n",
      "Accuracy of table : 37 %\n",
      "Accuracy of  tank : 61 %\n",
      "Accuracy of telephone : 63 %\n",
      "Accuracy of television : 63 %\n",
      "Accuracy of tiger : 42 %\n",
      "Accuracy of tractor : 57 %\n",
      "Accuracy of train : 61 %\n",
      "Accuracy of trout : 56 %\n",
      "Accuracy of tulip : 36 %\n",
      "Accuracy of turtle : 18 %\n",
      "Accuracy of wardrobe : 80 %\n",
      "Accuracy of whale : 56 %\n",
      "Accuracy of willow_tree : 33 %\n",
      "Accuracy of  wolf : 39 %\n",
      "Accuracy of woman :  9 %\n",
      "Accuracy of  worm : 50 %\n"
     ]
    }
   ],
   "source": [
    "# Get test accuracy for each class.\n",
    "class_correct = list(0. for i in range(100))\n",
    "class_total = list(0. for i in range(100))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(100):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
